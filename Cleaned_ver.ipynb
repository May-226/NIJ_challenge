{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c79af32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key savefig.frameon in file C:\\Users\\myjr\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle, line 421 ('savefig.frameon : True')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.4.2/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key verbose.level in file C:\\Users\\myjr\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle, line 472 ('verbose.level  : silent      # one of silent, helpful, debug, debug-annoying')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.4.2/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key verbose.fileo in file C:\\Users\\myjr\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle, line 473 ('verbose.fileo  : sys.stdout  # a log filename, sys.stdout or sys.stderr')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.4.2/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "In C:\\Users\\myjr\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The text.latex.preview rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\myjr\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The mathtext.fallback_to_cm rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\myjr\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: Support for setting the 'mathtext.fallback_to_cm' rcParam is deprecated since 3.3 and will be removed two minor releases later; use 'mathtext.fallback : 'cm' instead.\n",
      "In C:\\Users\\myjr\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The validate_bool_maybe_none function was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\myjr\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The savefig.jpeg_quality rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\myjr\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The keymap.all_axes rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\myjr\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The animation.avconv_path rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\myjr\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The animation.avconv_args rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "C:\\Users\\myjr\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\myjr\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\myjr\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\myjr\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\myjr\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\myjr\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import interp\n",
    "#import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from IPython.display import Image\n",
    "import pydotplus\n",
    "import graphviz\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression,Ridge\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV,train_test_split,cross_val_score,cross_validate\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_squared_error, roc_auc_score,roc_curve, auc, brier_score_loss\n",
    "from sklearn import tree\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from itertools import cycle\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, BatchNormalization, Activation, Embedding\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed9c117b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "def y_roc(estimator,X):\n",
    "    y_scores=[]\n",
    "    for list in estimator.predict_proba(X):\n",
    "        y_scores.append(list[1])\n",
    "    return y_scores\n",
    "def y_roc_regression(estimator,X):\n",
    "    y_scores=[]\n",
    "    for list in estimator.predict(X):\n",
    "        y_scores.append(list)\n",
    "    return y_scores\n",
    "def cv_roc_plot(estimator,X,y):\n",
    "    cv = StratifiedKFold(n_splits=4,shuffle=False)\n",
    "    tprs = []\n",
    "    aucs = []\n",
    "    mean_fpr = np.linspace(0,1,100)\n",
    "    for train,test in cv.split(X,y):\n",
    "        prediction = estimator.fit(X.iloc[train],y.iloc[train]).predict_proba(X.iloc[test])\n",
    "        fpr, tpr, t = roc_curve(y.iloc[test], prediction[:, 1])\n",
    "        tpr[0]=0\n",
    "        tpr[-1]=1\n",
    "        tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        aucs.append(roc_auc)\n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "    return mean_fpr, mean_tpr, mean_auc\n",
    "def brier_score(y_prob_raw,y_true):\n",
    "    y_prob=[prob[1] for prob in y_prob_raw]\n",
    "    if len(y_prob)!=len(y_true):\n",
    "        print('Error: two lists must have same length')\n",
    "        return\n",
    "    out = 0\n",
    "    for prob_1,y in zip(y_prob,y_true):\n",
    "        out+=(prob_1-y)**2\n",
    "    return out/len(y_prob)\n",
    "def get_prob_1(y_prob_raw):\n",
    "    return [prob[1] for prob in y_prob_raw]\n",
    "def aver_prob(prob_lists):\n",
    "    n=len(prob_lists)\n",
    "    return np.sum(np.array(prob_lists),0)/n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0d63b6",
   "metadata": {},
   "source": [
    "### Feature engineering\n",
    "Including filling missing value, scaling and encoding predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76849dbd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'Gender', 'Race', 'Age_at_Release', 'Residence_PUMA',\n",
       "       'Gang_Affiliated', 'Supervision_Risk_Score_First',\n",
       "       'Supervision_Level_First', 'Education_Level', 'Dependents',\n",
       "       'Prison_Offense', 'Prison_Years', 'Prior_Arrest_Episodes_Felony',\n",
       "       'Prior_Arrest_Episodes_Misd', 'Prior_Arrest_Episodes_Violent',\n",
       "       'Prior_Arrest_Episodes_Property', 'Prior_Arrest_Episodes_Drug',\n",
       "       'Prior_Arrest_Episodes_PPViolationCharges',\n",
       "       'Prior_Arrest_Episodes_DVCharges', 'Prior_Arrest_Episodes_GunCharges',\n",
       "       'Prior_Conviction_Episodes_Felony', 'Prior_Conviction_Episodes_Misd',\n",
       "       'Prior_Conviction_Episodes_Viol', 'Prior_Conviction_Episodes_Prop',\n",
       "       'Prior_Conviction_Episodes_Drug',\n",
       "       'Prior_Conviction_Episodes_PPViolationCharges',\n",
       "       'Prior_Conviction_Episodes_DomesticViolenceCharges',\n",
       "       'Prior_Conviction_Episodes_GunCharges', 'Prior_Revocations_Parole',\n",
       "       'Prior_Revocations_Probation', 'Condition_MH_SA', 'Condition_Cog_Ed',\n",
       "       'Condition_Other', 'Recidivism_Within_3years',\n",
       "       'Recidivism_Arrest_Year1', 'Recidivism_Arrest_Year2',\n",
       "       'Recidivism_Arrest_Year3'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Discard unavailable features\n",
    "raw_train=pd.read_csv(r'.\\data\\NIJ_s_Recidivism_Challenge_Training_Dataset.csv')\n",
    "raw_train=raw_train.drop(columns=['Violations_ElectronicMonitoring',\n",
    "       'Violations_Instruction', 'Violations_FailToReport',\n",
    "       'Violations_MoveWithoutPermission', 'Delinquency_Reports',\n",
    "       'Program_Attendances', 'Program_UnexcusedAbsences', 'Residence_Changes',\n",
    "       'Avg_Days_per_DrugTest', 'DrugTests_THC_Positive',\n",
    "       'DrugTests_Cocaine_Positive', 'DrugTests_Meth_Positive',\n",
    "       'DrugTests_Other_Positive', 'Percent_Days_Employed', 'Jobs_Per_Year',\n",
    "       'Employment_Exempt'])\n",
    "raw_train\n",
    "raw_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f370f7f3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Change the dtype of the feature from object to intagar\n",
    "raw_train['Residence_PUMA']=raw_train['Residence_PUMA'].astype('category')\n",
    "raw_train['Residence_PUMA']=raw_train['Residence_PUMA'].cat.codes\n",
    "raw_train['Age_at_Release']=raw_train['Age_at_Release'].apply(lambda x: int(x[:2]))\n",
    "raw_train['Dependents']=raw_train['Dependents'].apply(lambda x: int(x[:1]))\n",
    "raw_train['Prior_Arrest_Episodes_Felony']=raw_train['Prior_Arrest_Episodes_Felony'].apply(lambda x: int(x[:2]))\n",
    "raw_train['Prior_Arrest_Episodes_Drug']=raw_train['Prior_Arrest_Episodes_Drug'].apply(lambda x: int(x[:2]))\n",
    "raw_train['Prior_Arrest_Episodes_Misd']=raw_train['Prior_Arrest_Episodes_Misd'].apply(lambda x: int(x[:2]))\n",
    "raw_train['Prior_Arrest_Episodes_Violent']=raw_train['Prior_Arrest_Episodes_Violent'].apply(lambda x: int(x[:2]))\n",
    "raw_train['Prior_Arrest_Episodes_Property']=raw_train['Prior_Arrest_Episodes_Property'].apply(lambda x: int(x[:2]))\n",
    "raw_train['Prior_Arrest_Episodes_PPViolationCharges']=raw_train['Prior_Arrest_Episodes_PPViolationCharges'].apply(lambda x: int(x[:2]))\n",
    "raw_train['Prior_Conviction_Episodes_Felony']=raw_train['Prior_Conviction_Episodes_Felony'].apply(lambda x: int(x[:1]))\n",
    "raw_train['Prior_Conviction_Episodes_Misd']=raw_train['Prior_Conviction_Episodes_Misd'].apply(lambda x: int(x[:1]))\n",
    "raw_train['Prior_Conviction_Episodes_Prop']=raw_train['Prior_Conviction_Episodes_Prop'].apply(lambda x: int(x[:1]))\n",
    "raw_train['Prior_Conviction_Episodes_Drug']=raw_train['Prior_Conviction_Episodes_Drug'].apply(lambda x: int(x[:1]))\n",
    "#raw_train['Delinquency_Reports']=raw_train['Delinquency_Reports'].apply(lambda x: int(x[:1]))\n",
    "#raw_train['Program_Attendances']=raw_train['Program_Attendances'].apply(lambda x: int(x[:2]))\n",
    "#raw_train['Program_UnexcusedAbsences']=raw_train['Program_UnexcusedAbsences'].apply(lambda x: int(x[:1]))\n",
    "#raw_train['Residence_Changes']=raw_train['Residence_Changes'].apply(lambda x: int(x[:1]))\n",
    "\n",
    "\n",
    "# scale the columns which are not bool or category\n",
    "      \n",
    "scaler = StandardScaler()\n",
    "scaling_set=[]\n",
    "for column in raw_train.columns:\n",
    "    if raw_train[column].dtype == object:\n",
    "        raw_train[column]=raw_train[column].astype('category')\n",
    "        raw_train[column]=raw_train[column].cat.codes\n",
    "    elif raw_train[column].dtype in ['int64','float32','float64'] :\n",
    "        scaling_set+=[column]\n",
    "raw_train[scaling_set]=scaler.fit_transform(raw_train[scaling_set].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0b1ad7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\myjr\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\myjr\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "# remove outliers, missing values of 'Supervision_Risk_Score_First' and intersection of 'Supervision_Level_First' and 'Prison_Offense'\n",
    "raw_train=raw_train.drop(index=raw_train[raw_train.Supervision_Risk_Score_First.isnull()].index)\n",
    "raw_train=raw_train.drop(index=set(raw_train[raw_train.Supervision_Level_First.isnull()].index) & set(raw_train[raw_train.Prison_Offense .isnull()].index))\n",
    "raw_train=raw_train.reset_index(drop=True)\n",
    "# impute missing value 'Supervision_Level_First' and 'Prison_Offense' with relative feature\n",
    "for missing_column in ['Supervision_Level_First','Prison_Offense']:\n",
    "    test_index=raw_train[raw_train[missing_column]==-1].index\n",
    "    train_index=raw_train[raw_train[missing_column]!=-1].index\n",
    "    X=raw_train.drop(columns=[missing_column])\n",
    "    y=raw_train[missing_column]\n",
    "    logreg = LogisticRegression()\n",
    "    logreg.fit(X.iloc[train_index,:],y[train_index])\n",
    "    raw_train.loc[test_index,missing_column]=logreg.predict(X.iloc[test_index,:])\n",
    "    raw_train[missing_column]=raw_train[missing_column].astype('category')\n",
    "    raw_train[missing_column]=raw_train[missing_column].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1e43ad5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression train score: 0.7164853792908603 \n",
      " test score: 0.7096045197740113 \n",
      " Logistic regression train Brier score: 0.18688724440002552 \n",
      " test Brier score: 0.1912073443404502 \n",
      " AUROC: 0.6965366554518414\n",
      "Random forest train score: 1.0 \n",
      " test score: 0.7081920903954803 \n",
      " Random forest  train Brier score: 0.02633926009637379 \n",
      " test Brier score: 0.195459635907094 \n",
      " AUROC: 0.6752396761039465\n",
      "SGD best layer size: {'min_samples_split': 4, 'n_estimators': 100} \n",
      " best train score: -0.18693236132593608 \n",
      " test score: -0.1907786576195972 \n",
      " SGD  train Brier score: 0.1783118708881963 \n",
      " test Brier score: 0.19077865761959653 \n",
      " AUROC: 0.6973354820393405\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\myjr\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310/310 [==============================] - 1s 2ms/step - loss: 0.7273 - accuracy: 0.6220 - val_loss: 0.5805 - val_accuracy: 0.7032\n",
      "Epoch 2/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.6061 - accuracy: 0.6893 - val_loss: 0.5755 - val_accuracy: 0.7043\n",
      "Epoch 3/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5914 - accuracy: 0.7012 - val_loss: 0.5741 - val_accuracy: 0.7022\n",
      "Epoch 4/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5843 - accuracy: 0.7027 - val_loss: 0.5728 - val_accuracy: 0.7060\n",
      "Epoch 5/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5811 - accuracy: 0.7035 - val_loss: 0.5702 - val_accuracy: 0.7060\n",
      "Epoch 6/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5812 - accuracy: 0.7018 - val_loss: 0.5706 - val_accuracy: 0.7034\n",
      "Epoch 7/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5777 - accuracy: 0.7071 - val_loss: 0.5694 - val_accuracy: 0.7039\n",
      "Epoch 8/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5749 - accuracy: 0.7054 - val_loss: 0.5672 - val_accuracy: 0.7062\n",
      "Epoch 9/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5761 - accuracy: 0.7036 - val_loss: 0.5679 - val_accuracy: 0.7060\n",
      "Epoch 10/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5744 - accuracy: 0.7051 - val_loss: 0.5652 - val_accuracy: 0.7076\n",
      "Epoch 11/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5734 - accuracy: 0.7068 - val_loss: 0.5646 - val_accuracy: 0.7083\n",
      "Epoch 12/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5733 - accuracy: 0.7075 - val_loss: 0.5642 - val_accuracy: 0.7088\n",
      "Epoch 13/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5716 - accuracy: 0.7079 - val_loss: 0.5635 - val_accuracy: 0.7107\n",
      "Epoch 14/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5694 - accuracy: 0.7075 - val_loss: 0.5636 - val_accuracy: 0.7112\n",
      "Epoch 15/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5673 - accuracy: 0.7083 - val_loss: 0.5627 - val_accuracy: 0.7079\n",
      "Epoch 16/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5693 - accuracy: 0.7074 - val_loss: 0.5622 - val_accuracy: 0.7109\n",
      "Epoch 17/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5703 - accuracy: 0.7055 - val_loss: 0.5640 - val_accuracy: 0.7079\n",
      "Epoch 18/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5689 - accuracy: 0.7061 - val_loss: 0.5605 - val_accuracy: 0.7105\n",
      "Epoch 19/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5669 - accuracy: 0.7071 - val_loss: 0.5607 - val_accuracy: 0.7093\n",
      "Epoch 20/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5670 - accuracy: 0.7110 - val_loss: 0.5608 - val_accuracy: 0.7102\n",
      "Epoch 21/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5666 - accuracy: 0.7131 - val_loss: 0.5607 - val_accuracy: 0.7112\n",
      "Epoch 22/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5676 - accuracy: 0.7078 - val_loss: 0.5609 - val_accuracy: 0.7121\n",
      "Epoch 23/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5649 - accuracy: 0.7095 - val_loss: 0.5617 - val_accuracy: 0.7107\n",
      "Epoch 24/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5640 - accuracy: 0.7093 - val_loss: 0.5602 - val_accuracy: 0.7114\n",
      "Epoch 25/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5666 - accuracy: 0.7103 - val_loss: 0.5602 - val_accuracy: 0.7105\n",
      "Epoch 26/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5668 - accuracy: 0.7089 - val_loss: 0.5600 - val_accuracy: 0.7100\n",
      "Epoch 27/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5655 - accuracy: 0.7088 - val_loss: 0.5601 - val_accuracy: 0.7130\n",
      "Epoch 28/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5674 - accuracy: 0.7111 - val_loss: 0.5599 - val_accuracy: 0.7123\n",
      "Epoch 29/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5641 - accuracy: 0.7094 - val_loss: 0.5600 - val_accuracy: 0.7093\n",
      "Epoch 30/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5649 - accuracy: 0.7070 - val_loss: 0.5604 - val_accuracy: 0.7097\n",
      "Epoch 31/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5645 - accuracy: 0.7069 - val_loss: 0.5584 - val_accuracy: 0.7105\n",
      "Epoch 32/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5646 - accuracy: 0.7081 - val_loss: 0.5588 - val_accuracy: 0.7105\n",
      "Epoch 33/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5658 - accuracy: 0.7092 - val_loss: 0.5597 - val_accuracy: 0.7114\n",
      "Epoch 34/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5635 - accuracy: 0.7088 - val_loss: 0.5588 - val_accuracy: 0.7123\n",
      "Epoch 35/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5635 - accuracy: 0.7096 - val_loss: 0.5589 - val_accuracy: 0.7102\n",
      "Epoch 36/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5641 - accuracy: 0.7114 - val_loss: 0.5584 - val_accuracy: 0.7100\n",
      "Epoch 37/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5622 - accuracy: 0.7089 - val_loss: 0.5583 - val_accuracy: 0.7081\n",
      "Epoch 38/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5633 - accuracy: 0.7106 - val_loss: 0.5583 - val_accuracy: 0.7112\n",
      "Epoch 39/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5636 - accuracy: 0.7108 - val_loss: 0.5585 - val_accuracy: 0.7128\n",
      "Epoch 40/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5636 - accuracy: 0.7092 - val_loss: 0.5578 - val_accuracy: 0.7105\n",
      "Epoch 41/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5620 - accuracy: 0.7089 - val_loss: 0.5585 - val_accuracy: 0.7121\n",
      "Epoch 42/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5625 - accuracy: 0.7113 - val_loss: 0.5589 - val_accuracy: 0.7109\n",
      "Epoch 43/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5632 - accuracy: 0.7107 - val_loss: 0.5580 - val_accuracy: 0.7102\n",
      "Epoch 44/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5647 - accuracy: 0.7083 - val_loss: 0.5578 - val_accuracy: 0.7097\n",
      "Epoch 45/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5613 - accuracy: 0.7084 - val_loss: 0.5585 - val_accuracy: 0.7119\n",
      "Epoch 46/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5626 - accuracy: 0.7093 - val_loss: 0.5580 - val_accuracy: 0.7095\n",
      "Epoch 47/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5636 - accuracy: 0.7094 - val_loss: 0.5586 - val_accuracy: 0.7102\n",
      "Epoch 48/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5627 - accuracy: 0.7117 - val_loss: 0.5580 - val_accuracy: 0.7095\n",
      "Epoch 49/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5601 - accuracy: 0.7100 - val_loss: 0.5573 - val_accuracy: 0.7100\n",
      "Epoch 50/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5618 - accuracy: 0.7119 - val_loss: 0.5581 - val_accuracy: 0.7097\n",
      "Epoch 51/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5634 - accuracy: 0.7098 - val_loss: 0.5574 - val_accuracy: 0.7093\n",
      "Epoch 52/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5632 - accuracy: 0.7125 - val_loss: 0.5590 - val_accuracy: 0.7114\n",
      "Epoch 53/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5617 - accuracy: 0.7107 - val_loss: 0.5581 - val_accuracy: 0.7107\n",
      "Epoch 54/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5625 - accuracy: 0.7145 - val_loss: 0.5576 - val_accuracy: 0.7137\n",
      "Epoch 55/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5607 - accuracy: 0.7104 - val_loss: 0.5570 - val_accuracy: 0.7107\n",
      "Epoch 56/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5628 - accuracy: 0.7118 - val_loss: 0.5567 - val_accuracy: 0.7137\n",
      "Epoch 57/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5597 - accuracy: 0.7090 - val_loss: 0.5577 - val_accuracy: 0.7095\n",
      "Epoch 58/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5615 - accuracy: 0.7125 - val_loss: 0.5585 - val_accuracy: 0.7116\n",
      "Epoch 59/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5604 - accuracy: 0.7118 - val_loss: 0.5589 - val_accuracy: 0.7105\n",
      "Epoch 60/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5611 - accuracy: 0.7150 - val_loss: 0.5576 - val_accuracy: 0.7095\n",
      "Epoch 61/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5619 - accuracy: 0.7094 - val_loss: 0.5571 - val_accuracy: 0.7133\n",
      "Epoch 62/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5599 - accuracy: 0.7133 - val_loss: 0.5564 - val_accuracy: 0.7121\n",
      "Epoch 63/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5638 - accuracy: 0.7065 - val_loss: 0.5576 - val_accuracy: 0.7121\n",
      "Epoch 64/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5609 - accuracy: 0.7108 - val_loss: 0.5566 - val_accuracy: 0.7119\n",
      "Epoch 65/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5598 - accuracy: 0.7140 - val_loss: 0.5573 - val_accuracy: 0.7105\n",
      "Epoch 66/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5594 - accuracy: 0.7120 - val_loss: 0.5567 - val_accuracy: 0.7100\n",
      "Epoch 67/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5592 - accuracy: 0.7120 - val_loss: 0.5576 - val_accuracy: 0.7121\n",
      "Epoch 68/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5607 - accuracy: 0.7153 - val_loss: 0.5572 - val_accuracy: 0.7112\n",
      "Epoch 69/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5596 - accuracy: 0.7115 - val_loss: 0.5573 - val_accuracy: 0.7083\n",
      "Epoch 70/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5594 - accuracy: 0.7137 - val_loss: 0.5575 - val_accuracy: 0.7137\n",
      "Epoch 71/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5605 - accuracy: 0.7132 - val_loss: 0.5566 - val_accuracy: 0.7109\n",
      "Epoch 72/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5607 - accuracy: 0.7107 - val_loss: 0.5566 - val_accuracy: 0.7114\n",
      "Epoch 73/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5603 - accuracy: 0.7123 - val_loss: 0.5571 - val_accuracy: 0.7123\n",
      "Epoch 74/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5618 - accuracy: 0.7127 - val_loss: 0.5582 - val_accuracy: 0.7126\n",
      "Epoch 75/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5596 - accuracy: 0.7151 - val_loss: 0.5579 - val_accuracy: 0.7116\n",
      "Epoch 76/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5621 - accuracy: 0.7121 - val_loss: 0.5565 - val_accuracy: 0.7119\n",
      "Epoch 77/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5601 - accuracy: 0.7115 - val_loss: 0.5565 - val_accuracy: 0.7121\n",
      "Epoch 78/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5604 - accuracy: 0.7104 - val_loss: 0.5564 - val_accuracy: 0.7140\n",
      "Epoch 79/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5602 - accuracy: 0.7118 - val_loss: 0.5581 - val_accuracy: 0.7105\n",
      "Epoch 80/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5615 - accuracy: 0.7142 - val_loss: 0.5564 - val_accuracy: 0.7112\n",
      "Epoch 81/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5610 - accuracy: 0.7104 - val_loss: 0.5569 - val_accuracy: 0.7133\n",
      "Epoch 82/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5607 - accuracy: 0.7131 - val_loss: 0.5568 - val_accuracy: 0.7121\n",
      "Epoch 83/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5624 - accuracy: 0.7102 - val_loss: 0.5581 - val_accuracy: 0.7109\n",
      "Epoch 84/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5598 - accuracy: 0.7136 - val_loss: 0.5580 - val_accuracy: 0.7090\n",
      "Epoch 85/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5610 - accuracy: 0.7136 - val_loss: 0.5570 - val_accuracy: 0.7102\n",
      "Epoch 86/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5598 - accuracy: 0.7135 - val_loss: 0.5570 - val_accuracy: 0.7121\n",
      "Epoch 87/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5586 - accuracy: 0.7116 - val_loss: 0.5561 - val_accuracy: 0.7105\n",
      "Epoch 88/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5585 - accuracy: 0.7107 - val_loss: 0.5575 - val_accuracy: 0.7109\n",
      "Epoch 89/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5584 - accuracy: 0.7138 - val_loss: 0.5574 - val_accuracy: 0.7095\n",
      "Epoch 90/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5596 - accuracy: 0.7099 - val_loss: 0.5573 - val_accuracy: 0.7137\n",
      "Epoch 91/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5613 - accuracy: 0.7079 - val_loss: 0.5564 - val_accuracy: 0.7083\n",
      "Epoch 92/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5591 - accuracy: 0.7110 - val_loss: 0.5568 - val_accuracy: 0.7109\n",
      "Epoch 93/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5595 - accuracy: 0.7090 - val_loss: 0.5561 - val_accuracy: 0.7100\n",
      "Epoch 94/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5583 - accuracy: 0.7138 - val_loss: 0.5557 - val_accuracy: 0.7133\n",
      "Epoch 95/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5610 - accuracy: 0.7092 - val_loss: 0.5562 - val_accuracy: 0.7107\n",
      "Epoch 96/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5609 - accuracy: 0.7087 - val_loss: 0.5565 - val_accuracy: 0.7093\n",
      "Epoch 97/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5587 - accuracy: 0.7119 - val_loss: 0.5574 - val_accuracy: 0.7095\n",
      "Epoch 98/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5580 - accuracy: 0.7126 - val_loss: 0.5573 - val_accuracy: 0.7083\n",
      "Epoch 99/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5591 - accuracy: 0.7119 - val_loss: 0.5570 - val_accuracy: 0.7105\n",
      "Epoch 100/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5586 - accuracy: 0.7096 - val_loss: 0.5561 - val_accuracy: 0.7109\n",
      "Epoch 101/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5593 - accuracy: 0.7117 - val_loss: 0.5562 - val_accuracy: 0.7086\n",
      "Epoch 102/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5582 - accuracy: 0.7129 - val_loss: 0.5575 - val_accuracy: 0.7109\n",
      "Epoch 103/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5582 - accuracy: 0.7166 - val_loss: 0.5558 - val_accuracy: 0.7109\n",
      "Epoch 104/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5562 - accuracy: 0.7145 - val_loss: 0.5563 - val_accuracy: 0.7100\n",
      "Epoch 105/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5608 - accuracy: 0.7128 - val_loss: 0.5564 - val_accuracy: 0.7105\n",
      "Epoch 106/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5588 - accuracy: 0.7128 - val_loss: 0.5561 - val_accuracy: 0.7076\n",
      "Epoch 107/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5593 - accuracy: 0.7100 - val_loss: 0.5566 - val_accuracy: 0.7093\n",
      "Epoch 108/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5573 - accuracy: 0.7130 - val_loss: 0.5568 - val_accuracy: 0.7100\n",
      "Epoch 109/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5587 - accuracy: 0.7113 - val_loss: 0.5563 - val_accuracy: 0.7102\n",
      "Epoch 110/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5591 - accuracy: 0.7091 - val_loss: 0.5571 - val_accuracy: 0.7102\n",
      "Epoch 111/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5616 - accuracy: 0.7102 - val_loss: 0.5568 - val_accuracy: 0.7126\n",
      "Epoch 112/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5611 - accuracy: 0.7059 - val_loss: 0.5563 - val_accuracy: 0.7107\n",
      "Epoch 113/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5584 - accuracy: 0.7109 - val_loss: 0.5562 - val_accuracy: 0.7100\n",
      "Epoch 114/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5575 - accuracy: 0.7134 - val_loss: 0.5567 - val_accuracy: 0.7126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5596 - accuracy: 0.7087 - val_loss: 0.5573 - val_accuracy: 0.7112\n",
      "Epoch 116/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5586 - accuracy: 0.7113 - val_loss: 0.5560 - val_accuracy: 0.7126\n",
      "Epoch 117/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5566 - accuracy: 0.7148 - val_loss: 0.5558 - val_accuracy: 0.7105\n",
      "Epoch 118/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5585 - accuracy: 0.7107 - val_loss: 0.5558 - val_accuracy: 0.7130\n",
      "Epoch 119/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5587 - accuracy: 0.7151 - val_loss: 0.5556 - val_accuracy: 0.7119\n",
      "Epoch 120/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5570 - accuracy: 0.7137 - val_loss: 0.5558 - val_accuracy: 0.7102\n",
      "Epoch 121/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5600 - accuracy: 0.7124 - val_loss: 0.5563 - val_accuracy: 0.7107\n",
      "Epoch 122/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5595 - accuracy: 0.7130 - val_loss: 0.5560 - val_accuracy: 0.7105\n",
      "Epoch 123/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5594 - accuracy: 0.7115 - val_loss: 0.5566 - val_accuracy: 0.7123\n",
      "Epoch 124/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5609 - accuracy: 0.7112 - val_loss: 0.5565 - val_accuracy: 0.7069\n",
      "Epoch 125/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5588 - accuracy: 0.7092 - val_loss: 0.5565 - val_accuracy: 0.7088\n",
      "Epoch 126/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5605 - accuracy: 0.7094 - val_loss: 0.5556 - val_accuracy: 0.7116\n",
      "Epoch 127/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5614 - accuracy: 0.7084 - val_loss: 0.5557 - val_accuracy: 0.7107\n",
      "Epoch 128/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5589 - accuracy: 0.7105 - val_loss: 0.5573 - val_accuracy: 0.7116\n",
      "Epoch 129/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5577 - accuracy: 0.7142 - val_loss: 0.5560 - val_accuracy: 0.7102\n",
      "Epoch 130/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5570 - accuracy: 0.7101 - val_loss: 0.5554 - val_accuracy: 0.7119\n",
      "Epoch 131/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5582 - accuracy: 0.7104 - val_loss: 0.5565 - val_accuracy: 0.7130\n",
      "Epoch 132/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5582 - accuracy: 0.7144 - val_loss: 0.5569 - val_accuracy: 0.7119\n",
      "Epoch 133/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5586 - accuracy: 0.7151 - val_loss: 0.5569 - val_accuracy: 0.7116\n",
      "Epoch 134/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5562 - accuracy: 0.7131 - val_loss: 0.5558 - val_accuracy: 0.7112\n",
      "Epoch 135/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5568 - accuracy: 0.7134 - val_loss: 0.5563 - val_accuracy: 0.7102\n",
      "Epoch 136/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5593 - accuracy: 0.7105 - val_loss: 0.5558 - val_accuracy: 0.7105\n",
      "Epoch 137/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5589 - accuracy: 0.7096 - val_loss: 0.5554 - val_accuracy: 0.7105\n",
      "Epoch 138/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5565 - accuracy: 0.7130 - val_loss: 0.5563 - val_accuracy: 0.7112\n",
      "Epoch 139/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5592 - accuracy: 0.7122 - val_loss: 0.5567 - val_accuracy: 0.7112\n",
      "Epoch 140/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5577 - accuracy: 0.7158 - val_loss: 0.5558 - val_accuracy: 0.7121\n",
      "Epoch 141/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5561 - accuracy: 0.7148 - val_loss: 0.5565 - val_accuracy: 0.7102\n",
      "Epoch 142/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5574 - accuracy: 0.7163 - val_loss: 0.5559 - val_accuracy: 0.7123\n",
      "Epoch 143/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5589 - accuracy: 0.7119 - val_loss: 0.5558 - val_accuracy: 0.7107\n",
      "Epoch 144/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5577 - accuracy: 0.7132 - val_loss: 0.5565 - val_accuracy: 0.7102\n",
      "Epoch 145/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5598 - accuracy: 0.7131 - val_loss: 0.5559 - val_accuracy: 0.7119\n",
      "Epoch 146/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5574 - accuracy: 0.7122 - val_loss: 0.5554 - val_accuracy: 0.7128\n",
      "Epoch 147/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5573 - accuracy: 0.7152 - val_loss: 0.5558 - val_accuracy: 0.7135\n",
      "Epoch 148/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5562 - accuracy: 0.7115 - val_loss: 0.5562 - val_accuracy: 0.7142\n",
      "Epoch 149/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5577 - accuracy: 0.7126 - val_loss: 0.5553 - val_accuracy: 0.7112\n",
      "Epoch 150/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5581 - accuracy: 0.7149 - val_loss: 0.5568 - val_accuracy: 0.7090\n",
      "Epoch 151/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5582 - accuracy: 0.7141 - val_loss: 0.5555 - val_accuracy: 0.7090\n",
      "Epoch 152/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5569 - accuracy: 0.7155 - val_loss: 0.5560 - val_accuracy: 0.7093\n",
      "Epoch 153/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5579 - accuracy: 0.7092 - val_loss: 0.5555 - val_accuracy: 0.7109\n",
      "Epoch 154/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5589 - accuracy: 0.7098 - val_loss: 0.5566 - val_accuracy: 0.7114\n",
      "Epoch 155/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5588 - accuracy: 0.7115 - val_loss: 0.5556 - val_accuracy: 0.7114\n",
      "Epoch 156/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5577 - accuracy: 0.7085 - val_loss: 0.5561 - val_accuracy: 0.7145\n",
      "Epoch 157/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5569 - accuracy: 0.7181 - val_loss: 0.5557 - val_accuracy: 0.7095\n",
      "Epoch 158/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5565 - accuracy: 0.7139 - val_loss: 0.5571 - val_accuracy: 0.7105\n",
      "Epoch 159/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5583 - accuracy: 0.7106 - val_loss: 0.5563 - val_accuracy: 0.7083\n",
      "Epoch 160/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5597 - accuracy: 0.7074 - val_loss: 0.5579 - val_accuracy: 0.7097\n",
      "Epoch 161/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5572 - accuracy: 0.7136 - val_loss: 0.5561 - val_accuracy: 0.7114\n",
      "Epoch 162/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5574 - accuracy: 0.7103 - val_loss: 0.5551 - val_accuracy: 0.7123\n",
      "Epoch 163/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5593 - accuracy: 0.7128 - val_loss: 0.5556 - val_accuracy: 0.7105\n",
      "Epoch 164/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5575 - accuracy: 0.7108 - val_loss: 0.5555 - val_accuracy: 0.7137\n",
      "Epoch 165/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5584 - accuracy: 0.7133 - val_loss: 0.5556 - val_accuracy: 0.7097\n",
      "Epoch 166/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5591 - accuracy: 0.7113 - val_loss: 0.5557 - val_accuracy: 0.7119\n",
      "Epoch 167/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5592 - accuracy: 0.7123 - val_loss: 0.5555 - val_accuracy: 0.7121\n",
      "Epoch 168/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5597 - accuracy: 0.7132 - val_loss: 0.5554 - val_accuracy: 0.7069\n",
      "Epoch 169/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5580 - accuracy: 0.7127 - val_loss: 0.5558 - val_accuracy: 0.7107\n",
      "Epoch 170/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5569 - accuracy: 0.7141 - val_loss: 0.5566 - val_accuracy: 0.7121\n",
      "Epoch 171/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5562 - accuracy: 0.7149 - val_loss: 0.5563 - val_accuracy: 0.7123\n",
      "Epoch 172/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5592 - accuracy: 0.7121 - val_loss: 0.5558 - val_accuracy: 0.7114\n",
      "Epoch 173/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5566 - accuracy: 0.7117 - val_loss: 0.5551 - val_accuracy: 0.7116\n",
      "Epoch 174/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5587 - accuracy: 0.7106 - val_loss: 0.5549 - val_accuracy: 0.7114\n",
      "Epoch 175/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5558 - accuracy: 0.7144 - val_loss: 0.5551 - val_accuracy: 0.7107\n",
      "Epoch 176/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5567 - accuracy: 0.7138 - val_loss: 0.5551 - val_accuracy: 0.7105\n",
      "Epoch 177/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5567 - accuracy: 0.7110 - val_loss: 0.5582 - val_accuracy: 0.7105\n",
      "Epoch 178/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5593 - accuracy: 0.7082 - val_loss: 0.5561 - val_accuracy: 0.7088\n",
      "Epoch 179/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5563 - accuracy: 0.7110 - val_loss: 0.5558 - val_accuracy: 0.7097\n",
      "Epoch 180/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5558 - accuracy: 0.7106 - val_loss: 0.5557 - val_accuracy: 0.7109\n",
      "Epoch 181/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5580 - accuracy: 0.7144 - val_loss: 0.5553 - val_accuracy: 0.7130\n",
      "Epoch 182/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5585 - accuracy: 0.7101 - val_loss: 0.5552 - val_accuracy: 0.7119\n",
      "Epoch 183/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5574 - accuracy: 0.7143 - val_loss: 0.5554 - val_accuracy: 0.7107\n",
      "Epoch 184/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5587 - accuracy: 0.7130 - val_loss: 0.5558 - val_accuracy: 0.7100\n",
      "Epoch 185/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5564 - accuracy: 0.7143 - val_loss: 0.5556 - val_accuracy: 0.7100\n",
      "Epoch 186/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5570 - accuracy: 0.7128 - val_loss: 0.5552 - val_accuracy: 0.7107\n",
      "Epoch 187/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5569 - accuracy: 0.7129 - val_loss: 0.5551 - val_accuracy: 0.7121\n",
      "Epoch 188/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5568 - accuracy: 0.7123 - val_loss: 0.5553 - val_accuracy: 0.7114\n",
      "Epoch 189/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5572 - accuracy: 0.7156 - val_loss: 0.5560 - val_accuracy: 0.7090\n",
      "Epoch 190/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5573 - accuracy: 0.7151 - val_loss: 0.5561 - val_accuracy: 0.7062\n",
      "Epoch 191/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5574 - accuracy: 0.7128 - val_loss: 0.5564 - val_accuracy: 0.7105\n",
      "Epoch 192/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5573 - accuracy: 0.7105 - val_loss: 0.5556 - val_accuracy: 0.7107\n",
      "Epoch 193/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5577 - accuracy: 0.7134 - val_loss: 0.5566 - val_accuracy: 0.7105\n",
      "Epoch 194/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5592 - accuracy: 0.7127 - val_loss: 0.5555 - val_accuracy: 0.7109\n",
      "Epoch 195/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5580 - accuracy: 0.7095 - val_loss: 0.5555 - val_accuracy: 0.7112\n",
      "Epoch 196/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5557 - accuracy: 0.7103 - val_loss: 0.5558 - val_accuracy: 0.7109\n",
      "Epoch 197/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5546 - accuracy: 0.7131 - val_loss: 0.5552 - val_accuracy: 0.7123\n",
      "Epoch 198/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5569 - accuracy: 0.7095 - val_loss: 0.5553 - val_accuracy: 0.7112\n",
      "Epoch 199/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5573 - accuracy: 0.7144 - val_loss: 0.5560 - val_accuracy: 0.7112\n",
      "Epoch 200/200\n",
      "310/310 [==============================] - 0s 1ms/step - loss: 0.5571 - accuracy: 0.7103 - val_loss: 0.5551 - val_accuracy: 0.7114\n",
      "443/443 [==============================] - 0s 995us/step - loss: 0.5595 - accuracy: 0.7108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\myjr\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:430: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
      "  warnings.warn('`model.predict_proba()` is deprecated and '\n",
      "C:\\Users\\myjr\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:430: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
      "  warnings.warn('`model.predict_proba()` is deprecated and '\n",
      "C:\\Users\\myjr\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:430: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
      "  warnings.warn('`model.predict_proba()` is deprecated and '\n",
      "C:\\Users\\myjr\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:430: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
      "  warnings.warn('`model.predict_proba()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP train Brier score: 0.1853538088776672 \n",
      " test Brier score: 0.19115163396646015 \n",
      " AUROC: 0.6959410983528046\n",
      "Xgboost best layer size: {'max_depth': 10, 'n_estimators': 100, 'reg_alpha': 0.001, 'reg_lambda': 1000} \n",
      " best train score: -0.18703118876632155 \n",
      " test score: -0.19157262020243898 \n",
      " Xgboost train Brier score: 0.16919120187206355 \n",
      " test Brier score: 0.19157262008481185 \n",
      " AUROC: 0.6937692651282942\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression,LogisticRegression\n",
    "\n",
    "\n",
    "X_train1,X_test1,y_train1,y_test1=train_test_split(raw_train.drop(['ID','Recidivism_Within_3years','Recidivism_Arrest_Year1','Recidivism_Arrest_Year2','Recidivism_Arrest_Year3'],axis=1),raw_train['Recidivism_Arrest_Year1'],test_size=0.2)\n",
    "fpr_list,tpr_list,auc_list=dict(),dict(),dict()\n",
    "\n",
    "\n",
    "logistic=LogisticRegression(max_iter=1000)\n",
    "logistic.fit(X_train1.fillna(0),y_train1)\n",
    "fpr_list[0], tpr_list[0], _ = roc_curve(y_test1, y_roc(logistic,X_test1.fillna(0)))\n",
    "print('Logistic regression train score:',\n",
    "      logistic.score(X_train1.fillna(0),y_train1),'\\n test score:',logistic.score(X_test1.fillna(0),y_test1.fillna(0)),'\\n Logistic regression train Brier score:',\n",
    "      brier_score(logistic.predict_proba(X_train1.fillna(0)),y_train1),'\\n test Brier score:',brier_score(logistic.predict_proba(X_test1.fillna(0)),y_test1),\n",
    "     '\\n AUROC:',roc_auc_score(y_test1, y_roc(logistic,X_test1.fillna(0))))\n",
    "\n",
    "RF=RandomForestClassifier(n_estimators=150,min_samples_split=2)\n",
    "RF.fit(X_train1.fillna(0),y_train1)\n",
    "fpr_list[1], tpr_list[1], _ = roc_curve(y_test1, y_roc(RF,X_test1.fillna(0)))\n",
    "print('Random forest train score:',\n",
    "      RF.score(X_train1.fillna(0),y_train1),'\\n test score:',RF.score(X_test1.fillna(0),y_test1.fillna(0)),'\\n Random forest  train Brier score:',\n",
    "      brier_score(RF.predict_proba(X_train1.fillna(0)),y_train1),'\\n test Brier score:',brier_score(RF.predict_proba(X_test1.fillna(0)),y_test1),\n",
    "     '\\n AUROC:',roc_auc_score(y_test1, y_roc(RF,X_test1.fillna(0))))\n",
    "\n",
    "GBDT=GradientBoostingClassifier()\n",
    "params_SGD={'n_estimators':[150,100,50],'min_samples_split':[2,4]}\n",
    "grid_SGD=GridSearchCV(GBDT, scoring='neg_brier_score',param_grid=params_SGD,cv=3)\n",
    "grid_SGD.fit(X_train1.fillna(0),y_train1.fillna(0))\n",
    "fpr_list[2], tpr_list[2], _ = roc_curve(y_test1, y_roc(grid_SGD.best_estimator_,X_test1.fillna(0)))\n",
    "print('SGD best layer size:',grid_SGD.best_params_,'\\n best train score:',\n",
    "      grid_SGD.best_score_,'\\n test score:',grid_SGD.score(X_test1.fillna(0),y_test1.fillna(0)),'\\n SGD  train Brier score:',\n",
    "      brier_score(grid_SGD.predict_proba(X_train1.fillna(0)),y_train1),'\\n test Brier score:',brier_score(grid_SGD.predict_proba(X_test1.fillna(0)),y_test1),\n",
    "     '\\n AUROC:',roc_auc_score(y_test1, y_roc(grid_SGD.best_estimator_,X_test1.fillna(0))))\n",
    "\n",
    "pipe = Sequential()\n",
    "n_cols = X_train1.shape[1]\n",
    "pipe.add(BatchNormalization(input_shape=(n_cols,)))\n",
    "pipe.add(Dense(70, activation= 'linear'))\n",
    "pipe.add(Dropout(0.3))\n",
    "pipe.add(Dense(50, activation= 'relu'))\n",
    "pipe.add(Dropout(0.3))\n",
    "pipe.add(Dense(50, activation= 'relu'))\n",
    "pipe.add(Dropout(0.3))\n",
    "#model.add(Dense(100, activation='relu'))\n",
    "#model.add(Dropout(0.3))\n",
    "#model.add(Dense(100, activation='linear'))\n",
    "pipe.add(BatchNormalization())\n",
    "pipe.add(Dense(2, activation='softmax'))\n",
    "    #model.compile(\n",
    "        #optimizer='Adam',\n",
    "        #loss='mean_squared_error',\n",
    "        #metrics=['accuracy'])\n",
    "early_stopping_monitor = EarlyStopping(patience=50)\n",
    "sgd = keras.optimizers.SGD(lr=.001, decay=2e-4, momentum=0.9, nesterov=True)\n",
    "pipe.compile(loss = 'sparse_categorical_crossentropy',optimizer = 'sgd', metrics=['accuracy'])\n",
    "history=pipe.fit(X_train1.fillna(0).astype('float32'), y_train1, validation_split=0.3, epochs=200, callbacks=[early_stopping_monitor])\n",
    "#history=model.fit(X_train, y_train, validation_split=0.2, epochs=25)\n",
    "score = pipe.evaluate(X_test1.fillna(0).astype('float32'), y_test1, verbose=0)\n",
    "pipe.fit(X_train1.fillna(0).astype('float32'),y_train1.fillna(0))\n",
    "fpr_list[3], tpr_list[3], _ = roc_curve(y_test1, y_roc(pipe,X_test1.fillna(0).astype('float32')))\n",
    "print('MLP train Brier score:',\n",
    "      brier_score(pipe.predict_proba(X_train1.fillna(0).astype('float32')),y_train1),'\\n test Brier score:',brier_score(pipe.predict_proba(X_test1.fillna(0).astype('float32')),y_test1),\n",
    "     '\\n AUROC:',roc_auc_score(y_test1, y_roc(pipe,X_test1.fillna(0).astype('float32'))))\n",
    "\n",
    "\n",
    "XGB=XGBClassifier(objective='binary:logistic')\n",
    "params_XGB={'n_estimators':[100,50],'max_depth':[8,10],'reg_alpha':[0.001],'reg_lambda':[1000]}\n",
    "grid_XGB=GridSearchCV(XGB, scoring='neg_brier_score',param_grid=params_XGB,cv=3)\n",
    "grid_XGB.fit(X_train1.fillna(0),y_train1.fillna(0))\n",
    "fpr_list[4], tpr_list[4], _ = roc_curve(y_test1, y_roc(grid_XGB.best_estimator_,X_test1.fillna(0)))\n",
    "print('Xgboost best layer size:',grid_XGB.best_params_,'\\n best train score:',\n",
    "      grid_XGB.best_score_,'\\n test score:',grid_XGB.score(X_test1.fillna(0),y_test1.fillna(0)),'\\n Xgboost train Brier score:',\n",
    "      brier_score(grid_XGB.predict_proba(X_train1.fillna(0)),y_train1),'\\n test Brier score:',brier_score(grid_XGB.predict_proba(X_test1.fillna(0)),y_test1),\n",
    "     '\\n AUROC:',roc_auc_score(y_test1, y_roc(grid_XGB.best_estimator_,X_test1.fillna(0))))\n",
    "\n",
    "\n",
    "\n",
    "colors = cycle(['aqua', 'red', 'green','orange','blue'])\n",
    "labels=['Logistic Regression','Random Forest','Gradient Boosting','MLP','Xgboost']\n",
    "for i, label, color in zip(range(len(fpr_list)), labels, colors):\n",
    "    legend= label + ' (area = {1:0.2f})'''.format(i, auc(fpr_list[i], tpr_list[i]))\n",
    "    plt.plot(fpr_list[i], tpr_list[i], color=color, lw=2,label=legend)\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC curve for different methods on Recidivism_1Year')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('ROC_plot.png',dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb605ca0",
   "metadata": {},
   "source": [
    "### Feature importance\n",
    "As required by the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3be27d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Age_at_Release</td>\n",
       "      <td>0.212413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gang_Affiliated</td>\n",
       "      <td>0.139099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Prior_Arrest_Episodes_PPViolationCharges</td>\n",
       "      <td>0.134133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Prior_Arrest_Episodes_Property</td>\n",
       "      <td>0.105199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Prior_Arrest_Episodes_Felony</td>\n",
       "      <td>0.097512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Prison_Years</td>\n",
       "      <td>0.036586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Residence_PUMA</td>\n",
       "      <td>0.034104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Prior_Arrest_Episodes_Misd</td>\n",
       "      <td>0.033652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Prison_Offense</td>\n",
       "      <td>0.029626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Prior_Conviction_Episodes_Misd</td>\n",
       "      <td>0.019174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Supervision_Risk_Score_First</td>\n",
       "      <td>0.017869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Prior_Conviction_Episodes_Prop</td>\n",
       "      <td>0.016375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Prior_Arrest_Episodes_Drug</td>\n",
       "      <td>0.015474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Prior_Arrest_Episodes_Violent</td>\n",
       "      <td>0.014684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Condition_MH_SA</td>\n",
       "      <td>0.013581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Prior_Revocations_Parole</td>\n",
       "      <td>0.010496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Education_Level</td>\n",
       "      <td>0.010381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gender</td>\n",
       "      <td>0.008721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Condition_Cog_Ed</td>\n",
       "      <td>0.006403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Dependents</td>\n",
       "      <td>0.005976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Prior_Conviction_Episodes_Felony</td>\n",
       "      <td>0.005349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Prior_Conviction_Episodes_Drug</td>\n",
       "      <td>0.005240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Condition_Other</td>\n",
       "      <td>0.004894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Prior_Conviction_Episodes_Viol</td>\n",
       "      <td>0.004200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Prior_Arrest_Episodes_DVCharges</td>\n",
       "      <td>0.003986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Race</td>\n",
       "      <td>0.003959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Supervision_Level_First</td>\n",
       "      <td>0.003594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Prior_Conviction_Episodes_DomesticViolenceCharges</td>\n",
       "      <td>0.002547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Prior_Conviction_Episodes_PPViolationCharges</td>\n",
       "      <td>0.001837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Prior_Conviction_Episodes_GunCharges</td>\n",
       "      <td>0.001545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Prior_Arrest_Episodes_GunCharges</td>\n",
       "      <td>0.001076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Prior_Revocations_Probation</td>\n",
       "      <td>0.000312</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Feature  importance\n",
       "2                                      Age_at_Release    0.212413\n",
       "4                                     Gang_Affiliated    0.139099\n",
       "16           Prior_Arrest_Episodes_PPViolationCharges    0.134133\n",
       "14                     Prior_Arrest_Episodes_Property    0.105199\n",
       "11                       Prior_Arrest_Episodes_Felony    0.097512\n",
       "10                                       Prison_Years    0.036586\n",
       "3                                      Residence_PUMA    0.034104\n",
       "12                         Prior_Arrest_Episodes_Misd    0.033652\n",
       "9                                      Prison_Offense    0.029626\n",
       "20                     Prior_Conviction_Episodes_Misd    0.019174\n",
       "5                        Supervision_Risk_Score_First    0.017869\n",
       "22                     Prior_Conviction_Episodes_Prop    0.016375\n",
       "15                         Prior_Arrest_Episodes_Drug    0.015474\n",
       "13                      Prior_Arrest_Episodes_Violent    0.014684\n",
       "29                                    Condition_MH_SA    0.013581\n",
       "27                           Prior_Revocations_Parole    0.010496\n",
       "7                                     Education_Level    0.010381\n",
       "0                                              Gender    0.008721\n",
       "30                                   Condition_Cog_Ed    0.006403\n",
       "8                                          Dependents    0.005976\n",
       "19                   Prior_Conviction_Episodes_Felony    0.005349\n",
       "23                     Prior_Conviction_Episodes_Drug    0.005240\n",
       "31                                    Condition_Other    0.004894\n",
       "21                     Prior_Conviction_Episodes_Viol    0.004200\n",
       "17                    Prior_Arrest_Episodes_DVCharges    0.003986\n",
       "1                                                Race    0.003959\n",
       "6                             Supervision_Level_First    0.003594\n",
       "25  Prior_Conviction_Episodes_DomesticViolenceCharges    0.002547\n",
       "24       Prior_Conviction_Episodes_PPViolationCharges    0.001837\n",
       "26               Prior_Conviction_Episodes_GunCharges    0.001545\n",
       "18                   Prior_Arrest_Episodes_GunCharges    0.001076\n",
       "28                        Prior_Revocations_Probation    0.000312"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GBDT_importance=pd.DataFrame()\n",
    "GBDT_importance['Feature']=raw_train.drop(['ID','Recidivism_Within_3years','Recidivism_Arrest_Year1','Recidivism_Arrest_Year2','Recidivism_Arrest_Year3'],axis=1).columns\n",
    "GBDT_importance['importance']=grid_SGD.best_estimator_.feature_importances_\n",
    "GBDT_importance=GBDT_importance.sort_values(by='importance',ascending=False)\n",
    "GBDT_importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f20c1be5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               feature  importance\n",
      "4                                      Residence_PUMA        1696\n",
      "11                       Supervision_Risk_Score_First        1089\n",
      "3                        Prior_Arrest_Episodes_Felony         791\n",
      "10                                     Age_at_Release         748\n",
      "13                                     Prison_Offense         654\n",
      "20                         Prior_Arrest_Episodes_Drug         632\n",
      "7                          Prior_Arrest_Episodes_Misd         616\n",
      "17           Prior_Arrest_Episodes_PPViolationCharges         574\n",
      "5                                        Prison_Years         492\n",
      "1                      Prior_Arrest_Episodes_Property         476\n",
      "18                     Prior_Conviction_Episodes_Misd         475\n",
      "25                      Prior_Arrest_Episodes_Violent         462\n",
      "6                                          Dependents         449\n",
      "15                                    Education_Level         400\n",
      "14                   Prior_Conviction_Episodes_Felony         365\n",
      "2                      Prior_Conviction_Episodes_Prop         320\n",
      "26                     Prior_Conviction_Episodes_Drug         283\n",
      "21                            Supervision_Level_First         270\n",
      "16                        Prior_Revocations_Probation         249\n",
      "30                           Prior_Revocations_Parole         236\n",
      "23                                               Race         225\n",
      "19                                             Gender         212\n",
      "22                    Prior_Arrest_Episodes_DVCharges         209\n",
      "24               Prior_Conviction_Episodes_GunCharges         189\n",
      "0                                     Gang_Affiliated         185\n",
      "29  Prior_Conviction_Episodes_DomesticViolenceCharges         183\n",
      "28                                    Condition_MH_SA         176\n",
      "9                      Prior_Conviction_Episodes_Viol         172\n",
      "12                                   Condition_Cog_Ed         168\n",
      "31                                    Condition_Other         166\n",
      "8                    Prior_Arrest_Episodes_GunCharges         159\n",
      "27       Prior_Conviction_Episodes_PPViolationCharges         140\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "You must install matplotlib to plot importance",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\xgboost\\plotting.py\u001b[0m in \u001b[0;36mplot_importance\u001b[1;34m(booster, ax, height, xlim, ylim, title, xlabel, ylabel, importance_type, max_num_features, grid, show_values, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m         \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolorbar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\colorbar.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmpl\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_api\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontour\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mticker\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0martist\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmartist\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\contour.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollections\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmcoll\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfont_manager\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfont_manager\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\font_manager.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m     parse_fontconfig_pattern, generate_fontconfig_pattern)\n\u001b[1;32m---> 46\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrcsetup\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_validators\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name '_validators' from 'matplotlib.rcsetup' (C:\\Users\\myjr\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\rcsetup.py)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-024bf5e957b8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mplot_importance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mplot_importance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrid_XGB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\xgboost\\plotting.py\u001b[0m in \u001b[0;36mplot_importance\u001b[1;34m(booster, ax, height, xlim, ylim, title, xlabel, ylabel, importance_type, max_num_features, grid, show_values, **kwargs)\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'You must install matplotlib to plot importance'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbooster\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXGBModel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: You must install matplotlib to plot importance"
     ]
    }
   ],
   "source": [
    "xgb_fea_imp=pd.DataFrame(list(grid_XGB.best_estimator_.get_booster().get_fscore().items()),\n",
    "columns=['feature','importance']).sort_values('importance', ascending=False)\n",
    "print('',xgb_fea_imp)\n",
    "xgb_fea_imp.to_csv('xgb_fea_imp.csv')\n",
    "\n",
    "from xgboost import plot_importance\n",
    "plot_importance(grid_XGB.best_estimator_, )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d4c9a5",
   "metadata": {},
   "source": [
    "### Brier as loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "646af55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def custom_asymmetric_train(y_true, y_pred):\n",
    "    residual = (y_true - y_pred).astype(\"float\")\n",
    "    grad = np.where(residual<0, -2*10.0*residual, -2*residual)\n",
    "    hess = np.where(residual<0, 2*10.0, 2.0)\n",
    "    return grad, hess\n",
    "\n",
    "def custom_asymmetric_valid(y_true, y_pred):\n",
    "    residual = (y_true - y_pred).astype(\"float\")\n",
    "    loss = np.where(residual < 0, (residual**2)*10.0, residual**2) \n",
    "    return \"custom_asymmetric_eval\", np.mean(loss), False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "98e271a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LGBMRegressor' object has no attribute 'predict_prob'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-64-35a7f4114c0c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgbm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_prob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'LGBMRegressor' object has no attribute 'predict_prob'"
     ]
    }
   ],
   "source": [
    "gbm.predict(X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c00917e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"lgb_train = lgb.Dataset(X_train, y_train, free_raw_data=False)\\nlgb_eval = lgb.Dataset(X_valid, y_valid, reference=lgb_train, free_raw_data=False)\\n\\n# specify your configurations as a dict\\nparams = {\\n    'objective': 'regression',\\n    'verbose': 0\\n}\\n\\ngbm = lgb.train(params,\\n                lgb_train,\\n                num_boost_round=10,\\n                init_model=gbm,\\n                fobj=custom_asymmetric_train,\\n                feval=custom_asymmetric_valid,\\n                valid_sets=lgb_eval)\\n                \\ny_pred = gbm.predict(X_valid)\""
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lightgbm\n",
    "\n",
    "# default lightgbm model with sklearn api\n",
    "gbm = lightgbm.LGBMRegressor() \n",
    "\n",
    "# updating objective function to custom\n",
    "# default is \"regression\"\n",
    "# also adding metrics to check different scores\n",
    "gbm.set_params(**{'objective': custom_asymmetric_train}, metrics = [\"mse\", 'mae'])\n",
    "\n",
    "# fitting model \n",
    "gbm.fit(\n",
    "    X_train1,\n",
    "    y_train1,\n",
    "    eval_set=[(X_test1, y_test1)],\n",
    "    eval_metric=custom_asymmetric_valid,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "y_pred = gbm.predict(X_test1)\n",
    "\n",
    "\n",
    "# create dataset for lightgbm\n",
    "# if you want to re-use data, remember to set free_raw_data=False\n",
    "'''lgb_train = lgb.Dataset(X_train, y_train, free_raw_data=False)\n",
    "lgb_eval = lgb.Dataset(X_valid, y_valid, reference=lgb_train, free_raw_data=False)\n",
    "\n",
    "# specify your configurations as a dict\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=10,\n",
    "                init_model=gbm,\n",
    "                fobj=custom_asymmetric_train,\n",
    "                feval=custom_asymmetric_valid,\n",
    "                valid_sets=lgb_eval)\n",
    "                \n",
    "y_pred = gbm.predict(X_valid)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "217f09aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4507"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee11eee",
   "metadata": {},
   "source": [
    "### PUMA external data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05ce6cf1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RT</th>\n",
       "      <th>SERIALNO</th>\n",
       "      <th>DIVISION</th>\n",
       "      <th>PUMA</th>\n",
       "      <th>REGION</th>\n",
       "      <th>ST</th>\n",
       "      <th>ADJHSG</th>\n",
       "      <th>ADJINC</th>\n",
       "      <th>WGTP</th>\n",
       "      <th>NP</th>\n",
       "      <th>...</th>\n",
       "      <th>wgtp72</th>\n",
       "      <th>wgtp73</th>\n",
       "      <th>wgtp74</th>\n",
       "      <th>wgtp75</th>\n",
       "      <th>wgtp76</th>\n",
       "      <th>wgtp77</th>\n",
       "      <th>wgtp78</th>\n",
       "      <th>wgtp79</th>\n",
       "      <th>wgtp80</th>\n",
       "      <th>Code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H</td>\n",
       "      <td>127</td>\n",
       "      <td>5</td>\n",
       "      <td>1400</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>1000000</td>\n",
       "      <td>1007549</td>\n",
       "      <td>139</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>242</td>\n",
       "      <td>41</td>\n",
       "      <td>124</td>\n",
       "      <td>132</td>\n",
       "      <td>224</td>\n",
       "      <td>157</td>\n",
       "      <td>237</td>\n",
       "      <td>280</td>\n",
       "      <td>142</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>H</td>\n",
       "      <td>131</td>\n",
       "      <td>5</td>\n",
       "      <td>700</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>1000000</td>\n",
       "      <td>1007549</td>\n",
       "      <td>60</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>95</td>\n",
       "      <td>96</td>\n",
       "      <td>19</td>\n",
       "      <td>70</td>\n",
       "      <td>53</td>\n",
       "      <td>12</td>\n",
       "      <td>61</td>\n",
       "      <td>18</td>\n",
       "      <td>111</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>H</td>\n",
       "      <td>136</td>\n",
       "      <td>5</td>\n",
       "      <td>2200</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>1000000</td>\n",
       "      <td>1007549</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>23</td>\n",
       "      <td>33</td>\n",
       "      <td>40</td>\n",
       "      <td>38</td>\n",
       "      <td>19</td>\n",
       "      <td>33</td>\n",
       "      <td>19</td>\n",
       "      <td>8</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>H</td>\n",
       "      <td>151</td>\n",
       "      <td>5</td>\n",
       "      <td>1100</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>1000000</td>\n",
       "      <td>1007549</td>\n",
       "      <td>213</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>55</td>\n",
       "      <td>175</td>\n",
       "      <td>62</td>\n",
       "      <td>53</td>\n",
       "      <td>313</td>\n",
       "      <td>200</td>\n",
       "      <td>330</td>\n",
       "      <td>252</td>\n",
       "      <td>239</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>H</td>\n",
       "      <td>294</td>\n",
       "      <td>5</td>\n",
       "      <td>3400</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>1000000</td>\n",
       "      <td>1007549</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>59</td>\n",
       "      <td>19</td>\n",
       "      <td>61</td>\n",
       "      <td>109</td>\n",
       "      <td>102</td>\n",
       "      <td>99</td>\n",
       "      <td>74</td>\n",
       "      <td>73</td>\n",
       "      <td>97</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45693</th>\n",
       "      <td>H</td>\n",
       "      <td>1492766</td>\n",
       "      <td>5</td>\n",
       "      <td>5002</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>1000000</td>\n",
       "      <td>1007549</td>\n",
       "      <td>69</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>60</td>\n",
       "      <td>95</td>\n",
       "      <td>23</td>\n",
       "      <td>114</td>\n",
       "      <td>81</td>\n",
       "      <td>64</td>\n",
       "      <td>126</td>\n",
       "      <td>67</td>\n",
       "      <td>99</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45694</th>\n",
       "      <td>H</td>\n",
       "      <td>1492793</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>1000000</td>\n",
       "      <td>1007549</td>\n",
       "      <td>53</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>132</td>\n",
       "      <td>51</td>\n",
       "      <td>60</td>\n",
       "      <td>106</td>\n",
       "      <td>39</td>\n",
       "      <td>77</td>\n",
       "      <td>57</td>\n",
       "      <td>49</td>\n",
       "      <td>79</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45695</th>\n",
       "      <td>H</td>\n",
       "      <td>1492802</td>\n",
       "      <td>5</td>\n",
       "      <td>700</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>1000000</td>\n",
       "      <td>1007549</td>\n",
       "      <td>48</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>53</td>\n",
       "      <td>59</td>\n",
       "      <td>80</td>\n",
       "      <td>40</td>\n",
       "      <td>68</td>\n",
       "      <td>85</td>\n",
       "      <td>76</td>\n",
       "      <td>105</td>\n",
       "      <td>43</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45696</th>\n",
       "      <td>H</td>\n",
       "      <td>1492814</td>\n",
       "      <td>5</td>\n",
       "      <td>2800</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>1000000</td>\n",
       "      <td>1007549</td>\n",
       "      <td>109</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>84</td>\n",
       "      <td>114</td>\n",
       "      <td>101</td>\n",
       "      <td>79</td>\n",
       "      <td>35</td>\n",
       "      <td>203</td>\n",
       "      <td>117</td>\n",
       "      <td>108</td>\n",
       "      <td>182</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45697</th>\n",
       "      <td>H</td>\n",
       "      <td>1492830</td>\n",
       "      <td>5</td>\n",
       "      <td>4000</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>1000000</td>\n",
       "      <td>1007549</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>42</td>\n",
       "      <td>9</td>\n",
       "      <td>26</td>\n",
       "      <td>28</td>\n",
       "      <td>9</td>\n",
       "      <td>30</td>\n",
       "      <td>28</td>\n",
       "      <td>8</td>\n",
       "      <td>47</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45698 rows × 232 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      RT  SERIALNO  DIVISION  PUMA  REGION  ST   ADJHSG   ADJINC  WGTP  NP  \\\n",
       "0      H       127         5  1400       3  13  1000000  1007549   139   1   \n",
       "1      H       131         5   700       3  13  1000000  1007549    60   3   \n",
       "2      H       136         5  2200       3  13  1000000  1007549    22   1   \n",
       "3      H       151         5  1100       3  13  1000000  1007549   213   2   \n",
       "4      H       294         5  3400       3  13  1000000  1007549    64   2   \n",
       "...   ..       ...       ...   ...     ...  ..      ...      ...   ...  ..   \n",
       "45693  H   1492766         5  5002       3  13  1000000  1007549    69   1   \n",
       "45694  H   1492793         5   100       3  13  1000000  1007549    53   2   \n",
       "45695  H   1492802         5   700       3  13  1000000  1007549    48   2   \n",
       "45696  H   1492814         5  2800       3  13  1000000  1007549   109   2   \n",
       "45697  H   1492830         5  4000       3  13  1000000  1007549    28   1   \n",
       "\n",
       "       ...  wgtp72  wgtp73  wgtp74  wgtp75  wgtp76  wgtp77  wgtp78  wgtp79  \\\n",
       "0      ...     242      41     124     132     224     157     237     280   \n",
       "1      ...      95      96      19      70      53      12      61      18   \n",
       "2      ...       7      23      33      40      38      19      33      19   \n",
       "3      ...      55     175      62      53     313     200     330     252   \n",
       "4      ...      59      19      61     109     102      99      74      73   \n",
       "...    ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "45693  ...      60      95      23     114      81      64     126      67   \n",
       "45694  ...     132      51      60     106      39      77      57      49   \n",
       "45695  ...      53      59      80      40      68      85      76     105   \n",
       "45696  ...      84     114     101      79      35     203     117     108   \n",
       "45697  ...      42       9      26      28       9      30      28       8   \n",
       "\n",
       "       wgtp80  Code  \n",
       "0         142     4  \n",
       "1         111    18  \n",
       "2           8    22  \n",
       "3         239    19  \n",
       "4          97    12  \n",
       "...       ...   ...  \n",
       "45693      99    10  \n",
       "45694      79     7  \n",
       "45695      43    18  \n",
       "45696     182    17  \n",
       "45697      47     8  \n",
       "\n",
       "[45698 rows x 232 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_PUMA=pd.read_csv(r'.\\data\\ss13hga_13.csv')\n",
    "pdata_PUMA=pd.read_csv(r'.\\data\\ss13pga_13.csv')\n",
    "data_PUMA['Code']=None\n",
    "pdata_PUMA['Code']=None\n",
    "data_PUMA['PUMA']=data_PUMA['PUMA'].astype(str)\n",
    "pdata_PUMA['PUMA']=pdata_PUMA['PUMA'].astype(str)\n",
    "PUMA_code_map={'1003':1,'4400':1,'1008':2,'4300':2,'1200':3, '1300':3,'1400':4,'1500':4,'1600':4,'1700':5,'1800':5,\n",
    "              '2001':6,'2002':6,'2003':6,'4005':6,'100':7,'200':7,'500':7,'4000':8,'4100':8,'4200':8,'5001':9,'6001':9,'6002':9,\n",
    "              '2400':10,'5002':10,'1001':11,'3004':11,'4600':11,'1002':12, '1005':12, '3300':12, '3400':12, '4001':12, '4002':12,\n",
    "              '4006':12,'3101':13,'3102':13,'1900':14, '3900':14, '4003':14, '4004':14,'3001':15, '3002':15, '3003':15, '3005':15,\n",
    "              '2500':16, '4500':16,'2800':17, '2900':17, '3200':17, '3500':17,'600':18, '700':18, '800':18,'900':19, '1100':19,\n",
    "              '300':20, '401':20, '402':20, '1004':21, '2100':21 ,'2200':22, '2300':22, '1006':23, '1007':23, '2004':23,'2600':24, \n",
    "              '2700':24,'3600':25, '3700':25, '3800':25}\n",
    "data_PUMA['Code']=data_PUMA['PUMA'].map(PUMA_code_map)\n",
    "pdata_PUMA['Code']=pdata_PUMA['PUMA'].map(PUMA_code_map)\n",
    "data_PUMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab596d39",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>County</th>\n",
       "      <th>Total law\\nenforcement\\nemployees</th>\n",
       "      <th>Total\\nofficers</th>\n",
       "      <th>Total\\ncivilians</th>\n",
       "      <th>Murder</th>\n",
       "      <th>Rape</th>\n",
       "      <th>Robbery</th>\n",
       "      <th>Assault</th>\n",
       "      <th>Burglary</th>\n",
       "      <th>Larceny</th>\n",
       "      <th>Vehicle Theft</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Barrow</td>\n",
       "      <td>190</td>\n",
       "      <td>129</td>\n",
       "      <td>61</td>\n",
       "      <td>2</td>\n",
       "      <td>33</td>\n",
       "      <td>20</td>\n",
       "      <td>302</td>\n",
       "      <td>547</td>\n",
       "      <td>1551</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bartow</td>\n",
       "      <td>234</td>\n",
       "      <td>197</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>32</td>\n",
       "      <td>329</td>\n",
       "      <td>806</td>\n",
       "      <td>2263</td>\n",
       "      <td>264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bibb</td>\n",
       "      <td>361</td>\n",
       "      <td>296</td>\n",
       "      <td>65</td>\n",
       "      <td>19</td>\n",
       "      <td>67</td>\n",
       "      <td>296</td>\n",
       "      <td>416</td>\n",
       "      <td>2500</td>\n",
       "      <td>6297</td>\n",
       "      <td>715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Brantley</td>\n",
       "      <td>43</td>\n",
       "      <td>19</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "      <td>191</td>\n",
       "      <td>243</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Brooks</td>\n",
       "      <td>51</td>\n",
       "      <td>22</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>45</td>\n",
       "      <td>114</td>\n",
       "      <td>291</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>Webster</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>White</td>\n",
       "      <td>73</td>\n",
       "      <td>40</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>35</td>\n",
       "      <td>153</td>\n",
       "      <td>328</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>Wilcox</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>72</td>\n",
       "      <td>93</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>Wilkes</td>\n",
       "      <td>27</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>29</td>\n",
       "      <td>106</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>Wilkinson</td>\n",
       "      <td>28</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>47</td>\n",
       "      <td>75</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>125 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        County  Total law\\nenforcement\\nemployees  Total\\nofficers  \\\n",
       "0       Barrow                                190              129   \n",
       "1       Bartow                                234              197   \n",
       "2         Bibb                                361              296   \n",
       "3     Brantley                                 43               19   \n",
       "4       Brooks                                 51               22   \n",
       "..         ...                                ...              ...   \n",
       "120    Webster                                  6                5   \n",
       "121      White                                 73               40   \n",
       "122     Wilcox                                 16               10   \n",
       "123     Wilkes                                 27               14   \n",
       "124  Wilkinson                                 28               16   \n",
       "\n",
       "     Total\\ncivilians Murder Rape Robbery Assault Burglary Larceny  \\\n",
       "0                  61      2   33      20     302      547    1551   \n",
       "1                  37      0   13      32     329      806    2263   \n",
       "2                  65     19   67     296     416     2500    6297   \n",
       "3                  24      0    1       7      16      191     243   \n",
       "4                  29      2    2       9      45      114     291   \n",
       "..                ...    ...  ...     ...     ...      ...     ...   \n",
       "120                 1      0    0       1       0        8       7   \n",
       "121                33      0    3       4      35      153     328   \n",
       "122                 6      0    5       2      16       72      93   \n",
       "123                13      0    0       0      32       29     106   \n",
       "124                12      0    0       0       7       47      75   \n",
       "\n",
       "    Vehicle Theft  \n",
       "0             145  \n",
       "1             264  \n",
       "2             715  \n",
       "3              10  \n",
       "4              31  \n",
       "..            ...  \n",
       "120             3  \n",
       "121            15  \n",
       "122            10  \n",
       "123            11  \n",
       "124             3  \n",
       "\n",
       "[125 rows x 11 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Law_enforcement=pd.read_excel(r'.\\data\\table_80_full_time_law_enforcement_employees_georgia_by_metropolitan_and_nonmetropolitan_counties_2012.xls')\n",
    "crime_rates=pd.read_excel(r'.\\data\\georgia crime rates.xlsx',header=1)\n",
    "crime_rates=crime_rates.groupby(by='County').sum().iloc[11:,:]\n",
    "crime_len=pd.merge(Law_enforcement,crime_rates,left_on='County',right_on='County')\n",
    "crime_len=crime_len.drop(columns=crime_len.columns[crime_len.isnull().any()])\n",
    "crime_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4eb2f5b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>County</th>\n",
       "      <th>Total law\\nenforcement\\nemployees</th>\n",
       "      <th>Total\\nofficers</th>\n",
       "      <th>Total\\ncivilians</th>\n",
       "      <th>Murder</th>\n",
       "      <th>Rape</th>\n",
       "      <th>Robbery</th>\n",
       "      <th>Assault</th>\n",
       "      <th>Burglary</th>\n",
       "      <th>Larceny</th>\n",
       "      <th>Vehicle Theft</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Barrow</td>\n",
       "      <td>190</td>\n",
       "      <td>129</td>\n",
       "      <td>61</td>\n",
       "      <td>2</td>\n",
       "      <td>33</td>\n",
       "      <td>20</td>\n",
       "      <td>302</td>\n",
       "      <td>547</td>\n",
       "      <td>1551</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bartow</td>\n",
       "      <td>234</td>\n",
       "      <td>197</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>32</td>\n",
       "      <td>329</td>\n",
       "      <td>806</td>\n",
       "      <td>2263</td>\n",
       "      <td>264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bibb</td>\n",
       "      <td>361</td>\n",
       "      <td>296</td>\n",
       "      <td>65</td>\n",
       "      <td>19</td>\n",
       "      <td>67</td>\n",
       "      <td>296</td>\n",
       "      <td>416</td>\n",
       "      <td>2500</td>\n",
       "      <td>6297</td>\n",
       "      <td>715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Brantley</td>\n",
       "      <td>43</td>\n",
       "      <td>19</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "      <td>191</td>\n",
       "      <td>243</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Brooks</td>\n",
       "      <td>51</td>\n",
       "      <td>22</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>45</td>\n",
       "      <td>114</td>\n",
       "      <td>291</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>Webster</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>White</td>\n",
       "      <td>73</td>\n",
       "      <td>40</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>35</td>\n",
       "      <td>153</td>\n",
       "      <td>328</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>Wilcox</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>72</td>\n",
       "      <td>93</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>Wilkes</td>\n",
       "      <td>27</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>29</td>\n",
       "      <td>106</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>Wilkinson</td>\n",
       "      <td>28</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>47</td>\n",
       "      <td>75</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>125 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        County  Total law\\nenforcement\\nemployees  Total\\nofficers  \\\n",
       "0       Barrow                                190              129   \n",
       "1       Bartow                                234              197   \n",
       "2         Bibb                                361              296   \n",
       "3     Brantley                                 43               19   \n",
       "4       Brooks                                 51               22   \n",
       "..         ...                                ...              ...   \n",
       "120    Webster                                  6                5   \n",
       "121      White                                 73               40   \n",
       "122     Wilcox                                 16               10   \n",
       "123     Wilkes                                 27               14   \n",
       "124  Wilkinson                                 28               16   \n",
       "\n",
       "     Total\\ncivilians Murder Rape Robbery Assault Burglary Larceny  \\\n",
       "0                  61      2   33      20     302      547    1551   \n",
       "1                  37      0   13      32     329      806    2263   \n",
       "2                  65     19   67     296     416     2500    6297   \n",
       "3                  24      0    1       7      16      191     243   \n",
       "4                  29      2    2       9      45      114     291   \n",
       "..                ...    ...  ...     ...     ...      ...     ...   \n",
       "120                 1      0    0       1       0        8       7   \n",
       "121                33      0    3       4      35      153     328   \n",
       "122                 6      0    5       2      16       72      93   \n",
       "123                13      0    0       0      32       29     106   \n",
       "124                12      0    0       0       7       47      75   \n",
       "\n",
       "    Vehicle Theft  \n",
       "0             145  \n",
       "1             264  \n",
       "2             715  \n",
       "3              10  \n",
       "4              31  \n",
       "..            ...  \n",
       "120             3  \n",
       "121            15  \n",
       "122            10  \n",
       "123            11  \n",
       "124             3  \n",
       "\n",
       "[125 rows x 11 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crime_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4378db3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "crime_len=crime_len.set_index(keys='County')\n",
    "crime_len['PUMA']=None\n",
    "crime_len.loc['Barrow','PUMA']=3800\n",
    "crime_len.loc['Bartow','PUMA']=2900\n",
    "crime_len.loc['Bibb','PUMA']=1700\n",
    "crime_len.loc['Bryan','PUMA']=200\n",
    "crime_len.loc['Carroll','PUMA']=2300\n",
    "crime_len.loc['Catoosa','PUMA']=2600\n",
    "crime_len.loc['Chatham','PUMA']=401\n",
    "crime_len.loc['Chattahoochee','PUMA']=1700\n",
    "crime_len.loc['Cherokee','PUMA']=3101\n",
    "crime_len.loc['Clarke','PUMA']=3600\n",
    "crime_len.loc['Clayton','PUMA']=5001\n",
    "crime_len.loc['Cobb','PUMA']=3001\n",
    "crime_len.loc['Columbia','PUMA']=4100\n",
    "crime_len.loc['Coweta','PUMA']=2100\n",
    "crime_len.loc['Dade','PUMA']=2600\n",
    "crime_len.loc['Dougherty','PUMA']=900\n",
    "crime_len.loc['Effingham','PUMA']=300\n",
    "crime_len.loc['Floyd','PUMA']=2500\n",
    "crime_len.loc['Forsyth','PUMA']=3300\n",
    "crime_len.loc['Fulton','PUMA']=4600\n",
    "crime_len.loc['Glynn','PUMA']=100\n",
    "crime_len.loc['Hall','PUMA']=3400\n",
    "crime_len.loc['Haralson','PUMA']=2500\n",
    "crime_len.loc['Heard','PUMA']=2200\n",
    "crime_len.loc['Houston','PUMA']=1500\n",
    "crime_len.loc['Long','PUMA']=200\n",
    "crime_len.loc['Lowndes','PUMA']=600\n",
    "crime_len.loc['McIntosh','PUMA']=100\n",
    "crime_len.loc['Morgan','PUMA']=3900\n",
    "crime_len.loc['Newton','PUMA']=4300\n",
    "crime_len.loc['Paulding','PUMA']=4500\n",
    "crime_len.loc['Pulaski','PUMA']=1500\n",
    "crime_len.loc['Rockdale','PUMA']=4300\n",
    "crime_len.loc['Walker','PUMA']=2600\n",
    "crime_len.loc['Walton','PUMA']=3900\n",
    "crime_len.loc['Whitfield','PUMA']=2700\n",
    "crime_len.loc['Worth','PUMA']=800\n",
    "crime_len.loc['Bulloch','PUMA']=300\n",
    "crime_len.loc['Camden','PUMA']=100\n",
    "crime_len.loc['Chattooga','PUMA']=2600\n",
    "crime_len.loc['Decatur','PUMA']=2002\n",
    "crime_len.loc['Dooly','PUMA']=2600\n",
    "crime_len.loc['Jackson','PUMA']=3800\n",
    "crime_len.loc['Macon','PUMA']=1400\n",
    "crime_len.loc['Polk','PUMA']=2500\n",
    "crime_len.loc['Screven','PUMA']=300\n",
    "crime_len.loc['Thomas','PUMA']=800\n",
    "crime_len.loc['Troup','PUMA']=2200\n",
    "crime_len.loc['Union','PUMA']=1004\n",
    "crime_len.drop(index=crime_len[crime_len.PUMA.isnull()].index,inplace=True)\n",
    "crime_len['Code']=(crime_len['PUMA']).astype(str).map(PUMA_code_map)\n",
    "crime_len=crime_len.astype(int)\n",
    "crime_len=crime_len.groupby('Code').mean()\n",
    "crime_len.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfa287b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "PUMA_group=data_PUMA.groupby(by='Code',as_index=False).mean()\n",
    "PUMA_group=PUMA_group.iloc[:,:-81].drop(columns=['SERIALNO', 'DIVISION', 'REGION', 'ST', 'ADJHSG', 'ADJINC',\n",
    "       'WGTP'])\n",
    "PUMA_group['TEN']=data_PUMA[['Code','TEN']].groupby(by='Code',as_index=False).agg(lambda x:x.value_counts().index[0])['TEN']\n",
    "PUMA_new=pd.merge(PUMA_group,crime_len,left_on='Code',right_on='Code',how='outer')\n",
    "PUMA_new=PUMA_new.fillna(PUMA_new.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb7b9286",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train=pd.read_csv(r'.\\data\\NIJ_s_Recidivism_Challenge_Training_Dataset.csv')\n",
    "raw_train=raw_train.drop(columns=['Violations_ElectronicMonitoring',\n",
    "       'Violations_Instruction', 'Violations_FailToReport',\n",
    "       'Violations_MoveWithoutPermission', 'Delinquency_Reports',\n",
    "       'Program_Attendances', 'Program_UnexcusedAbsences', 'Residence_Changes',\n",
    "       'Avg_Days_per_DrugTest', 'DrugTests_THC_Positive',\n",
    "       'DrugTests_Cocaine_Positive', 'DrugTests_Meth_Positive',\n",
    "       'DrugTests_Other_Positive', 'Percent_Days_Employed', 'Jobs_Per_Year',\n",
    "       'Employment_Exempt'])\n",
    "raw_extend=pd.merge(raw_train,PUMA_new,left_on='Residence_PUMA',right_on='Code')\n",
    "\n",
    "# Change the dtype of the feature from object to intagar\n",
    "raw_extend['TEN']=raw_extend['TEN'].astype('category')\n",
    "raw_extend['TEN']=raw_extend['TEN'].cat.codes\n",
    "raw_extend['Residence_PUMA']=raw_extend['Residence_PUMA'].astype('category')\n",
    "raw_extend['Residence_PUMA']=raw_extend['Residence_PUMA'].cat.codes\n",
    "raw_extend['Age_at_Release']=raw_extend['Age_at_Release'].apply(lambda x: int(x[:2]))\n",
    "raw_extend['Dependents']=raw_extend['Dependents'].apply(lambda x: int(x[:1]))\n",
    "raw_extend['Prior_Arrest_Episodes_Felony']=raw_extend['Prior_Arrest_Episodes_Felony'].apply(lambda x: int(x[:2]))\n",
    "raw_extend['Prior_Arrest_Episodes_Drug']=raw_extend['Prior_Arrest_Episodes_Drug'].apply(lambda x: int(x[:2]))\n",
    "raw_extend['Prior_Arrest_Episodes_Misd']=raw_extend['Prior_Arrest_Episodes_Misd'].apply(lambda x: int(x[:2]))\n",
    "raw_extend['Prior_Arrest_Episodes_Violent']=raw_extend['Prior_Arrest_Episodes_Violent'].apply(lambda x: int(x[:2]))\n",
    "raw_extend['Prior_Arrest_Episodes_Property']=raw_extend['Prior_Arrest_Episodes_Property'].apply(lambda x: int(x[:2]))\n",
    "raw_extend['Prior_Arrest_Episodes_PPViolationCharges']=raw_extend['Prior_Arrest_Episodes_PPViolationCharges'].apply(lambda x: int(x[:2]))\n",
    "raw_extend['Prior_Conviction_Episodes_Felony']=raw_extend['Prior_Conviction_Episodes_Felony'].apply(lambda x: int(x[:1]))\n",
    "raw_extend['Prior_Conviction_Episodes_Misd']=raw_extend['Prior_Conviction_Episodes_Misd'].apply(lambda x: int(x[:1]))\n",
    "raw_extend['Prior_Conviction_Episodes_Prop']=raw_extend['Prior_Conviction_Episodes_Prop'].apply(lambda x: int(x[:1]))\n",
    "raw_extend['Prior_Conviction_Episodes_Drug']=raw_extend['Prior_Conviction_Episodes_Drug'].apply(lambda x: int(x[:1]))\n",
    "#raw_extend['Delinquency_Reports']=raw_extend['Delinquency_Reports'].apply(lambda x: int(x[:1]))\n",
    "#raw_extend['Program_Attendances']=raw_extend['Program_Attendances'].apply(lambda x: int(x[:2]))\n",
    "#raw_extend['Program_UnexcusedAbsences']=raw_extend['Program_UnexcusedAbsences'].apply(lambda x: int(x[:1]))\n",
    "#raw_extend['Residence_Changes']=raw_extend['Residence_Changes'].apply(lambda x: int(x[:1]))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaling_set=[]\n",
    "for column in raw_extend.columns:\n",
    "    if raw_extend[column].dtype == object:\n",
    "        raw_extend[column]=raw_extend[column].astype('category')\n",
    "        raw_extend[column]=raw_extend[column].cat.codes\n",
    "    elif raw_extend[column].dtype in ['int64','float32','float64'] :\n",
    "        scaling_set+=[column]\n",
    "raw_extend[scaling_set]=scaler.fit_transform(raw_extend[scaling_set].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bff11118",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression train score: 0.7175504770357222 \n",
      " test score: 0.7106722875526958 \n",
      " Logistic regression train Brier score: 0.18691464638118652 \n",
      " test Brier score: 0.18810059465232717 \n",
      " AUROC: 0.7005194805194805\n",
      "Random forest train score: 1.0 \n",
      " test score: 0.7064566230308409 \n",
      " Random forest  train Brier score: 0.027115374438117824 \n",
      " test Brier score: 0.19288194660158314 \n",
      " AUROC: 0.676551578502798\n",
      "SGD best layer size: {'min_samples_split': 2, 'n_estimators': 100} \n",
      " best train score: -0.18858316159952715 \n",
      " test score: -0.18755638311823125 \n",
      " SGD  train Brier score: 0.1779068598468777 \n",
      " test Brier score: 0.18755638311823083 \n",
      " AUROC: 0.7048681941364869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\myjr\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "296/296 [==============================] - 1s 2ms/step - loss: 0.7612 - accuracy: 0.6052 - val_loss: 0.6051 - val_accuracy: 0.6993\n",
      "Epoch 2/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.6404 - accuracy: 0.6769 - val_loss: 0.6051 - val_accuracy: 0.6993\n",
      "Epoch 3/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.6106 - accuracy: 0.6953 - val_loss: 0.5992 - val_accuracy: 0.6993\n",
      "Epoch 4/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.6080 - accuracy: 0.6954 - val_loss: 0.5987 - val_accuracy: 0.6993\n",
      "Epoch 5/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.6019 - accuracy: 0.7019 - val_loss: 0.5955 - val_accuracy: 0.6993\n",
      "Epoch 6/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5981 - accuracy: 0.7012 - val_loss: 0.5932 - val_accuracy: 0.6993\n",
      "Epoch 7/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5934 - accuracy: 0.7022 - val_loss: 0.5890 - val_accuracy: 0.6993\n",
      "Epoch 8/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5950 - accuracy: 0.6985 - val_loss: 0.5901 - val_accuracy: 0.6993\n",
      "Epoch 9/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5875 - accuracy: 0.7029 - val_loss: 0.5838 - val_accuracy: 0.7003\n",
      "Epoch 10/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5869 - accuracy: 0.7028 - val_loss: 0.5808 - val_accuracy: 0.7000\n",
      "Epoch 11/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5860 - accuracy: 0.7018 - val_loss: 0.5791 - val_accuracy: 0.7022\n",
      "Epoch 12/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5864 - accuracy: 0.7010 - val_loss: 0.5781 - val_accuracy: 0.7013\n",
      "Epoch 13/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5834 - accuracy: 0.7038 - val_loss: 0.5774 - val_accuracy: 0.7025\n",
      "Epoch 14/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5795 - accuracy: 0.7042 - val_loss: 0.5759 - val_accuracy: 0.7025\n",
      "Epoch 15/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5774 - accuracy: 0.7041 - val_loss: 0.5737 - val_accuracy: 0.7047\n",
      "Epoch 16/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5810 - accuracy: 0.7055 - val_loss: 0.5723 - val_accuracy: 0.7069\n",
      "Epoch 17/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5746 - accuracy: 0.7052 - val_loss: 0.5712 - val_accuracy: 0.7074\n",
      "Epoch 18/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5731 - accuracy: 0.7085 - val_loss: 0.5691 - val_accuracy: 0.7084\n",
      "Epoch 19/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5745 - accuracy: 0.7022 - val_loss: 0.5695 - val_accuracy: 0.7057\n",
      "Epoch 20/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5771 - accuracy: 0.7017 - val_loss: 0.5693 - val_accuracy: 0.7074\n",
      "Epoch 21/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5741 - accuracy: 0.7038 - val_loss: 0.5693 - val_accuracy: 0.7106\n",
      "Epoch 22/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5719 - accuracy: 0.7020 - val_loss: 0.5685 - val_accuracy: 0.7111\n",
      "Epoch 23/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5720 - accuracy: 0.7049 - val_loss: 0.5681 - val_accuracy: 0.7143\n",
      "Epoch 24/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5690 - accuracy: 0.7073 - val_loss: 0.5670 - val_accuracy: 0.7148\n",
      "Epoch 25/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5711 - accuracy: 0.7071 - val_loss: 0.5681 - val_accuracy: 0.7064\n",
      "Epoch 26/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5706 - accuracy: 0.7086 - val_loss: 0.5668 - val_accuracy: 0.7106\n",
      "Epoch 27/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5697 - accuracy: 0.7052 - val_loss: 0.5663 - val_accuracy: 0.7114\n",
      "Epoch 28/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5683 - accuracy: 0.7096 - val_loss: 0.5648 - val_accuracy: 0.7126\n",
      "Epoch 29/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5669 - accuracy: 0.7097 - val_loss: 0.5658 - val_accuracy: 0.7109\n",
      "Epoch 30/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5684 - accuracy: 0.7088 - val_loss: 0.5650 - val_accuracy: 0.7128\n",
      "Epoch 31/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5691 - accuracy: 0.7053 - val_loss: 0.5646 - val_accuracy: 0.7153\n",
      "Epoch 32/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5652 - accuracy: 0.7079 - val_loss: 0.5647 - val_accuracy: 0.7121\n",
      "Epoch 33/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5628 - accuracy: 0.7040 - val_loss: 0.5641 - val_accuracy: 0.7141\n",
      "Epoch 34/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5663 - accuracy: 0.7078 - val_loss: 0.5661 - val_accuracy: 0.7121\n",
      "Epoch 35/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5656 - accuracy: 0.7075 - val_loss: 0.5640 - val_accuracy: 0.7136\n",
      "Epoch 36/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5646 - accuracy: 0.7066 - val_loss: 0.5641 - val_accuracy: 0.7143\n",
      "Epoch 37/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5668 - accuracy: 0.7072 - val_loss: 0.5651 - val_accuracy: 0.7109\n",
      "Epoch 38/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5650 - accuracy: 0.7064 - val_loss: 0.5641 - val_accuracy: 0.7136\n",
      "Epoch 39/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5647 - accuracy: 0.7086 - val_loss: 0.5647 - val_accuracy: 0.7136\n",
      "Epoch 40/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5637 - accuracy: 0.7108 - val_loss: 0.5625 - val_accuracy: 0.7131\n",
      "Epoch 41/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5638 - accuracy: 0.7096 - val_loss: 0.5630 - val_accuracy: 0.7141\n",
      "Epoch 42/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5654 - accuracy: 0.7074 - val_loss: 0.5641 - val_accuracy: 0.7143\n",
      "Epoch 43/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5661 - accuracy: 0.7044 - val_loss: 0.5641 - val_accuracy: 0.7138\n",
      "Epoch 44/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5619 - accuracy: 0.7086 - val_loss: 0.5635 - val_accuracy: 0.7126\n",
      "Epoch 45/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5627 - accuracy: 0.7085 - val_loss: 0.5639 - val_accuracy: 0.7143\n",
      "Epoch 46/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5637 - accuracy: 0.7091 - val_loss: 0.5649 - val_accuracy: 0.7121\n",
      "Epoch 47/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5637 - accuracy: 0.7077 - val_loss: 0.5635 - val_accuracy: 0.7156\n",
      "Epoch 48/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5639 - accuracy: 0.7061 - val_loss: 0.5660 - val_accuracy: 0.7079\n",
      "Epoch 49/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5639 - accuracy: 0.7089 - val_loss: 0.5631 - val_accuracy: 0.7156\n",
      "Epoch 50/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5631 - accuracy: 0.7057 - val_loss: 0.5644 - val_accuracy: 0.7143\n",
      "Epoch 51/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5622 - accuracy: 0.7098 - val_loss: 0.5634 - val_accuracy: 0.7163\n",
      "Epoch 52/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5626 - accuracy: 0.7091 - val_loss: 0.5639 - val_accuracy: 0.7158\n",
      "Epoch 53/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5628 - accuracy: 0.7104 - val_loss: 0.5634 - val_accuracy: 0.7148\n",
      "Epoch 54/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5625 - accuracy: 0.7106 - val_loss: 0.5626 - val_accuracy: 0.7156\n",
      "Epoch 55/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5633 - accuracy: 0.7058 - val_loss: 0.5636 - val_accuracy: 0.7133\n",
      "Epoch 56/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5630 - accuracy: 0.7066 - val_loss: 0.5633 - val_accuracy: 0.7168\n",
      "Epoch 57/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5632 - accuracy: 0.7070 - val_loss: 0.5635 - val_accuracy: 0.7151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5631 - accuracy: 0.7074 - val_loss: 0.5650 - val_accuracy: 0.7133\n",
      "Epoch 59/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5595 - accuracy: 0.7100 - val_loss: 0.5637 - val_accuracy: 0.7138\n",
      "Epoch 60/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5627 - accuracy: 0.7061 - val_loss: 0.5633 - val_accuracy: 0.7173\n",
      "Epoch 61/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5619 - accuracy: 0.7100 - val_loss: 0.5628 - val_accuracy: 0.7143\n",
      "Epoch 62/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5605 - accuracy: 0.7092 - val_loss: 0.5632 - val_accuracy: 0.7148\n",
      "Epoch 63/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5634 - accuracy: 0.7084 - val_loss: 0.5632 - val_accuracy: 0.7165\n",
      "Epoch 64/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5620 - accuracy: 0.7091 - val_loss: 0.5644 - val_accuracy: 0.7128\n",
      "Epoch 65/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5576 - accuracy: 0.7124 - val_loss: 0.5625 - val_accuracy: 0.7163\n",
      "Epoch 66/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5617 - accuracy: 0.7088 - val_loss: 0.5628 - val_accuracy: 0.7148\n",
      "Epoch 67/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5609 - accuracy: 0.7113 - val_loss: 0.5630 - val_accuracy: 0.7168\n",
      "Epoch 68/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5637 - accuracy: 0.7106 - val_loss: 0.5639 - val_accuracy: 0.7188\n",
      "Epoch 69/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5608 - accuracy: 0.7104 - val_loss: 0.5623 - val_accuracy: 0.7165\n",
      "Epoch 70/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5631 - accuracy: 0.7069 - val_loss: 0.5635 - val_accuracy: 0.7165\n",
      "Epoch 71/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5608 - accuracy: 0.7120 - val_loss: 0.5614 - val_accuracy: 0.7153\n",
      "Epoch 72/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5610 - accuracy: 0.7114 - val_loss: 0.5618 - val_accuracy: 0.7170\n",
      "Epoch 73/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5589 - accuracy: 0.7139 - val_loss: 0.5620 - val_accuracy: 0.7185\n",
      "Epoch 74/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5622 - accuracy: 0.7108 - val_loss: 0.5625 - val_accuracy: 0.7168\n",
      "Epoch 75/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5597 - accuracy: 0.7095 - val_loss: 0.5633 - val_accuracy: 0.7153\n",
      "Epoch 76/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5631 - accuracy: 0.7122 - val_loss: 0.5622 - val_accuracy: 0.7170\n",
      "Epoch 77/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5590 - accuracy: 0.7132 - val_loss: 0.5654 - val_accuracy: 0.7143\n",
      "Epoch 78/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5600 - accuracy: 0.7084 - val_loss: 0.5636 - val_accuracy: 0.7158\n",
      "Epoch 79/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5616 - accuracy: 0.7081 - val_loss: 0.5631 - val_accuracy: 0.7165\n",
      "Epoch 80/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5590 - accuracy: 0.7107 - val_loss: 0.5622 - val_accuracy: 0.7153\n",
      "Epoch 81/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5591 - accuracy: 0.7104 - val_loss: 0.5629 - val_accuracy: 0.7160\n",
      "Epoch 82/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5579 - accuracy: 0.7139 - val_loss: 0.5627 - val_accuracy: 0.7156\n",
      "Epoch 83/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5596 - accuracy: 0.7093 - val_loss: 0.5629 - val_accuracy: 0.7158\n",
      "Epoch 84/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5573 - accuracy: 0.7156 - val_loss: 0.5626 - val_accuracy: 0.7163\n",
      "Epoch 85/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5590 - accuracy: 0.7110 - val_loss: 0.5641 - val_accuracy: 0.7158\n",
      "Epoch 86/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5585 - accuracy: 0.7101 - val_loss: 0.5640 - val_accuracy: 0.7141\n",
      "Epoch 87/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5588 - accuracy: 0.7135 - val_loss: 0.5628 - val_accuracy: 0.7141\n",
      "Epoch 88/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5596 - accuracy: 0.7111 - val_loss: 0.5629 - val_accuracy: 0.7151\n",
      "Epoch 89/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5600 - accuracy: 0.7086 - val_loss: 0.5658 - val_accuracy: 0.7106\n",
      "Epoch 90/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5601 - accuracy: 0.7091 - val_loss: 0.5629 - val_accuracy: 0.7141\n",
      "Epoch 91/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5610 - accuracy: 0.7086 - val_loss: 0.5658 - val_accuracy: 0.7099\n",
      "Epoch 92/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5600 - accuracy: 0.7114 - val_loss: 0.5630 - val_accuracy: 0.7170\n",
      "Epoch 93/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5599 - accuracy: 0.7130 - val_loss: 0.5649 - val_accuracy: 0.7131\n",
      "Epoch 94/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5627 - accuracy: 0.7064 - val_loss: 0.5648 - val_accuracy: 0.7114\n",
      "Epoch 95/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5565 - accuracy: 0.7111 - val_loss: 0.5633 - val_accuracy: 0.7148\n",
      "Epoch 96/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5591 - accuracy: 0.7120 - val_loss: 0.5627 - val_accuracy: 0.7128\n",
      "Epoch 97/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5592 - accuracy: 0.7094 - val_loss: 0.5641 - val_accuracy: 0.7163\n",
      "Epoch 98/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5568 - accuracy: 0.7141 - val_loss: 0.5633 - val_accuracy: 0.7143\n",
      "Epoch 99/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5564 - accuracy: 0.7131 - val_loss: 0.5624 - val_accuracy: 0.7146\n",
      "Epoch 100/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5588 - accuracy: 0.7107 - val_loss: 0.5623 - val_accuracy: 0.7153\n",
      "Epoch 101/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5565 - accuracy: 0.7098 - val_loss: 0.5639 - val_accuracy: 0.7146\n",
      "Epoch 102/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5581 - accuracy: 0.7131 - val_loss: 0.5633 - val_accuracy: 0.7131\n",
      "Epoch 103/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5587 - accuracy: 0.7100 - val_loss: 0.5658 - val_accuracy: 0.7138\n",
      "Epoch 104/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5581 - accuracy: 0.7131 - val_loss: 0.5630 - val_accuracy: 0.7153\n",
      "Epoch 105/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5576 - accuracy: 0.7119 - val_loss: 0.5631 - val_accuracy: 0.7173\n",
      "Epoch 106/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5596 - accuracy: 0.7102 - val_loss: 0.5626 - val_accuracy: 0.7153\n",
      "Epoch 107/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5585 - accuracy: 0.7106 - val_loss: 0.5625 - val_accuracy: 0.7180\n",
      "Epoch 108/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5585 - accuracy: 0.7104 - val_loss: 0.5621 - val_accuracy: 0.7148\n",
      "Epoch 109/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5575 - accuracy: 0.7149 - val_loss: 0.5629 - val_accuracy: 0.7156\n",
      "Epoch 110/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5602 - accuracy: 0.7105 - val_loss: 0.5628 - val_accuracy: 0.7168\n",
      "Epoch 111/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5571 - accuracy: 0.7116 - val_loss: 0.5640 - val_accuracy: 0.7165\n",
      "Epoch 112/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5590 - accuracy: 0.7082 - val_loss: 0.5628 - val_accuracy: 0.7160\n",
      "Epoch 113/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5596 - accuracy: 0.7095 - val_loss: 0.5637 - val_accuracy: 0.7163\n",
      "Epoch 114/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5584 - accuracy: 0.7144 - val_loss: 0.5629 - val_accuracy: 0.7170\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5591 - accuracy: 0.7113 - val_loss: 0.5621 - val_accuracy: 0.7165\n",
      "Epoch 116/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5583 - accuracy: 0.7119 - val_loss: 0.5621 - val_accuracy: 0.7180\n",
      "Epoch 117/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5548 - accuracy: 0.7134 - val_loss: 0.5626 - val_accuracy: 0.7165\n",
      "Epoch 118/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5602 - accuracy: 0.7120 - val_loss: 0.5632 - val_accuracy: 0.7193\n",
      "Epoch 119/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5591 - accuracy: 0.7102 - val_loss: 0.5628 - val_accuracy: 0.7175\n",
      "Epoch 120/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5579 - accuracy: 0.7119 - val_loss: 0.5635 - val_accuracy: 0.7158\n",
      "Epoch 121/200\n",
      "296/296 [==============================] - 0s 1ms/step - loss: 0.5572 - accuracy: 0.7147 - val_loss: 0.5630 - val_accuracy: 0.7168\n",
      "423/423 [==============================] - 0s 1ms/step - loss: 0.5636 - accuracy: 0.7134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\myjr\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:430: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
      "  warnings.warn('`model.predict_proba()` is deprecated and '\n",
      "C:\\Users\\myjr\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:430: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
      "  warnings.warn('`model.predict_proba()` is deprecated and '\n",
      "C:\\Users\\myjr\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:430: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
      "  warnings.warn('`model.predict_proba()` is deprecated and '\n",
      "C:\\Users\\myjr\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:430: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
      "  warnings.warn('`model.predict_proba()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP train Brier score: 0.1865408403094302 \n",
      " test Brier score: 0.18917124704734167 \n",
      " AUROC: 0.6985588756320464\n",
      "Xgboost best layer size: {'colsample_bytree': 0.8, 'max_depth': 3, 'min_child_weight': 6, 'n_estimators': 100, 'reg_lambda': 1000, 'subsample': 0.9} \n",
      " best train score: -0.18811904238606814 \n",
      " test score: -0.1877925517419035 \n",
      " Xgboost train Brier score: 0.18134933853812088 \n",
      " test Brier score: 0.18779255174583864 \n",
      " AUROC: 0.7040091976677343\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression,LogisticRegression\n",
    "\n",
    "\n",
    "X_train1,X_test1,y_train1,y_test1=train_test_split(raw_extend.drop(['ID','Recidivism_Within_3years','Recidivism_Arrest_Year1','Recidivism_Arrest_Year2','Recidivism_Arrest_Year3'],axis=1),raw_extend['Recidivism_Arrest_Year1'])\n",
    "fpr_list,tpr_list,auc_list=dict(),dict(),dict()\n",
    "\n",
    "\n",
    "logistic=LogisticRegression(max_iter=1000)\n",
    "logistic.fit(X_train1.fillna(0),y_train1)\n",
    "fpr_list[0], tpr_list[0], _ = roc_curve(y_test1, y_roc(logistic,X_test1.fillna(0)))\n",
    "print('Logistic regression train score:',\n",
    "      logistic.score(X_train1.fillna(0),y_train1),'\\n test score:',logistic.score(X_test1.fillna(0),y_test1.fillna(0)),'\\n Logistic regression train Brier score:',\n",
    "      brier_score(logistic.predict_proba(X_train1.fillna(0)),y_train1),'\\n test Brier score:',brier_score(logistic.predict_proba(X_test1.fillna(0)),y_test1),\n",
    "     '\\n AUROC:',roc_auc_score(y_test1, y_roc(logistic,X_test1.fillna(0))))\n",
    "\n",
    "RF=RandomForestClassifier(n_estimators=150,min_samples_split=2)\n",
    "RF.fit(X_train1.fillna(0),y_train1)\n",
    "fpr_list[1], tpr_list[1], _ = roc_curve(y_test1, y_roc(RF,X_test1.fillna(0)))\n",
    "print('Random forest train score:',\n",
    "      RF.score(X_train1.fillna(0),y_train1),'\\n test score:',RF.score(X_test1.fillna(0),y_test1.fillna(0)),'\\n Random forest  train Brier score:',\n",
    "      brier_score(RF.predict_proba(X_train1.fillna(0)),y_train1),'\\n test Brier score:',brier_score(RF.predict_proba(X_test1.fillna(0)),y_test1),\n",
    "     '\\n AUROC:',roc_auc_score(y_test1, y_roc(RF,X_test1.fillna(0))))\n",
    "\n",
    "GBDT=GradientBoostingClassifier()\n",
    "params_SGD={'n_estimators':[150,100],'min_samples_split':[2,4]}\n",
    "grid_SGD=GridSearchCV(GBDT,param_grid=params_SGD, scoring='neg_brier_score',cv=3)\n",
    "grid_SGD.fit(X_train1.fillna(0),y_train1.fillna(0))\n",
    "fpr_list[2], tpr_list[2], _ = roc_curve(y_test1, y_roc(grid_SGD.best_estimator_,X_test1.fillna(0)))\n",
    "print('SGD best layer size:',grid_SGD.best_params_,'\\n best train score:',\n",
    "      grid_SGD.best_score_,'\\n test score:',grid_SGD.score(X_test1.fillna(0),y_test1.fillna(0)),'\\n SGD  train Brier score:',\n",
    "      brier_score(grid_SGD.predict_proba(X_train1.fillna(0)),y_train1),'\\n test Brier score:',brier_score(grid_SGD.predict_proba(X_test1.fillna(0)),y_test1),\n",
    "     '\\n AUROC:',roc_auc_score(y_test1, y_roc(grid_SGD.best_estimator_,X_test1.fillna(0))))\n",
    "\n",
    "pipe = Sequential()\n",
    "n_cols = X_train1.shape[1]\n",
    "pipe.add(BatchNormalization(input_shape=(n_cols,)))\n",
    "pipe.add(Dense(70, activation= 'linear'))\n",
    "pipe.add(Dropout(0.3))\n",
    "pipe.add(Dense(50, activation= 'relu'))\n",
    "pipe.add(Dropout(0.3))\n",
    "pipe.add(Dense(50, activation= 'relu'))\n",
    "pipe.add(Dropout(0.3))\n",
    "#model.add(Dense(100, activation='relu'))\n",
    "#model.add(Dropout(0.3))\n",
    "#model.add(Dense(100, activation='linear'))\n",
    "pipe.add(BatchNormalization())\n",
    "pipe.add(Dense(2, activation='softmax'))\n",
    "    #model.compile(\n",
    "        #optimizer='Adam',\n",
    "        #loss='mean_squared_error',\n",
    "        #metrics=['accuracy'])\n",
    "early_stopping_monitor = EarlyStopping(patience=50)\n",
    "sgd = keras.optimizers.SGD(lr=.001, decay=2e-4, momentum=0.9, nesterov=True)\n",
    "pipe.compile(loss = 'sparse_categorical_crossentropy',optimizer = 'sgd', metrics=['accuracy'])\n",
    "history=pipe.fit(X_train1.fillna(0).astype('float32'), y_train1, validation_split=0.3, epochs=200, callbacks=[early_stopping_monitor])\n",
    "#history=model.fit(X_train, y_train, validation_split=0.2, epochs=25)\n",
    "score = pipe.evaluate(X_test1.fillna(0).astype('float32'), y_test1, verbose=0)\n",
    "pipe.fit(X_train1.fillna(0).astype('float32'),y_train1.fillna(0))\n",
    "fpr_list[3], tpr_list[3], _ = roc_curve(y_test1, y_roc(pipe,X_test1.fillna(0).astype('float32')))\n",
    "print('MLP train Brier score:',\n",
    "      brier_score(pipe.predict_proba(X_train1.fillna(0).astype('float32')),y_train1),'\\n test Brier score:',brier_score(pipe.predict_proba(X_test1.fillna(0).astype('float32')),y_test1),\n",
    "     '\\n AUROC:',roc_auc_score(y_test1, y_roc(pipe,X_test1.fillna(0).astype('float32'))))\n",
    "\n",
    "\n",
    "XGB=XGBClassifier()\n",
    "params_XGB={    'n_estimators': [50, 100],\n",
    "    'max_depth': [2, 3],\n",
    "    'min_child_weight': [2,4,6],\n",
    "    'subsample': [0.9],\n",
    "    'colsample_bytree': [0.8],'reg_lambda':[1000]}\n",
    "grid_XGB=GridSearchCV(XGB,param_grid=params_XGB, scoring='neg_brier_score',cv=3)\n",
    "grid_XGB.fit(X_train1.fillna(0),y_train1.fillna(0))\n",
    "fpr_list[4], tpr_list[4], _ = roc_curve(y_test1, y_roc(grid_XGB.best_estimator_,X_test1.fillna(0)))\n",
    "print('Xgboost best layer size:',grid_XGB.best_params_,'\\n best train score:',\n",
    "      grid_XGB.best_score_,'\\n test score:',grid_XGB.score(X_test1.fillna(0),y_test1.fillna(0)),'\\n Xgboost train Brier score:',\n",
    "      brier_score(grid_XGB.predict_proba(X_train1.fillna(0)),y_train1),'\\n test Brier score:',brier_score(grid_XGB.predict_proba(X_test1.fillna(0)),y_test1),\n",
    "     '\\n AUROC:',roc_auc_score(y_test1, y_roc(grid_XGB.best_estimator_,X_test1.fillna(0))))\n",
    "\n",
    "\n",
    "\n",
    "colors = cycle(['aqua', 'red', 'green','orange','blue'])\n",
    "labels=['Logistic Regression','Random Forest','Gradient Boosting','MLP','Xgboost']\n",
    "for i, label, color in zip(range(len(fpr_list)), labels, colors):\n",
    "    legend= label + ' (area = {1:0.2f})'''.format(i, auc(fpr_list[i], tpr_list[i]))\n",
    "    plt.plot(fpr_list[i], tpr_list[i], color=color, lw=2,label=legend)\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC curve for different methods on Recidivism_1Year')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('ROC_plot.png',dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0a50cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "GBDT_importance=pd.DataFrame()\n",
    "GBDT_importance['Feature']=raw_extend.drop(['ID','Recidivism_Within_3years','Recidivism_Arrest_Year1','Recidivism_Arrest_Year2','Recidivism_Arrest_Year3'],axis=1).columns\n",
    "GBDT_importance['importance']=grid_SGD.best_estimator_.feature_importances_\n",
    "GBDT_importance=GBDT_importance.sort_values(by='importance',ascending=False)\n",
    "GBDT_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a51f72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_extend=raw_extend.drop(list(GBDT_importance[GBDT_importance.importance==0].Feature),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc332f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression,LogisticRegression\n",
    "\n",
    "\n",
    "X_train1,X_test1,y_train1,y_test1=train_test_split(raw_extend.drop(['ID','Recidivism_Within_3years','Recidivism_Arrest_Year1','Recidivism_Arrest_Year2','Recidivism_Arrest_Year3'],axis=1),raw_extend['Recidivism_Arrest_Year1'])\n",
    "fpr_list,tpr_list,auc_list=dict(),dict(),dict()\n",
    "\n",
    "\n",
    "logistic_t=LogisticRegression(max_iter=1000)\n",
    "logistic_t.fit(X_train1.fillna(0),y_train1)\n",
    "fpr_list[0], tpr_list[0], _ = roc_curve(y_test1, y_roc(logistic_t,X_test1.fillna(0)))\n",
    "print('logistic_t regression train score:',\n",
    "      logistic_t.score(X_train1.fillna(0),y_train1),'\\n test score:',logistic_t.score(X_test1.fillna(0),y_test1.fillna(0)),'\\n logistic_t regression train Brier score:',\n",
    "      brier_score(logistic_t.predict_proba(X_train1.fillna(0)),y_train1),'\\n test Brier score:',brier_score(logistic_t.predict_proba(X_test1.fillna(0)),y_test1),\n",
    "     '\\n AUROC:',roc_auc_score(y_test1, y_roc(logistic_t,X_test1.fillna(0))))\n",
    "\n",
    "RF=RandomForestClassifier(n_estimators=150,min_samples_split=2)\n",
    "RF.fit(X_train1.fillna(0),y_train1)\n",
    "fpr_list[1], tpr_list[1], _ = roc_curve(y_test1, y_roc(RF,X_test1.fillna(0)))\n",
    "print('Random forest train score:',\n",
    "      RF.score(X_train1.fillna(0),y_train1),'\\n test score:',RF.score(X_test1.fillna(0),y_test1.fillna(0)),'\\n Random forest  train Brier score:',\n",
    "      brier_score(RF.predict_proba(X_train1.fillna(0)),y_train1),'\\n test Brier score:',brier_score(RF.predict_proba(X_test1.fillna(0)),y_test1),\n",
    "     '\\n AUROC:',roc_auc_score(y_test1, y_roc(RF,X_test1.fillna(0))))\n",
    "\n",
    "GBDT=GradientBoostingClassifier()\n",
    "params_SGD={'n_estimators':[150,100],'min_samples_split':[2,4]}\n",
    "grid_SGD_t=GridSearchCV(GBDT,param_grid=params_SGD, scoring='neg_brier_score',cv=3)\n",
    "grid_SGD_t.fit(X_train1.fillna(0),y_train1.fillna(0))\n",
    "fpr_list[2], tpr_list[2], _ = roc_curve(y_test1, y_roc(grid_SGD_t.best_estimator_,X_test1.fillna(0)))\n",
    "print('SGD best layer size:',grid_SGD_t.best_params_,'\\n best train score:',\n",
    "      grid_SGD_t.best_score_,'\\n test score:',grid_SGD_t.score(X_test1.fillna(0),y_test1.fillna(0)),'\\n SGD  train Brier score:',\n",
    "      brier_score(grid_SGD_t.predict_proba(X_train1.fillna(0)),y_train1),'\\n test Brier score:',brier_score(grid_SGD_t.predict_proba(X_test1.fillna(0)),y_test1),\n",
    "     '\\n AUROC:',roc_auc_score(y_test1, y_roc(grid_SGD_t.best_estimator_,X_test1.fillna(0))))\n",
    "\n",
    "pipe_t = Sequential()\n",
    "n_cols = X_train1.shape[1]\n",
    "pipe_t.add(BatchNormalization(input_shape=(n_cols,)))\n",
    "pipe_t.add(Dense(70, activation= 'linear'))\n",
    "pipe_t.add(Dropout(0.3))\n",
    "pipe_t.add(Dense(50, activation= 'relu'))\n",
    "pipe_t.add(Dropout(0.3))\n",
    "pipe_t.add(Dense(50, activation= 'relu'))\n",
    "pipe_t.add(Dropout(0.3))\n",
    "#model.add(Dense(100, activation='relu'))\n",
    "#model.add(Dropout(0.3))\n",
    "#model.add(Dense(100, activation='linear'))\n",
    "pipe_t.add(BatchNormalization())\n",
    "pipe_t.add(Dense(2, activation='softmax'))\n",
    "    #model.compile(\n",
    "        #optimizer='Adam',\n",
    "        #loss='mean_squared_error',\n",
    "        #metrics=['accuracy'])\n",
    "early_stopping_monitor = EarlyStopping(patience=50)\n",
    "sgd = keras.optimizers.SGD(lr=.001, decay=2e-4, momentum=0.9, nesterov=True)\n",
    "pipe_t.compile(loss = 'sparse_categorical_crossentropy',optimizer = 'sgd', metrics=['accuracy'])\n",
    "history=pipe_t.fit(X_train1.fillna(0).astype('float32'), y_train1, validation_split=0.3, epochs=200, callbacks=[early_stopping_monitor])\n",
    "#history=model.fit(X_train, y_train, validation_split=0.2, epochs=25)\n",
    "score = pipe_t.evaluate(X_test1.fillna(0).astype('float32'), y_test1, verbose=0)\n",
    "pipe_t.fit(X_train1.fillna(0).astype('float32'),y_train1.fillna(0))\n",
    "fpr_list[3], tpr_list[3], _ = roc_curve(y_test1, y_roc(pipe_t,X_test1.fillna(0).astype('float32')))\n",
    "print('MLP train Brier score:',\n",
    "      brier_score(pipe_t.predict_proba(X_train1.fillna(0).astype('float32')),y_train1),'\\n test Brier score:',brier_score(pipe_t.predict_proba(X_test1.fillna(0).astype('float32')),y_test1),\n",
    "     '\\n AUROC:',roc_auc_score(y_test1, y_roc(pipe_t,X_test1.fillna(0).astype('float32'))))\n",
    "\n",
    "\n",
    "XGB=XGBClassifier()\n",
    "params_XGB={    'n_estimators': [50, 100],\n",
    "    'max_depth': [2, 3],\n",
    "    'min_child_weight': [2,4,6],\n",
    "    'subsample': [0.9],\n",
    "    'colsample_bytree': [0.8],'reg_lambda':[1000]}\n",
    "grid_XGB_t=GridSearchCV(XGB,param_grid=params_XGB, scoring='neg_brier_score',cv=3)\n",
    "grid_XGB_t.fit(X_train1.fillna(0),y_train1.fillna(0))\n",
    "fpr_list[4], tpr_list[4], _ = roc_curve(y_test1, y_roc(grid_XGB_t.best_estimator_,X_test1.fillna(0)))\n",
    "print('Xgboost best layer size:',grid_XGB_t.best_params_,'\\n best train score:',\n",
    "      grid_XGB_t.best_score_,'\\n test score:',grid_XGB_t.score(X_test1.fillna(0),y_test1.fillna(0)),'\\n Xgboost train Brier score:',\n",
    "      brier_score(grid_XGB_t.predict_proba(X_train1.fillna(0)),y_train1),'\\n test Brier score:',brier_score(grid_XGB_t.predict_proba(X_test1.fillna(0)),y_test1),\n",
    "     '\\n AUROC:',roc_auc_score(y_test1, y_roc(grid_XGB_t.best_estimator_,X_test1.fillna(0))))\n",
    "\n",
    "\n",
    "\n",
    "colors = cycle(['aqua', 'red', 'green','orange','blue'])\n",
    "labels=['logistic_t Regression','Random Forest','Gradient Boosting','MLP','Xgboost']\n",
    "for i, label, color in zip(range(len(fpr_list)), labels, colors):\n",
    "    legend= label + ' (area = {1:0.2f})'''.format(i, auc(fpr_list[i], tpr_list[i]))\n",
    "    plt.plot(fpr_list[i], tpr_list[i], color=color, lw=2,label=legend)\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC curve for different methods on Recidivism_1Year')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('ROC_plot.png',dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce7551b",
   "metadata": {},
   "source": [
    "### Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92f75c42",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Race</th>\n",
       "      <th>Age_at_Release</th>\n",
       "      <th>Residence_PUMA</th>\n",
       "      <th>Gang_Affiliated</th>\n",
       "      <th>Supervision_Risk_Score_First</th>\n",
       "      <th>Supervision_Level_First</th>\n",
       "      <th>Education_Level</th>\n",
       "      <th>Dependents</th>\n",
       "      <th>...</th>\n",
       "      <th>Total\\nofficers</th>\n",
       "      <th>Total\\ncivilians</th>\n",
       "      <th>Murder</th>\n",
       "      <th>Rape</th>\n",
       "      <th>Robbery</th>\n",
       "      <th>Assault</th>\n",
       "      <th>Burglary</th>\n",
       "      <th>Larceny</th>\n",
       "      <th>Vehicle Theft</th>\n",
       "      <th>PUMA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>M</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>38</td>\n",
       "      <td>16</td>\n",
       "      <td>False</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Standard</td>\n",
       "      <td>High School Diploma</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>197.000000</td>\n",
       "      <td>37.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>329.000000</td>\n",
       "      <td>806.000000</td>\n",
       "      <td>2263.000000</td>\n",
       "      <td>264.000000</td>\n",
       "      <td>2900.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>M</td>\n",
       "      <td>BLACK</td>\n",
       "      <td>38</td>\n",
       "      <td>15</td>\n",
       "      <td>False</td>\n",
       "      <td>5.0</td>\n",
       "      <td>High</td>\n",
       "      <td>High School Diploma</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>111.750000</td>\n",
       "      <td>24.00000</td>\n",
       "      <td>1.50000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>36.750000</td>\n",
       "      <td>158.250000</td>\n",
       "      <td>541.250000</td>\n",
       "      <td>1707.750000</td>\n",
       "      <td>150.750000</td>\n",
       "      <td>3000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>M</td>\n",
       "      <td>BLACK</td>\n",
       "      <td>33</td>\n",
       "      <td>15</td>\n",
       "      <td>False</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Specialized</td>\n",
       "      <td>High School Diploma</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>111.750000</td>\n",
       "      <td>24.00000</td>\n",
       "      <td>1.50000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>36.750000</td>\n",
       "      <td>158.250000</td>\n",
       "      <td>541.250000</td>\n",
       "      <td>1707.750000</td>\n",
       "      <td>150.750000</td>\n",
       "      <td>3000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>M</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>33</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Standard</td>\n",
       "      <td>Less than HS diploma</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>151.500000</td>\n",
       "      <td>35.50000</td>\n",
       "      <td>9.50000</td>\n",
       "      <td>33.500000</td>\n",
       "      <td>148.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>1261.500000</td>\n",
       "      <td>3150.000000</td>\n",
       "      <td>358.000000</td>\n",
       "      <td>1700.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16</td>\n",
       "      <td>M</td>\n",
       "      <td>BLACK</td>\n",
       "      <td>33</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Standard</td>\n",
       "      <td>Less than HS diploma</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>205.511905</td>\n",
       "      <td>74.68254</td>\n",
       "      <td>11.95873</td>\n",
       "      <td>34.633333</td>\n",
       "      <td>282.034921</td>\n",
       "      <td>421.737302</td>\n",
       "      <td>1394.621429</td>\n",
       "      <td>3936.879365</td>\n",
       "      <td>579.465873</td>\n",
       "      <td>2602.646825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7802</th>\n",
       "      <td>26746</td>\n",
       "      <td>M</td>\n",
       "      <td>BLACK</td>\n",
       "      <td>43</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Standard</td>\n",
       "      <td>At least some college</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>233.000000</td>\n",
       "      <td>592.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>2002.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7803</th>\n",
       "      <td>26747</td>\n",
       "      <td>M</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>33</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Standard</td>\n",
       "      <td>At least some college</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>306.000000</td>\n",
       "      <td>63.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>348.000000</td>\n",
       "      <td>1865.000000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>4100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7804</th>\n",
       "      <td>26749</td>\n",
       "      <td>M</td>\n",
       "      <td>BLACK</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Standard</td>\n",
       "      <td>High School Diploma</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>203.000000</td>\n",
       "      <td>56.50000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>14.500000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>285.000000</td>\n",
       "      <td>883.500000</td>\n",
       "      <td>1903.000000</td>\n",
       "      <td>213.500000</td>\n",
       "      <td>4300.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7805</th>\n",
       "      <td>26752</td>\n",
       "      <td>M</td>\n",
       "      <td>BLACK</td>\n",
       "      <td>28</td>\n",
       "      <td>6</td>\n",
       "      <td>False</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Standard</td>\n",
       "      <td>At least some college</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>24.00000</td>\n",
       "      <td>1.80000</td>\n",
       "      <td>5.800000</td>\n",
       "      <td>25.400000</td>\n",
       "      <td>117.400000</td>\n",
       "      <td>292.800000</td>\n",
       "      <td>829.800000</td>\n",
       "      <td>41.200000</td>\n",
       "      <td>140.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7806</th>\n",
       "      <td>26755</td>\n",
       "      <td>M</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>33</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Standard</td>\n",
       "      <td>At least some college</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>151.500000</td>\n",
       "      <td>35.50000</td>\n",
       "      <td>9.50000</td>\n",
       "      <td>33.500000</td>\n",
       "      <td>148.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>1261.500000</td>\n",
       "      <td>3150.000000</td>\n",
       "      <td>358.000000</td>\n",
       "      <td>1700.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7807 rows × 186 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID Gender   Race  Age_at_Release  Residence_PUMA Gang_Affiliated  \\\n",
       "0         6      M  WHITE              38              16           False   \n",
       "1         8      M  BLACK              38              15           False   \n",
       "2        12      M  BLACK              33              15           False   \n",
       "3        15      M  WHITE              33               4           False   \n",
       "4        16      M  BLACK              33               2           False   \n",
       "...     ...    ...    ...             ...             ...             ...   \n",
       "7802  26746      M  BLACK              43               5           False   \n",
       "7803  26747      M  WHITE              33               7           False   \n",
       "7804  26749      M  BLACK              28               1           False   \n",
       "7805  26752      M  BLACK              28               6           False   \n",
       "7806  26755      M  WHITE              33               4           False   \n",
       "\n",
       "      Supervision_Risk_Score_First Supervision_Level_First  \\\n",
       "0                              5.0                Standard   \n",
       "1                              5.0                    High   \n",
       "2                              5.0             Specialized   \n",
       "3                              7.0                Standard   \n",
       "4                              4.0                Standard   \n",
       "...                            ...                     ...   \n",
       "7802                           5.0                Standard   \n",
       "7803                           5.0                Standard   \n",
       "7804                           5.0                Standard   \n",
       "7805                           5.0                Standard   \n",
       "7806                           5.0                Standard   \n",
       "\n",
       "            Education_Level  Dependents  ... Total\\nofficers Total\\ncivilians  \\\n",
       "0       High School Diploma           0  ...      197.000000         37.00000   \n",
       "1       High School Diploma           3  ...      111.750000         24.00000   \n",
       "2       High School Diploma           3  ...      111.750000         24.00000   \n",
       "3      Less than HS diploma           1  ...      151.500000         35.50000   \n",
       "4      Less than HS diploma           3  ...      205.511905         74.68254   \n",
       "...                     ...         ...  ...             ...              ...   \n",
       "7802  At least some college           3  ...       31.000000          5.00000   \n",
       "7803  At least some college           3  ...      306.000000         63.00000   \n",
       "7804    High School Diploma           3  ...      203.000000         56.50000   \n",
       "7805  At least some college           3  ...       41.000000         24.00000   \n",
       "7806  At least some college           3  ...      151.500000         35.50000   \n",
       "\n",
       "        Murder       Rape     Robbery     Assault     Burglary      Larceny  \\\n",
       "0      0.00000  13.000000   32.000000  329.000000   806.000000  2263.000000   \n",
       "1      1.50000  15.000000   36.750000  158.250000   541.250000  1707.750000   \n",
       "2      1.50000  15.000000   36.750000  158.250000   541.250000  1707.750000   \n",
       "3      9.50000  33.500000  148.000000  208.000000  1261.500000  3150.000000   \n",
       "4     11.95873  34.633333  282.034921  421.737302  1394.621429  3936.879365   \n",
       "...        ...        ...         ...         ...          ...          ...   \n",
       "7802   1.00000   3.000000    9.000000  118.000000   233.000000   592.000000   \n",
       "7803   1.00000  13.000000   20.000000   31.000000   348.000000  1865.000000   \n",
       "7804   4.00000  14.500000   61.000000  285.000000   883.500000  1903.000000   \n",
       "7805   1.80000   5.800000   25.400000  117.400000   292.800000   829.800000   \n",
       "7806   9.50000  33.500000  148.000000  208.000000  1261.500000  3150.000000   \n",
       "\n",
       "      Vehicle Theft         PUMA  \n",
       "0        264.000000  2900.000000  \n",
       "1        150.750000  3000.000000  \n",
       "2        150.750000  3000.000000  \n",
       "3        358.000000  1700.000000  \n",
       "4        579.465873  2602.646825  \n",
       "...             ...          ...  \n",
       "7802      13.000000  2002.000000  \n",
       "7803      92.000000  4100.000000  \n",
       "7804     213.500000  4300.000000  \n",
       "7805      41.200000   140.000000  \n",
       "7806     358.000000  1700.000000  \n",
       "\n",
       "[7807 rows x 186 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_test=pd.read_csv(r'.\\data\\NIJ_s_Recidivism_Challenge_Test_Dataset1.csv')\n",
    "raw_test=pd.merge(raw_test,PUMA_new,left_on='Residence_PUMA',right_on='Code',how='left')\n",
    "# Change the dtype of the feature from object to intagar\n",
    "#raw_extend_test['TEN']=raw_extend_test['TEN'].astype('category')\n",
    "#raw_extend_test['TEN']=raw_extend_test['TEN'].cat.codes\n",
    "raw_test['Residence_PUMA']=raw_test['Residence_PUMA'].astype('category')\n",
    "raw_test['Residence_PUMA']=raw_test['Residence_PUMA'].cat.codes\n",
    "raw_test['Age_at_Release']=raw_test['Age_at_Release'].apply(lambda x: int(x[:2]))\n",
    "raw_test['Dependents']=raw_test['Dependents'].apply(lambda x: int(x[:1]))\n",
    "raw_test['Prior_Arrest_Episodes_Felony']=raw_test['Prior_Arrest_Episodes_Felony'].apply(lambda x: int(x[:2]))\n",
    "raw_test['Prior_Arrest_Episodes_Drug']=raw_test['Prior_Arrest_Episodes_Drug'].apply(lambda x: int(x[:2]))\n",
    "raw_test['Prior_Arrest_Episodes_Misd']=raw_test['Prior_Arrest_Episodes_Misd'].apply(lambda x: int(x[:2]))\n",
    "raw_test['Prior_Arrest_Episodes_Violent']=raw_test['Prior_Arrest_Episodes_Violent'].apply(lambda x: int(x[:2]))\n",
    "raw_test['Prior_Arrest_Episodes_Property']=raw_test['Prior_Arrest_Episodes_Property'].apply(lambda x: int(x[:2]))\n",
    "raw_test['Prior_Arrest_Episodes_PPViolationCharges']=raw_test['Prior_Arrest_Episodes_PPViolationCharges'].apply(lambda x: int(x[:2]))\n",
    "raw_test['Prior_Conviction_Episodes_Felony']=raw_test['Prior_Conviction_Episodes_Felony'].apply(lambda x: int(x[:1]))\n",
    "raw_test['Prior_Conviction_Episodes_Misd']=raw_test['Prior_Conviction_Episodes_Misd'].apply(lambda x: int(x[:1]))\n",
    "raw_test['Prior_Conviction_Episodes_Prop']=raw_test['Prior_Conviction_Episodes_Prop'].apply(lambda x: int(x[:1]))\n",
    "raw_test['Prior_Conviction_Episodes_Drug']=raw_test['Prior_Conviction_Episodes_Drug'].apply(lambda x: int(x[:1]))\n",
    "#raw_test['Delinquency_Reports']=raw_test['Delinquency_Reports'].apply(lambda x: int(x[:1]))\n",
    "#raw_test['Program_Attendances']=raw_test['Program_Attendances'].apply(lambda x: int(x[:2]))\n",
    "#raw_test['Program_UnexcusedAbsences']=raw_test['Program_UnexcusedAbsences'].apply(lambda x: int(x[:1]))\n",
    "#raw_test['Residence_Changes']=raw_test['Residence_Changes'].apply(lambda x: int(x[:1]))\n",
    "\n",
    "\n",
    "raw_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07ec36c7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\myjr\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\myjr\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "#raw_extend_test=pd.merge(raw_test,PUMA_new,left_on='Residence_PUMA',right_on='Code',how='outer')\n",
    "raw_extend_test=raw_test.copy()\n",
    "# Change the dtype of the feature from object to intagar\n",
    "\n",
    "# scale the columns which are not bool or category\n",
    "scaler = StandardScaler()\n",
    "scaling_set=[]\n",
    "for column in raw_extend_test.columns:\n",
    "    if raw_extend_test[column].dtype == object:\n",
    "        raw_extend_test[column]=raw_extend_test[column].astype('category')\n",
    "        raw_extend_test[column]=raw_extend_test[column].cat.codes\n",
    "    elif raw_extend_test[column].dtype in ['int64','float32','float64'] :\n",
    "        scaling_set+=[column]\n",
    "raw_extend_test[scaling_set]=scaler.fit_transform(raw_extend_test[scaling_set].values)\n",
    "raw_extend_test\n",
    "#raw_extend_test=raw_extend_test.drop(index=raw_extend_test[raw_extend_test.Supervision_Risk_Score_First.isnull()].index)\n",
    "#raw_extend_test=raw_extend_test.drop(index=set(raw_extend_test[raw_extend_test.Supervision_Level_First.isnull()].index) & set(raw_extend_test[raw_extend_test.Prison_Offense .isnull()].index))\n",
    "#raw_extend_test=raw_extend_test.reset_index(drop=True)\n",
    "# impute missing value 'Supervision_Level_First' and 'Prison_Offense' with relative feature\n",
    "for missing_column in ['Supervision_Level_First','Prison_Offense']:\n",
    "    test_index=raw_extend_test[raw_extend_test[missing_column]==-1].index\n",
    "    train_index=raw_extend_test[raw_extend_test[missing_column]!=-1].index\n",
    "    X=raw_extend_test.drop(columns=[missing_column])\n",
    "    y=raw_extend_test[missing_column]\n",
    "    logreg = LogisticRegression()\n",
    "    logreg.fit(X.iloc[train_index,:].fillna(0),y[train_index])\n",
    "    raw_extend_test.loc[test_index,missing_column]=logreg.predict(X.iloc[test_index,:].fillna(0))\n",
    "    raw_extend_test[missing_column]=raw_extend_test[missing_column].astype('category')\n",
    "    raw_extend_test[missing_column]=raw_extend_test[missing_column].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "405a8bd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GBDT_importance' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-4f4ef7e4e285>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mraw_extend_test_t\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mraw_extend_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGBDT_importance\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mGBDT_importance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimportance\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFeature\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mraw_extend_test_t\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'GBDT_importance' is not defined"
     ]
    }
   ],
   "source": [
    "raw_extend_test_t=raw_extend_test.drop(list(GBDT_importance[GBDT_importance.importance==0].Feature),axis=1)\n",
    "raw_extend_test_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b24dbb1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.88175026, 0.11824974],\n",
       "       [0.78860689, 0.21139311],\n",
       "       [0.78137321, 0.21862679],\n",
       "       ...,\n",
       "       [0.87786695, 0.12213305],\n",
       "       [0.89507588, 0.10492412],\n",
       "       [0.92154774, 0.07845226]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_SGD.best_estimator_.predict_proba(raw_extend_test.drop(['ID'],axis=1).fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8bfefda6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\myjr\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:430: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
      "  warnings.warn('`model.predict_proba()` is deprecated and '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.8723745 , 0.12762551],\n",
       "       [0.7873806 , 0.21261944],\n",
       "       [0.7115455 , 0.28845447],\n",
       "       ...,\n",
       "       [0.8947715 , 0.10522854],\n",
       "       [0.8627937 , 0.13720632],\n",
       "       [0.8704729 , 0.1295271 ]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.predict_proba(raw_extend_test.astype(float).drop(['ID'],axis=1).fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "edb61e6b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.8790859 , 0.12091407],\n",
       "       [0.774881  , 0.22511896],\n",
       "       [0.807922  , 0.19207798],\n",
       "       ...,\n",
       "       [0.92469525, 0.07530472],\n",
       "       [0.91021514, 0.08978488],\n",
       "       [0.9195307 , 0.0804693 ]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_XGB.best_estimator_.predict_proba(raw_extend_test.drop(['ID'],axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "c652c2c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.84013089, 0.15986911],\n",
       "       [0.8002617 , 0.1997383 ],\n",
       "       [0.75505917, 0.24494083],\n",
       "       ...,\n",
       "       [0.87061416, 0.12938584],\n",
       "       [0.8550241 , 0.1449759 ],\n",
       "       [0.90450848, 0.09549152]])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_SGD_t.best_estimator_.predict_proba(raw_extend_test_t.drop(['ID'],axis=1).fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "9074e067",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\myjr\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:430: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
      "  warnings.warn('`model.predict_proba()` is deprecated and '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.87590116, 0.12409879],\n",
       "       [0.7614838 , 0.23851624],\n",
       "       [0.73425555, 0.26574448],\n",
       "       ...,\n",
       "       [0.88719267, 0.11280731],\n",
       "       [0.863066  , 0.13693395],\n",
       "       [0.897431  , 0.10256904]], dtype=float32)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_t.predict_proba(raw_extend_test_t.astype(float).drop(['ID'],axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "de893955",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.8849331 , 0.11506691],\n",
       "       [0.7579944 , 0.24200562],\n",
       "       [0.7911178 , 0.20888221],\n",
       "       ...,\n",
       "       [0.91650367, 0.08349633],\n",
       "       [0.90556824, 0.09443176],\n",
       "       [0.9140593 , 0.0859407 ]], dtype=float32)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_XGB_t.best_estimator_.predict_proba(raw_extend_test_t.drop(['ID'],axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1624aa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aver_prob(prob_lists):\n",
    "    n=len(prob_lists)\n",
    "    return np.sum(np.array(prob_lists),0)/n\n",
    "prob_list=aver_prob([[x[1] for x in grid_SGD.best_estimator_.predict_proba(raw_extend_test.drop(['ID'],axis=1).fillna(0))],\n",
    "          #[x[1] for x in grid_XGB_t.best_estimator_.predict_proba(raw_extend_test_t.drop(['ID'],axis=1))],\n",
    "          [x[1] for x in grid_XGB.best_estimator_.predict_proba(raw_extend_test.drop(['ID'],axis=1))],\n",
    "          #[x[1] for x in pipe_t.predict_proba(raw_extend_test_t.astype(float).drop(['ID'],axis=1))],\n",
    "          #[x[1] for x in grid_SGD_t.best_estimator_.predict_proba(raw_extend_test_t.drop(['ID'],axis=1).fillna(0))],\n",
    "          ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "16da0c7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Race</th>\n",
       "      <th>Age_at_Release</th>\n",
       "      <th>Residence_PUMA</th>\n",
       "      <th>Gang_Affiliated</th>\n",
       "      <th>Supervision_Risk_Score_First</th>\n",
       "      <th>Supervision_Level_First</th>\n",
       "      <th>Education_Level</th>\n",
       "      <th>Dependents</th>\n",
       "      <th>...</th>\n",
       "      <th>Total\\ncivilians</th>\n",
       "      <th>Murder</th>\n",
       "      <th>Rape</th>\n",
       "      <th>Robbery</th>\n",
       "      <th>Assault</th>\n",
       "      <th>Burglary</th>\n",
       "      <th>Larceny</th>\n",
       "      <th>Vehicle Theft</th>\n",
       "      <th>PUMA</th>\n",
       "      <th>risk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.590334</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.472100</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.209466</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.431162</td>\n",
       "      <td>-0.407371</td>\n",
       "      <td>-0.396844</td>\n",
       "      <td>-0.315890</td>\n",
       "      <td>-0.089840</td>\n",
       "      <td>-0.228450</td>\n",
       "      <td>-0.226258</td>\n",
       "      <td>-0.200076</td>\n",
       "      <td>0.282383</td>\n",
       "      <td>0.119582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.590334</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.472100</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.261263</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.618199</td>\n",
       "      <td>-0.351645</td>\n",
       "      <td>-0.353865</td>\n",
       "      <td>-0.309368</td>\n",
       "      <td>-0.308022</td>\n",
       "      <td>-0.352084</td>\n",
       "      <td>-0.313558</td>\n",
       "      <td>-0.282815</td>\n",
       "      <td>0.357912</td>\n",
       "      <td>0.218256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.052621</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.472100</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.261263</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.618199</td>\n",
       "      <td>-0.351645</td>\n",
       "      <td>-0.353865</td>\n",
       "      <td>-0.309368</td>\n",
       "      <td>-0.308022</td>\n",
       "      <td>-0.352084</td>\n",
       "      <td>-0.313558</td>\n",
       "      <td>-0.282815</td>\n",
       "      <td>0.357912</td>\n",
       "      <td>0.205352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.052621</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.369019</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.385890</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.452743</td>\n",
       "      <td>-0.054440</td>\n",
       "      <td>0.043683</td>\n",
       "      <td>-0.156613</td>\n",
       "      <td>-0.244452</td>\n",
       "      <td>-0.015737</td>\n",
       "      <td>-0.086797</td>\n",
       "      <td>-0.131401</td>\n",
       "      <td>-0.623967</td>\n",
       "      <td>0.253898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.052621</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.892659</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.261263</td>\n",
       "      <td>...</td>\n",
       "      <td>0.110995</td>\n",
       "      <td>0.036904</td>\n",
       "      <td>0.068037</td>\n",
       "      <td>0.027427</td>\n",
       "      <td>0.028658</td>\n",
       "      <td>0.046428</td>\n",
       "      <td>0.036922</td>\n",
       "      <td>0.030400</td>\n",
       "      <td>0.057795</td>\n",
       "      <td>0.229290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7802</th>\n",
       "      <td>26746</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.128046</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.472100</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.261263</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.891561</td>\n",
       "      <td>-0.370221</td>\n",
       "      <td>-0.611735</td>\n",
       "      <td>-0.347471</td>\n",
       "      <td>-0.359453</td>\n",
       "      <td>-0.496033</td>\n",
       "      <td>-0.488985</td>\n",
       "      <td>-0.383453</td>\n",
       "      <td>-0.395869</td>\n",
       "      <td>0.067137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7803</th>\n",
       "      <td>26747</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.052621</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.472100</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.261263</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.057087</td>\n",
       "      <td>-0.370221</td>\n",
       "      <td>-0.396844</td>\n",
       "      <td>-0.332367</td>\n",
       "      <td>-0.470620</td>\n",
       "      <td>-0.442330</td>\n",
       "      <td>-0.288834</td>\n",
       "      <td>-0.325737</td>\n",
       "      <td>1.188732</td>\n",
       "      <td>0.082599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7804</th>\n",
       "      <td>26749</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.485092</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.472100</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.261263</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.150606</td>\n",
       "      <td>-0.258769</td>\n",
       "      <td>-0.364610</td>\n",
       "      <td>-0.276071</td>\n",
       "      <td>-0.146063</td>\n",
       "      <td>-0.192258</td>\n",
       "      <td>-0.282860</td>\n",
       "      <td>-0.236970</td>\n",
       "      <td>1.339791</td>\n",
       "      <td>0.098719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7805</th>\n",
       "      <td>26752</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.485092</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.472100</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.261263</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.618199</td>\n",
       "      <td>-0.340500</td>\n",
       "      <td>-0.551565</td>\n",
       "      <td>-0.324952</td>\n",
       "      <td>-0.360220</td>\n",
       "      <td>-0.468107</td>\n",
       "      <td>-0.451596</td>\n",
       "      <td>-0.362851</td>\n",
       "      <td>-1.802221</td>\n",
       "      <td>0.097355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7806</th>\n",
       "      <td>26755</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.052621</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.472100</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.261263</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.452743</td>\n",
       "      <td>-0.054440</td>\n",
       "      <td>0.043683</td>\n",
       "      <td>-0.156613</td>\n",
       "      <td>-0.244452</td>\n",
       "      <td>-0.015737</td>\n",
       "      <td>-0.086797</td>\n",
       "      <td>-0.131401</td>\n",
       "      <td>-0.623967</td>\n",
       "      <td>0.079461</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7807 rows × 187 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID  Gender  Race  Age_at_Release  Residence_PUMA  Gang_Affiliated  \\\n",
       "0         6       1     1        0.590334              16                0   \n",
       "1         8       1     0        0.590334              15                0   \n",
       "2        12       1     0        0.052621              15                0   \n",
       "3        15       1     1        0.052621               4                0   \n",
       "4        16       1     0        0.052621               2                0   \n",
       "...     ...     ...   ...             ...             ...              ...   \n",
       "7802  26746       1     0        1.128046               5                0   \n",
       "7803  26747       1     1        0.052621               7                0   \n",
       "7804  26749       1     0       -0.485092               1                0   \n",
       "7805  26752       1     0       -0.485092               6                0   \n",
       "7806  26755       1     1        0.052621               4                0   \n",
       "\n",
       "      Supervision_Risk_Score_First  Supervision_Level_First  Education_Level  \\\n",
       "0                        -0.472100                        2                1   \n",
       "1                        -0.472100                        0                1   \n",
       "2                        -0.472100                        1                1   \n",
       "3                         0.369019                        2                2   \n",
       "4                        -0.892659                        2                2   \n",
       "...                            ...                      ...              ...   \n",
       "7802                     -0.472100                        2                0   \n",
       "7803                     -0.472100                        2                0   \n",
       "7804                     -0.472100                        2                1   \n",
       "7805                     -0.472100                        2                0   \n",
       "7806                     -0.472100                        2                0   \n",
       "\n",
       "      Dependents  ...  Total\\ncivilians    Murder      Rape   Robbery  \\\n",
       "0      -1.209466  ...         -0.431162 -0.407371 -0.396844 -0.315890   \n",
       "1       1.261263  ...         -0.618199 -0.351645 -0.353865 -0.309368   \n",
       "2       1.261263  ...         -0.618199 -0.351645 -0.353865 -0.309368   \n",
       "3      -0.385890  ...         -0.452743 -0.054440  0.043683 -0.156613   \n",
       "4       1.261263  ...          0.110995  0.036904  0.068037  0.027427   \n",
       "...          ...  ...               ...       ...       ...       ...   \n",
       "7802    1.261263  ...         -0.891561 -0.370221 -0.611735 -0.347471   \n",
       "7803    1.261263  ...         -0.057087 -0.370221 -0.396844 -0.332367   \n",
       "7804    1.261263  ...         -0.150606 -0.258769 -0.364610 -0.276071   \n",
       "7805    1.261263  ...         -0.618199 -0.340500 -0.551565 -0.324952   \n",
       "7806    1.261263  ...         -0.452743 -0.054440  0.043683 -0.156613   \n",
       "\n",
       "       Assault  Burglary   Larceny  Vehicle Theft      PUMA      risk  \n",
       "0    -0.089840 -0.228450 -0.226258      -0.200076  0.282383  0.119582  \n",
       "1    -0.308022 -0.352084 -0.313558      -0.282815  0.357912  0.218256  \n",
       "2    -0.308022 -0.352084 -0.313558      -0.282815  0.357912  0.205352  \n",
       "3    -0.244452 -0.015737 -0.086797      -0.131401 -0.623967  0.253898  \n",
       "4     0.028658  0.046428  0.036922       0.030400  0.057795  0.229290  \n",
       "...        ...       ...       ...            ...       ...       ...  \n",
       "7802 -0.359453 -0.496033 -0.488985      -0.383453 -0.395869  0.067137  \n",
       "7803 -0.470620 -0.442330 -0.288834      -0.325737  1.188732  0.082599  \n",
       "7804 -0.146063 -0.192258 -0.282860      -0.236970  1.339791  0.098719  \n",
       "7805 -0.360220 -0.468107 -0.451596      -0.362851 -1.802221  0.097355  \n",
       "7806 -0.244452 -0.015737 -0.086797      -0.131401 -0.623967  0.079461  \n",
       "\n",
       "[7807 rows x 187 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_extend_test['risk']=prob_list\n",
    "raw_extend_test['ID']=raw_test.ID\n",
    "raw_extend_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "07720cf0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Male\n",
    "Output_male=raw_extend_test[raw_extend_test['Gender']==1][['ID','Gender','risk']]\n",
    "Output_male.ID=Output_male.ID.astype(int)\n",
    "Output_male.Gender='M'\n",
    "# Female\n",
    "Output_female=raw_extend_test[raw_extend_test['Gender']==0][['ID','Gender','risk']]\n",
    "Output_female.ID=Output_female.ID.astype(int)\n",
    "Output_female.Gender='F'\n",
    "Final_out=Output_male.append(Output_female).reset_index(drop=True)\n",
    "Final_out\n",
    "Final_out.to_csv(r'C:\\Users\\myjr\\Dropbox\\research\\Recidivism_1styear.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "75713d2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(Final_out[Final_out.Gender=='F'].ID)==list(raw_test[raw_test['Gender']=='F'].ID) and list(Final_out[Final_out.Gender=='G'].ID)==list(raw_test[raw_test['Gender']=='G'].ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "62937c98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>risk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>M</td>\n",
       "      <td>0.119582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>M</td>\n",
       "      <td>0.218256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>M</td>\n",
       "      <td>0.205352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>M</td>\n",
       "      <td>0.253898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16</td>\n",
       "      <td>M</td>\n",
       "      <td>0.229290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7802</th>\n",
       "      <td>26577</td>\n",
       "      <td>F</td>\n",
       "      <td>0.240920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7803</th>\n",
       "      <td>26604</td>\n",
       "      <td>F</td>\n",
       "      <td>0.171301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7804</th>\n",
       "      <td>26617</td>\n",
       "      <td>F</td>\n",
       "      <td>0.094751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7805</th>\n",
       "      <td>26622</td>\n",
       "      <td>F</td>\n",
       "      <td>0.100783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7806</th>\n",
       "      <td>26701</td>\n",
       "      <td>F</td>\n",
       "      <td>0.168483</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7807 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID Gender      risk\n",
       "0         6      M  0.119582\n",
       "1         8      M  0.218256\n",
       "2        12      M  0.205352\n",
       "3        15      M  0.253898\n",
       "4        16      M  0.229290\n",
       "...     ...    ...       ...\n",
       "7802  26577      F  0.240920\n",
       "7803  26604      F  0.171301\n",
       "7804  26617      F  0.094751\n",
       "7805  26622      F  0.100783\n",
       "7806  26701      F  0.168483\n",
       "\n",
       "[7807 rows x 3 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Final_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb955f0",
   "metadata": {},
   "source": [
    "### Gender seperated prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3fbcf4aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression train score: 0.7924187725631769 \n",
      " test score: 0.8162162162162162 \n",
      " Logistic regression train Brier score: 0.14707952415927938 \n",
      " test Brier score: 0.14041002067979333 \n",
      " AUROC: 0.6831599160451928\n",
      "Random forest train score: 1.0 \n",
      " test score: 0.8234234234234235 \n",
      " Random forest  train Brier score: 0.023069233854793352 \n",
      " test Brier score: 0.14572924924924913 \n",
      " AUROC: 0.6266467199571294\n",
      "SGD best layer size: {'min_samples_split': 4, 'n_estimators': 150} \n",
      " best train score: 0.7713598074608905 \n",
      " test score: 0.818018018018018 \n",
      " SGD  train Brier score: 0.09129882458266762 \n",
      " test Brier score: 0.14077057585743485 \n",
      " AUROC: 0.6873129995980887\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\myjr\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 1s 5ms/step - loss: 0.9257 - accuracy: 0.5262 - val_loss: 0.6501 - val_accuracy: 0.6012\n",
      "Epoch 2/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.7688 - accuracy: 0.5959 - val_loss: 0.5612 - val_accuracy: 0.7655\n",
      "Epoch 3/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.6782 - accuracy: 0.6518 - val_loss: 0.5191 - val_accuracy: 0.8076\n",
      "Epoch 4/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.6429 - accuracy: 0.6793 - val_loss: 0.5025 - val_accuracy: 0.8176\n",
      "Epoch 5/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.6128 - accuracy: 0.7034 - val_loss: 0.4909 - val_accuracy: 0.8216\n",
      "Epoch 6/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.6280 - accuracy: 0.6896 - val_loss: 0.4833 - val_accuracy: 0.8216\n",
      "Epoch 7/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5890 - accuracy: 0.7163 - val_loss: 0.4807 - val_accuracy: 0.8216\n",
      "Epoch 8/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5896 - accuracy: 0.7120 - val_loss: 0.4785 - val_accuracy: 0.8216\n",
      "Epoch 9/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5922 - accuracy: 0.7206 - val_loss: 0.4788 - val_accuracy: 0.8216\n",
      "Epoch 10/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5926 - accuracy: 0.7223 - val_loss: 0.4782 - val_accuracy: 0.8216\n",
      "Epoch 11/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5953 - accuracy: 0.7223 - val_loss: 0.4751 - val_accuracy: 0.8216\n",
      "Epoch 12/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5630 - accuracy: 0.7420 - val_loss: 0.4768 - val_accuracy: 0.8216\n",
      "Epoch 13/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5598 - accuracy: 0.7455 - val_loss: 0.4776 - val_accuracy: 0.8216\n",
      "Epoch 14/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5478 - accuracy: 0.7524 - val_loss: 0.4766 - val_accuracy: 0.8216\n",
      "Epoch 15/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5488 - accuracy: 0.7635 - val_loss: 0.4764 - val_accuracy: 0.8216\n",
      "Epoch 16/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5535 - accuracy: 0.7463 - val_loss: 0.4757 - val_accuracy: 0.8216\n",
      "Epoch 17/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5496 - accuracy: 0.7472 - val_loss: 0.4754 - val_accuracy: 0.8216\n",
      "Epoch 18/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5484 - accuracy: 0.7592 - val_loss: 0.4763 - val_accuracy: 0.8216\n",
      "Epoch 19/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5368 - accuracy: 0.7575 - val_loss: 0.4739 - val_accuracy: 0.8216\n",
      "Epoch 20/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5449 - accuracy: 0.7592 - val_loss: 0.4746 - val_accuracy: 0.8216\n",
      "Epoch 21/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5392 - accuracy: 0.7601 - val_loss: 0.4745 - val_accuracy: 0.8216\n",
      "Epoch 22/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5365 - accuracy: 0.7661 - val_loss: 0.4749 - val_accuracy: 0.8216\n",
      "Epoch 23/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5392 - accuracy: 0.7670 - val_loss: 0.4760 - val_accuracy: 0.8216\n",
      "Epoch 24/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5356 - accuracy: 0.7584 - val_loss: 0.4773 - val_accuracy: 0.8216\n",
      "Epoch 25/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5126 - accuracy: 0.7653 - val_loss: 0.4769 - val_accuracy: 0.8216\n",
      "Epoch 26/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5389 - accuracy: 0.7627 - val_loss: 0.4764 - val_accuracy: 0.8216\n",
      "Epoch 27/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5270 - accuracy: 0.7661 - val_loss: 0.4761 - val_accuracy: 0.8216\n",
      "Epoch 28/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5242 - accuracy: 0.7635 - val_loss: 0.4759 - val_accuracy: 0.8216\n",
      "Epoch 29/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5295 - accuracy: 0.7515 - val_loss: 0.4765 - val_accuracy: 0.8216\n",
      "Epoch 30/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5289 - accuracy: 0.7644 - val_loss: 0.4766 - val_accuracy: 0.8216\n",
      "Epoch 31/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5227 - accuracy: 0.7678 - val_loss: 0.4741 - val_accuracy: 0.8216\n",
      "Epoch 32/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5214 - accuracy: 0.7704 - val_loss: 0.4743 - val_accuracy: 0.8216\n",
      "Epoch 33/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5300 - accuracy: 0.7670 - val_loss: 0.4738 - val_accuracy: 0.8216\n",
      "Epoch 34/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5194 - accuracy: 0.7696 - val_loss: 0.4729 - val_accuracy: 0.8216\n",
      "Epoch 35/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5196 - accuracy: 0.7687 - val_loss: 0.4723 - val_accuracy: 0.8216\n",
      "Epoch 36/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5225 - accuracy: 0.7661 - val_loss: 0.4739 - val_accuracy: 0.8216\n",
      "Epoch 37/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5181 - accuracy: 0.7704 - val_loss: 0.4739 - val_accuracy: 0.8216\n",
      "Epoch 38/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5193 - accuracy: 0.7687 - val_loss: 0.4751 - val_accuracy: 0.8216\n",
      "Epoch 39/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5238 - accuracy: 0.7592 - val_loss: 0.4764 - val_accuracy: 0.8216\n",
      "Epoch 40/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5274 - accuracy: 0.7584 - val_loss: 0.4747 - val_accuracy: 0.8216\n",
      "Epoch 41/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5242 - accuracy: 0.7687 - val_loss: 0.4741 - val_accuracy: 0.8216\n",
      "Epoch 42/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5182 - accuracy: 0.7635 - val_loss: 0.4733 - val_accuracy: 0.8216\n",
      "Epoch 43/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5152 - accuracy: 0.7627 - val_loss: 0.4731 - val_accuracy: 0.8216\n",
      "Epoch 44/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5184 - accuracy: 0.7696 - val_loss: 0.4720 - val_accuracy: 0.8216\n",
      "Epoch 45/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5218 - accuracy: 0.7592 - val_loss: 0.4726 - val_accuracy: 0.8216\n",
      "Epoch 46/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5157 - accuracy: 0.7678 - val_loss: 0.4726 - val_accuracy: 0.8216\n",
      "Epoch 47/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5121 - accuracy: 0.7670 - val_loss: 0.4730 - val_accuracy: 0.8216\n",
      "Epoch 48/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5151 - accuracy: 0.7601 - val_loss: 0.4744 - val_accuracy: 0.8216\n",
      "Epoch 49/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5158 - accuracy: 0.7696 - val_loss: 0.4742 - val_accuracy: 0.8236\n",
      "Epoch 50/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5265 - accuracy: 0.7610 - val_loss: 0.4748 - val_accuracy: 0.8236\n",
      "Epoch 51/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5117 - accuracy: 0.7721 - val_loss: 0.4738 - val_accuracy: 0.8236\n",
      "Epoch 52/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5181 - accuracy: 0.7618 - val_loss: 0.4738 - val_accuracy: 0.8236\n",
      "Epoch 53/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5028 - accuracy: 0.7601 - val_loss: 0.4758 - val_accuracy: 0.8216\n",
      "Epoch 54/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5093 - accuracy: 0.7773 - val_loss: 0.4741 - val_accuracy: 0.8216\n",
      "Epoch 55/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5112 - accuracy: 0.7644 - val_loss: 0.4752 - val_accuracy: 0.8216\n",
      "Epoch 56/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5188 - accuracy: 0.7704 - val_loss: 0.4755 - val_accuracy: 0.8216\n",
      "Epoch 57/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5170 - accuracy: 0.7704 - val_loss: 0.4764 - val_accuracy: 0.8196\n",
      "Epoch 58/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5076 - accuracy: 0.7704 - val_loss: 0.4769 - val_accuracy: 0.8196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5092 - accuracy: 0.7644 - val_loss: 0.4746 - val_accuracy: 0.8196\n",
      "Epoch 60/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5226 - accuracy: 0.7610 - val_loss: 0.4743 - val_accuracy: 0.8216\n",
      "Epoch 61/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5099 - accuracy: 0.7618 - val_loss: 0.4740 - val_accuracy: 0.8216\n",
      "Epoch 62/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5037 - accuracy: 0.7670 - val_loss: 0.4740 - val_accuracy: 0.8216\n",
      "Epoch 63/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5118 - accuracy: 0.7627 - val_loss: 0.4739 - val_accuracy: 0.8196\n",
      "Epoch 64/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5019 - accuracy: 0.7687 - val_loss: 0.4754 - val_accuracy: 0.8156\n",
      "Epoch 65/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5084 - accuracy: 0.7635 - val_loss: 0.4753 - val_accuracy: 0.8156\n",
      "Epoch 66/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5087 - accuracy: 0.7713 - val_loss: 0.4748 - val_accuracy: 0.8176\n",
      "Epoch 67/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5050 - accuracy: 0.7592 - val_loss: 0.4744 - val_accuracy: 0.8196\n",
      "Epoch 68/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4975 - accuracy: 0.7756 - val_loss: 0.4749 - val_accuracy: 0.8176\n",
      "Epoch 69/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4973 - accuracy: 0.7627 - val_loss: 0.4742 - val_accuracy: 0.8156\n",
      "Epoch 70/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5081 - accuracy: 0.7653 - val_loss: 0.4745 - val_accuracy: 0.8116\n",
      "Epoch 71/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5029 - accuracy: 0.7618 - val_loss: 0.4757 - val_accuracy: 0.8156\n",
      "Epoch 72/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5029 - accuracy: 0.7601 - val_loss: 0.4739 - val_accuracy: 0.8196\n",
      "Epoch 73/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5102 - accuracy: 0.7704 - val_loss: 0.4743 - val_accuracy: 0.8216\n",
      "Epoch 74/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5073 - accuracy: 0.7670 - val_loss: 0.4748 - val_accuracy: 0.8216\n",
      "Epoch 75/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5036 - accuracy: 0.7713 - val_loss: 0.4748 - val_accuracy: 0.8196\n",
      "Epoch 76/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4988 - accuracy: 0.7756 - val_loss: 0.4760 - val_accuracy: 0.8116\n",
      "Epoch 77/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5071 - accuracy: 0.7661 - val_loss: 0.4758 - val_accuracy: 0.8136\n",
      "Epoch 78/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5092 - accuracy: 0.7687 - val_loss: 0.4733 - val_accuracy: 0.8176\n",
      "Epoch 79/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4984 - accuracy: 0.7653 - val_loss: 0.4746 - val_accuracy: 0.8156\n",
      "Epoch 80/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5026 - accuracy: 0.7670 - val_loss: 0.4751 - val_accuracy: 0.8196\n",
      "Epoch 81/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4992 - accuracy: 0.7644 - val_loss: 0.4751 - val_accuracy: 0.8216\n",
      "Epoch 82/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4991 - accuracy: 0.7747 - val_loss: 0.4763 - val_accuracy: 0.8136\n",
      "Epoch 83/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4934 - accuracy: 0.7721 - val_loss: 0.4770 - val_accuracy: 0.8116\n",
      "Epoch 84/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4912 - accuracy: 0.7756 - val_loss: 0.4766 - val_accuracy: 0.8116\n",
      "Epoch 85/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4993 - accuracy: 0.7678 - val_loss: 0.4751 - val_accuracy: 0.8136\n",
      "Epoch 86/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4938 - accuracy: 0.7678 - val_loss: 0.4754 - val_accuracy: 0.8156\n",
      "Epoch 87/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5004 - accuracy: 0.7635 - val_loss: 0.4748 - val_accuracy: 0.8116\n",
      "Epoch 88/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4919 - accuracy: 0.7747 - val_loss: 0.4770 - val_accuracy: 0.8156\n",
      "Epoch 89/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5044 - accuracy: 0.7687 - val_loss: 0.4759 - val_accuracy: 0.8136\n",
      "Epoch 90/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4938 - accuracy: 0.7678 - val_loss: 0.4745 - val_accuracy: 0.8156\n",
      "Epoch 91/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4977 - accuracy: 0.7644 - val_loss: 0.4746 - val_accuracy: 0.8176\n",
      "Epoch 92/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4994 - accuracy: 0.7713 - val_loss: 0.4753 - val_accuracy: 0.8156\n",
      "Epoch 93/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4936 - accuracy: 0.7661 - val_loss: 0.4765 - val_accuracy: 0.8096\n",
      "Epoch 94/200\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4938 - accuracy: 0.7670 - val_loss: 0.4745 - val_accuracy: 0.8136\n",
      "52/52 [==============================] - 0s 1ms/step - loss: 0.4911 - accuracy: 0.7780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\myjr\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:430: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
      "  warnings.warn('`model.predict_proba()` is deprecated and '\n",
      "C:\\Users\\myjr\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:430: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
      "  warnings.warn('`model.predict_proba()` is deprecated and '\n",
      "C:\\Users\\myjr\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:430: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
      "  warnings.warn('`model.predict_proba()` is deprecated and '\n",
      "C:\\Users\\myjr\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:430: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
      "  warnings.warn('`model.predict_proba()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP train Brier score: 0.1534704395064796 \n",
      " test Brier score: 0.14443323244654885 \n",
      " AUROC: 0.6442191756352432\n",
      "Xgboost best layer size: {'max_depth': 8, 'n_estimators': 100, 'reg_alpha': 0.001, 'reg_lambda': 10000} \n",
      " best train score: 0.7839951865222624 \n",
      " test score: 0.8234234234234235 \n",
      " Xgboost train Brier score: 0.17744191549880703 \n",
      " test Brier score: 0.16286965995066122 \n",
      " AUROC: 0.6528379404278122\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression,LogisticRegression\n",
    "\n",
    "\n",
    "X_train1,X_test1,y_train1,y_test1=train_test_split(raw_extend[raw_extend['Gender']==0].drop(['ID','Recidivism_Within_3years','Recidivism_Arrest_Year1','Recidivism_Arrest_Year2','Recidivism_Arrest_Year3'],axis=1),raw_extend[raw_extend['Gender']==0]['Recidivism_Arrest_Year1'])\n",
    "fpr_list,tpr_list,auc_list=dict(),dict(),dict()\n",
    "\n",
    "\n",
    "logistic=LogisticRegression(max_iter=1000)\n",
    "logistic.fit(X_train1.fillna(0),y_train1)\n",
    "fpr_list[0], tpr_list[0], _ = roc_curve(y_test1, y_roc(logistic,X_test1.fillna(0)))\n",
    "print('Logistic regression train score:',\n",
    "      logistic.score(X_train1.fillna(0),y_train1),'\\n test score:',logistic.score(X_test1.fillna(0),y_test1.fillna(0)),'\\n Logistic regression train Brier score:',\n",
    "      brier_score(logistic.predict_proba(X_train1.fillna(0)),y_train1),'\\n test Brier score:',brier_score(logistic.predict_proba(X_test1.fillna(0)),y_test1),\n",
    "     '\\n AUROC:',roc_auc_score(y_test1, y_roc(logistic,X_test1.fillna(0))))\n",
    "\n",
    "RF=RandomForestClassifier(n_estimators=150,min_samples_split=2)\n",
    "RF.fit(X_train1.fillna(0),y_train1)\n",
    "fpr_list[1], tpr_list[1], _ = roc_curve(y_test1, y_roc(RF,X_test1.fillna(0)))\n",
    "print('Random forest train score:',\n",
    "      RF.score(X_train1.fillna(0),y_train1),'\\n test score:',RF.score(X_test1.fillna(0),y_test1.fillna(0)),'\\n Random forest  train Brier score:',\n",
    "      brier_score(RF.predict_proba(X_train1.fillna(0)),y_train1),'\\n test Brier score:',brier_score(RF.predict_proba(X_test1.fillna(0)),y_test1),\n",
    "     '\\n AUROC:',roc_auc_score(y_test1, y_roc(RF,X_test1.fillna(0))))\n",
    "\n",
    "GBDT=GradientBoostingClassifier()\n",
    "params_SGD={'n_estimators':[150],'min_samples_split':[2,4]}\n",
    "grid_SGD=GridSearchCV(GBDT,param_grid=params_SGD,cv=3)\n",
    "grid_SGD.fit(X_train1.fillna(0),y_train1.fillna(0))\n",
    "fpr_list[2], tpr_list[2], _ = roc_curve(y_test1, y_roc(grid_SGD.best_estimator_,X_test1.fillna(0)))\n",
    "print('SGD best layer size:',grid_SGD.best_params_,'\\n best train score:',\n",
    "      grid_SGD.best_score_,'\\n test score:',grid_SGD.score(X_test1.fillna(0),y_test1.fillna(0)),'\\n SGD  train Brier score:',\n",
    "      brier_score(grid_SGD.predict_proba(X_train1.fillna(0)),y_train1),'\\n test Brier score:',brier_score(grid_SGD.predict_proba(X_test1.fillna(0)),y_test1),\n",
    "     '\\n AUROC:',roc_auc_score(y_test1, y_roc(grid_SGD.best_estimator_,X_test1.fillna(0))))\n",
    "\n",
    "pipe = Sequential()\n",
    "n_cols = X_train1.shape[1]\n",
    "pipe.add(BatchNormalization(input_shape=(n_cols,)))\n",
    "pipe.add(Dense(70, activation= 'linear'))\n",
    "pipe.add(Dropout(0.3))\n",
    "pipe.add(Dense(50, activation= 'relu'))\n",
    "pipe.add(Dropout(0.3))\n",
    "pipe.add(Dense(50, activation= 'relu'))\n",
    "pipe.add(Dropout(0.3))\n",
    "#model.add(Dense(100, activation='relu'))\n",
    "#model.add(Dropout(0.3))\n",
    "#model.add(Dense(100, activation='linear'))\n",
    "pipe.add(BatchNormalization())\n",
    "pipe.add(Dense(2, activation='softmax'))\n",
    "    #model.compile(\n",
    "        #optimizer='Adam',\n",
    "        #loss='mean_squared_error',\n",
    "        #metrics=['accuracy'])\n",
    "early_stopping_monitor = EarlyStopping(patience=50)\n",
    "sgd = keras.optimizers.SGD(lr=.001, decay=2e-4, momentum=0.9, nesterov=True)\n",
    "pipe.compile(loss = 'sparse_categorical_crossentropy',optimizer = 'sgd', metrics=['accuracy'])\n",
    "history=pipe.fit(X_train1.fillna(0).astype('float32'), y_train1, validation_split=0.3, epochs=200, callbacks=[early_stopping_monitor])\n",
    "#history=model.fit(X_train, y_train, validation_split=0.2, epochs=25)\n",
    "score = pipe.evaluate(X_test1.fillna(0).astype('float32'), y_test1, verbose=0)\n",
    "pipe.fit(X_train1.fillna(0).astype('float32'),y_train1.fillna(0))\n",
    "fpr_list[3], tpr_list[3], _ = roc_curve(y_test1, y_roc(pipe,X_test1.fillna(0).astype('float32')))\n",
    "print('MLP train Brier score:',\n",
    "      brier_score(pipe.predict_proba(X_train1.fillna(0).astype('float32')),y_train1),'\\n test Brier score:',brier_score(pipe.predict_proba(X_test1.fillna(0).astype('float32')),y_test1),\n",
    "     '\\n AUROC:',roc_auc_score(y_test1, y_roc(pipe,X_test1.fillna(0).astype('float32'))))\n",
    "\n",
    "\n",
    "XGB=XGBClassifier()\n",
    "params_XGB={'n_estimators':[100,200],'max_depth':[8,10],'reg_alpha':[0.001],'reg_lambda':[10000,1000,500]}\n",
    "grid_XGB=GridSearchCV(XGB,param_grid=params_XGB,cv=3)\n",
    "grid_XGB.fit(X_train1.fillna(0),y_train1.fillna(0))\n",
    "fpr_list[4], tpr_list[4], _ = roc_curve(y_test1, y_roc(grid_XGB.best_estimator_,X_test1.fillna(0)))\n",
    "print('Xgboost best layer size:',grid_XGB.best_params_,'\\n best train score:',\n",
    "      grid_XGB.best_score_,'\\n test score:',grid_XGB.score(X_test1.fillna(0),y_test1.fillna(0)),'\\n Xgboost train Brier score:',\n",
    "      brier_score(grid_XGB.predict_proba(X_train1.fillna(0)),y_train1),'\\n test Brier score:',brier_score(grid_XGB.predict_proba(X_test1.fillna(0)),y_test1),\n",
    "     '\\n AUROC:',roc_auc_score(y_test1, y_roc(grid_XGB.best_estimator_,X_test1.fillna(0))))\n",
    "\n",
    "\n",
    "\n",
    "colors = cycle(['aqua', 'red', 'green','orange','blue'])\n",
    "labels=['Logistic Regression','Random Forest','Gradient Boosting','MLP','Xgboost']\n",
    "for i, label, color in zip(range(len(fpr_list)), labels, colors):\n",
    "    legend= label + ' (area = {1:0.2f})'''.format(i, auc(fpr_list[i], tpr_list[i]))\n",
    "    plt.plot(fpr_list[i], tpr_list[i], color=color, lw=2,label=legend)\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC curve for different methods on Recidivism_1Year')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('ROC_plot.png',dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1ec45547",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression train score: 0.7071175577669084 \n",
      " test score: 0.6976979509233494 \n",
      " Logistic regression train Brier score: 0.1911511398195571 \n",
      " test Brier score: 0.19624342201348394 \n",
      " AUROC: 0.6807684459972221\n",
      "Random forest train score: 1.0 \n",
      " test score: 0.6971920060713382 \n",
      " Random forest  train Brier score: 0.027696868499466558 \n",
      " test Brier score: 0.20025378193776927 \n",
      " AUROC: 0.6584534309865071\n",
      "SGD best layer size: {'min_samples_split': 4, 'n_estimators': 150} \n",
      " best train score: 0.6971667856422273 \n",
      " test score: 0.6984568682013661 \n",
      " SGD  train Brier score: 0.1767656235270351 \n",
      " test Brier score: 0.1951431776816539 \n",
      " AUROC: 0.6851114706682513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\myjr\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "260/260 [==============================] - 1s 2ms/step - loss: 0.7883 - accuracy: 0.5839 - val_loss: 0.6122 - val_accuracy: 0.6889\n",
      "Epoch 2/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.6624 - accuracy: 0.6470 - val_loss: 0.6046 - val_accuracy: 0.6903\n",
      "Epoch 3/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.6303 - accuracy: 0.6767 - val_loss: 0.6050 - val_accuracy: 0.6906\n",
      "Epoch 4/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.6208 - accuracy: 0.6796 - val_loss: 0.6028 - val_accuracy: 0.6897\n",
      "Epoch 5/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.6158 - accuracy: 0.6828 - val_loss: 0.5998 - val_accuracy: 0.6897\n",
      "Epoch 6/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.6103 - accuracy: 0.6863 - val_loss: 0.5994 - val_accuracy: 0.6897\n",
      "Epoch 7/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.6061 - accuracy: 0.6857 - val_loss: 0.5946 - val_accuracy: 0.6914\n",
      "Epoch 8/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.6037 - accuracy: 0.6852 - val_loss: 0.5907 - val_accuracy: 0.6928\n",
      "Epoch 9/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.6035 - accuracy: 0.6871 - val_loss: 0.5889 - val_accuracy: 0.6906\n",
      "Epoch 10/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.6016 - accuracy: 0.6893 - val_loss: 0.5876 - val_accuracy: 0.6922\n",
      "Epoch 11/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5972 - accuracy: 0.6867 - val_loss: 0.5841 - val_accuracy: 0.6942\n",
      "Epoch 12/200\n",
      "260/260 [==============================] - 0s 2ms/step - loss: 0.5942 - accuracy: 0.6899 - val_loss: 0.5825 - val_accuracy: 0.6962\n",
      "Epoch 13/200\n",
      "260/260 [==============================] - 0s 2ms/step - loss: 0.5954 - accuracy: 0.6888 - val_loss: 0.5812 - val_accuracy: 0.6970\n",
      "Epoch 14/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5920 - accuracy: 0.6913 - val_loss: 0.5792 - val_accuracy: 0.7004\n",
      "Epoch 15/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5917 - accuracy: 0.6886 - val_loss: 0.5790 - val_accuracy: 0.6998\n",
      "Epoch 16/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5876 - accuracy: 0.6910 - val_loss: 0.5776 - val_accuracy: 0.6987\n",
      "Epoch 17/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5873 - accuracy: 0.6913 - val_loss: 0.5769 - val_accuracy: 0.7007\n",
      "Epoch 18/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5871 - accuracy: 0.6955 - val_loss: 0.5754 - val_accuracy: 0.7024\n",
      "Epoch 19/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5902 - accuracy: 0.6847 - val_loss: 0.5760 - val_accuracy: 0.7032\n",
      "Epoch 20/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5884 - accuracy: 0.6884 - val_loss: 0.5750 - val_accuracy: 0.7046\n",
      "Epoch 21/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5840 - accuracy: 0.6951 - val_loss: 0.5733 - val_accuracy: 0.7074\n",
      "Epoch 22/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5818 - accuracy: 0.6957 - val_loss: 0.5737 - val_accuracy: 0.7069\n",
      "Epoch 23/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5842 - accuracy: 0.6911 - val_loss: 0.5731 - val_accuracy: 0.7043\n",
      "Epoch 24/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5830 - accuracy: 0.6928 - val_loss: 0.5727 - val_accuracy: 0.7040\n",
      "Epoch 25/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5808 - accuracy: 0.6919 - val_loss: 0.5716 - val_accuracy: 0.7063\n",
      "Epoch 26/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5808 - accuracy: 0.6918 - val_loss: 0.5719 - val_accuracy: 0.7032\n",
      "Epoch 27/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5827 - accuracy: 0.6922 - val_loss: 0.5730 - val_accuracy: 0.7057\n",
      "Epoch 28/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5827 - accuracy: 0.6919 - val_loss: 0.5729 - val_accuracy: 0.7071\n",
      "Epoch 29/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5812 - accuracy: 0.6945 - val_loss: 0.5712 - val_accuracy: 0.7071\n",
      "Epoch 30/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5779 - accuracy: 0.6927 - val_loss: 0.5704 - val_accuracy: 0.7063\n",
      "Epoch 31/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5789 - accuracy: 0.6971 - val_loss: 0.5708 - val_accuracy: 0.7080\n",
      "Epoch 32/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5779 - accuracy: 0.6936 - val_loss: 0.5715 - val_accuracy: 0.7060\n",
      "Epoch 33/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5808 - accuracy: 0.6906 - val_loss: 0.5735 - val_accuracy: 0.7029\n",
      "Epoch 34/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5791 - accuracy: 0.6937 - val_loss: 0.5713 - val_accuracy: 0.7071\n",
      "Epoch 35/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5779 - accuracy: 0.6927 - val_loss: 0.5712 - val_accuracy: 0.7057\n",
      "Epoch 36/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5775 - accuracy: 0.6939 - val_loss: 0.5710 - val_accuracy: 0.7046\n",
      "Epoch 37/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5752 - accuracy: 0.6987 - val_loss: 0.5714 - val_accuracy: 0.7071\n",
      "Epoch 38/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5761 - accuracy: 0.6941 - val_loss: 0.5709 - val_accuracy: 0.7066\n",
      "Epoch 39/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5764 - accuracy: 0.6933 - val_loss: 0.5704 - val_accuracy: 0.7060\n",
      "Epoch 40/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5763 - accuracy: 0.6964 - val_loss: 0.5703 - val_accuracy: 0.7049\n",
      "Epoch 41/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5809 - accuracy: 0.6928 - val_loss: 0.5699 - val_accuracy: 0.7066\n",
      "Epoch 42/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5795 - accuracy: 0.6954 - val_loss: 0.5706 - val_accuracy: 0.7060\n",
      "Epoch 43/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5772 - accuracy: 0.6898 - val_loss: 0.5694 - val_accuracy: 0.7060\n",
      "Epoch 44/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5776 - accuracy: 0.6963 - val_loss: 0.5702 - val_accuracy: 0.7063\n",
      "Epoch 45/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5776 - accuracy: 0.6918 - val_loss: 0.5695 - val_accuracy: 0.7069\n",
      "Epoch 46/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5751 - accuracy: 0.6983 - val_loss: 0.5689 - val_accuracy: 0.7052\n",
      "Epoch 47/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5758 - accuracy: 0.7007 - val_loss: 0.5695 - val_accuracy: 0.7069\n",
      "Epoch 48/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5801 - accuracy: 0.6927 - val_loss: 0.5703 - val_accuracy: 0.7052\n",
      "Epoch 49/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5760 - accuracy: 0.6948 - val_loss: 0.5705 - val_accuracy: 0.7063\n",
      "Epoch 50/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5750 - accuracy: 0.6966 - val_loss: 0.5718 - val_accuracy: 0.7057\n",
      "Epoch 51/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5742 - accuracy: 0.6959 - val_loss: 0.5690 - val_accuracy: 0.7057\n",
      "Epoch 52/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5726 - accuracy: 0.6977 - val_loss: 0.5697 - val_accuracy: 0.7052\n",
      "Epoch 53/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5754 - accuracy: 0.6988 - val_loss: 0.5701 - val_accuracy: 0.7066\n",
      "Epoch 54/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5778 - accuracy: 0.6911 - val_loss: 0.5699 - val_accuracy: 0.7052\n",
      "Epoch 55/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5763 - accuracy: 0.6931 - val_loss: 0.5684 - val_accuracy: 0.7063\n",
      "Epoch 56/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5731 - accuracy: 0.6965 - val_loss: 0.5691 - val_accuracy: 0.7055\n",
      "Epoch 57/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5777 - accuracy: 0.6900 - val_loss: 0.5687 - val_accuracy: 0.7063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5762 - accuracy: 0.6966 - val_loss: 0.5693 - val_accuracy: 0.7040\n",
      "Epoch 59/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5720 - accuracy: 0.6982 - val_loss: 0.5690 - val_accuracy: 0.7057\n",
      "Epoch 60/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5749 - accuracy: 0.6958 - val_loss: 0.5697 - val_accuracy: 0.7057\n",
      "Epoch 61/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5767 - accuracy: 0.6947 - val_loss: 0.5707 - val_accuracy: 0.7046\n",
      "Epoch 62/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5752 - accuracy: 0.6971 - val_loss: 0.5696 - val_accuracy: 0.7032\n",
      "Epoch 63/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5770 - accuracy: 0.6976 - val_loss: 0.5708 - val_accuracy: 0.7057\n",
      "Epoch 64/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5736 - accuracy: 0.6980 - val_loss: 0.5681 - val_accuracy: 0.7057\n",
      "Epoch 65/200\n",
      "260/260 [==============================] - 0s 2ms/step - loss: 0.5724 - accuracy: 0.6975 - val_loss: 0.5682 - val_accuracy: 0.7046\n",
      "Epoch 66/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5755 - accuracy: 0.6945 - val_loss: 0.5683 - val_accuracy: 0.7043\n",
      "Epoch 67/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5754 - accuracy: 0.6971 - val_loss: 0.5680 - val_accuracy: 0.7066\n",
      "Epoch 68/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5747 - accuracy: 0.6960 - val_loss: 0.5694 - val_accuracy: 0.7066\n",
      "Epoch 69/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5757 - accuracy: 0.6945 - val_loss: 0.5694 - val_accuracy: 0.7077\n",
      "Epoch 70/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5748 - accuracy: 0.7000 - val_loss: 0.5680 - val_accuracy: 0.7069\n",
      "Epoch 71/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5726 - accuracy: 0.6977 - val_loss: 0.5682 - val_accuracy: 0.7063\n",
      "Epoch 72/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5789 - accuracy: 0.6969 - val_loss: 0.5688 - val_accuracy: 0.7060\n",
      "Epoch 73/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5716 - accuracy: 0.6959 - val_loss: 0.5690 - val_accuracy: 0.7052\n",
      "Epoch 74/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5737 - accuracy: 0.6935 - val_loss: 0.5686 - val_accuracy: 0.7060\n",
      "Epoch 75/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5713 - accuracy: 0.7025 - val_loss: 0.5681 - val_accuracy: 0.7066\n",
      "Epoch 76/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5704 - accuracy: 0.6959 - val_loss: 0.5687 - val_accuracy: 0.7032\n",
      "Epoch 77/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5732 - accuracy: 0.6972 - val_loss: 0.5690 - val_accuracy: 0.7040\n",
      "Epoch 78/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5718 - accuracy: 0.7000 - val_loss: 0.5679 - val_accuracy: 0.7046\n",
      "Epoch 79/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5729 - accuracy: 0.6996 - val_loss: 0.5677 - val_accuracy: 0.7046\n",
      "Epoch 80/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5741 - accuracy: 0.6934 - val_loss: 0.5691 - val_accuracy: 0.7069\n",
      "Epoch 81/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5750 - accuracy: 0.6931 - val_loss: 0.5685 - val_accuracy: 0.7032\n",
      "Epoch 82/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5736 - accuracy: 0.6946 - val_loss: 0.5689 - val_accuracy: 0.7035\n",
      "Epoch 83/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5745 - accuracy: 0.6937 - val_loss: 0.5675 - val_accuracy: 0.7040\n",
      "Epoch 84/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5734 - accuracy: 0.6971 - val_loss: 0.5690 - val_accuracy: 0.7080\n",
      "Epoch 85/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5717 - accuracy: 0.7007 - val_loss: 0.5692 - val_accuracy: 0.7046\n",
      "Epoch 86/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5709 - accuracy: 0.6970 - val_loss: 0.5681 - val_accuracy: 0.7071\n",
      "Epoch 87/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5732 - accuracy: 0.6996 - val_loss: 0.5685 - val_accuracy: 0.7046\n",
      "Epoch 88/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5701 - accuracy: 0.7013 - val_loss: 0.5678 - val_accuracy: 0.7029\n",
      "Epoch 89/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5726 - accuracy: 0.6994 - val_loss: 0.5673 - val_accuracy: 0.7055\n",
      "Epoch 90/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5716 - accuracy: 0.6977 - val_loss: 0.5674 - val_accuracy: 0.7046\n",
      "Epoch 91/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5708 - accuracy: 0.6994 - val_loss: 0.5685 - val_accuracy: 0.7057\n",
      "Epoch 92/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5725 - accuracy: 0.6990 - val_loss: 0.5682 - val_accuracy: 0.7063\n",
      "Epoch 93/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5723 - accuracy: 0.6988 - val_loss: 0.5681 - val_accuracy: 0.7060\n",
      "Epoch 94/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5717 - accuracy: 0.7011 - val_loss: 0.5687 - val_accuracy: 0.7074\n",
      "Epoch 95/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5742 - accuracy: 0.6927 - val_loss: 0.5675 - val_accuracy: 0.7057\n",
      "Epoch 96/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5720 - accuracy: 0.6977 - val_loss: 0.5671 - val_accuracy: 0.7091\n",
      "Epoch 97/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5719 - accuracy: 0.6981 - val_loss: 0.5672 - val_accuracy: 0.7066\n",
      "Epoch 98/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5687 - accuracy: 0.7016 - val_loss: 0.5671 - val_accuracy: 0.7057\n",
      "Epoch 99/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5711 - accuracy: 0.6941 - val_loss: 0.5671 - val_accuracy: 0.7077\n",
      "Epoch 100/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5730 - accuracy: 0.6981 - val_loss: 0.5677 - val_accuracy: 0.7063\n",
      "Epoch 101/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5702 - accuracy: 0.6960 - val_loss: 0.5673 - val_accuracy: 0.7077\n",
      "Epoch 102/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5717 - accuracy: 0.6990 - val_loss: 0.5672 - val_accuracy: 0.7066\n",
      "Epoch 103/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5706 - accuracy: 0.7006 - val_loss: 0.5664 - val_accuracy: 0.7091\n",
      "Epoch 104/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5707 - accuracy: 0.6988 - val_loss: 0.5673 - val_accuracy: 0.7060\n",
      "Epoch 105/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5701 - accuracy: 0.6977 - val_loss: 0.5676 - val_accuracy: 0.7063\n",
      "Epoch 106/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5709 - accuracy: 0.6975 - val_loss: 0.5693 - val_accuracy: 0.7069\n",
      "Epoch 107/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5708 - accuracy: 0.7043 - val_loss: 0.5681 - val_accuracy: 0.7043\n",
      "Epoch 108/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5707 - accuracy: 0.7002 - val_loss: 0.5671 - val_accuracy: 0.7063\n",
      "Epoch 109/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5734 - accuracy: 0.6964 - val_loss: 0.5687 - val_accuracy: 0.7071\n",
      "Epoch 110/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5712 - accuracy: 0.6969 - val_loss: 0.5680 - val_accuracy: 0.7060\n",
      "Epoch 111/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5707 - accuracy: 0.6967 - val_loss: 0.5681 - val_accuracy: 0.7057\n",
      "Epoch 112/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5696 - accuracy: 0.7019 - val_loss: 0.5677 - val_accuracy: 0.7057\n",
      "Epoch 113/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5698 - accuracy: 0.7007 - val_loss: 0.5674 - val_accuracy: 0.7055\n",
      "Epoch 114/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5692 - accuracy: 0.7022 - val_loss: 0.5674 - val_accuracy: 0.7046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5703 - accuracy: 0.7000 - val_loss: 0.5662 - val_accuracy: 0.7085\n",
      "Epoch 116/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5731 - accuracy: 0.6966 - val_loss: 0.5685 - val_accuracy: 0.7046\n",
      "Epoch 117/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5714 - accuracy: 0.6971 - val_loss: 0.5669 - val_accuracy: 0.7057\n",
      "Epoch 118/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5727 - accuracy: 0.6957 - val_loss: 0.5679 - val_accuracy: 0.7057\n",
      "Epoch 119/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5705 - accuracy: 0.6961 - val_loss: 0.5697 - val_accuracy: 0.7055\n",
      "Epoch 120/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5721 - accuracy: 0.6983 - val_loss: 0.5676 - val_accuracy: 0.7071\n",
      "Epoch 121/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5706 - accuracy: 0.6978 - val_loss: 0.5691 - val_accuracy: 0.7071\n",
      "Epoch 122/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5703 - accuracy: 0.7006 - val_loss: 0.5667 - val_accuracy: 0.7071\n",
      "Epoch 123/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5697 - accuracy: 0.7031 - val_loss: 0.5671 - val_accuracy: 0.7077\n",
      "Epoch 124/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5715 - accuracy: 0.7023 - val_loss: 0.5662 - val_accuracy: 0.7097\n",
      "Epoch 125/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5720 - accuracy: 0.6986 - val_loss: 0.5684 - val_accuracy: 0.7049\n",
      "Epoch 126/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5694 - accuracy: 0.6969 - val_loss: 0.5672 - val_accuracy: 0.7091\n",
      "Epoch 127/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5726 - accuracy: 0.6996 - val_loss: 0.5687 - val_accuracy: 0.7049\n",
      "Epoch 128/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5710 - accuracy: 0.6958 - val_loss: 0.5669 - val_accuracy: 0.7071\n",
      "Epoch 129/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5698 - accuracy: 0.7011 - val_loss: 0.5673 - val_accuracy: 0.7055\n",
      "Epoch 130/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5695 - accuracy: 0.6996 - val_loss: 0.5691 - val_accuracy: 0.7071\n",
      "Epoch 131/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5715 - accuracy: 0.6983 - val_loss: 0.5674 - val_accuracy: 0.7057\n",
      "Epoch 132/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5737 - accuracy: 0.6928 - val_loss: 0.5678 - val_accuracy: 0.7066\n",
      "Epoch 133/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5736 - accuracy: 0.6982 - val_loss: 0.5682 - val_accuracy: 0.7049\n",
      "Epoch 134/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5697 - accuracy: 0.6994 - val_loss: 0.5671 - val_accuracy: 0.7060\n",
      "Epoch 135/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5721 - accuracy: 0.7002 - val_loss: 0.5673 - val_accuracy: 0.7057\n",
      "Epoch 136/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5690 - accuracy: 0.7027 - val_loss: 0.5686 - val_accuracy: 0.7055\n",
      "Epoch 137/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5714 - accuracy: 0.6967 - val_loss: 0.5668 - val_accuracy: 0.7057\n",
      "Epoch 138/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5708 - accuracy: 0.6980 - val_loss: 0.5680 - val_accuracy: 0.7029\n",
      "Epoch 139/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5713 - accuracy: 0.7006 - val_loss: 0.5684 - val_accuracy: 0.7043\n",
      "Epoch 140/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5689 - accuracy: 0.7017 - val_loss: 0.5677 - val_accuracy: 0.7066\n",
      "Epoch 141/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5689 - accuracy: 0.7041 - val_loss: 0.5661 - val_accuracy: 0.7097\n",
      "Epoch 142/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5681 - accuracy: 0.7024 - val_loss: 0.5673 - val_accuracy: 0.7077\n",
      "Epoch 143/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5711 - accuracy: 0.6999 - val_loss: 0.5684 - val_accuracy: 0.7043\n",
      "Epoch 144/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5691 - accuracy: 0.6992 - val_loss: 0.5673 - val_accuracy: 0.7063\n",
      "Epoch 145/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5674 - accuracy: 0.7008 - val_loss: 0.5673 - val_accuracy: 0.7049\n",
      "Epoch 146/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5699 - accuracy: 0.6973 - val_loss: 0.5681 - val_accuracy: 0.7032\n",
      "Epoch 147/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5706 - accuracy: 0.6986 - val_loss: 0.5671 - val_accuracy: 0.7066\n",
      "Epoch 148/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5700 - accuracy: 0.6970 - val_loss: 0.5671 - val_accuracy: 0.7074\n",
      "Epoch 149/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5741 - accuracy: 0.6989 - val_loss: 0.5677 - val_accuracy: 0.7055\n",
      "Epoch 150/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5685 - accuracy: 0.7006 - val_loss: 0.5674 - val_accuracy: 0.7029\n",
      "Epoch 151/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5706 - accuracy: 0.6943 - val_loss: 0.5669 - val_accuracy: 0.7049\n",
      "Epoch 152/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5714 - accuracy: 0.6996 - val_loss: 0.5669 - val_accuracy: 0.7052\n",
      "Epoch 153/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5677 - accuracy: 0.6981 - val_loss: 0.5692 - val_accuracy: 0.7066\n",
      "Epoch 154/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5689 - accuracy: 0.6971 - val_loss: 0.5663 - val_accuracy: 0.7063\n",
      "Epoch 155/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5689 - accuracy: 0.6978 - val_loss: 0.5672 - val_accuracy: 0.7066\n",
      "Epoch 156/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5705 - accuracy: 0.6992 - val_loss: 0.5673 - val_accuracy: 0.7066\n",
      "Epoch 157/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5703 - accuracy: 0.6989 - val_loss: 0.5667 - val_accuracy: 0.7060\n",
      "Epoch 158/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5686 - accuracy: 0.7004 - val_loss: 0.5669 - val_accuracy: 0.7071\n",
      "Epoch 159/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5673 - accuracy: 0.7028 - val_loss: 0.5672 - val_accuracy: 0.7057\n",
      "Epoch 160/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5694 - accuracy: 0.6965 - val_loss: 0.5679 - val_accuracy: 0.7057\n",
      "Epoch 161/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5688 - accuracy: 0.7004 - val_loss: 0.5688 - val_accuracy: 0.7029\n",
      "Epoch 162/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5673 - accuracy: 0.6994 - val_loss: 0.5675 - val_accuracy: 0.7066\n",
      "Epoch 163/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5704 - accuracy: 0.6972 - val_loss: 0.5685 - val_accuracy: 0.7057\n",
      "Epoch 164/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5683 - accuracy: 0.7060 - val_loss: 0.5668 - val_accuracy: 0.7049\n",
      "Epoch 165/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5686 - accuracy: 0.6998 - val_loss: 0.5670 - val_accuracy: 0.7052\n",
      "Epoch 166/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5710 - accuracy: 0.6967 - val_loss: 0.5673 - val_accuracy: 0.7049\n",
      "Epoch 167/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5695 - accuracy: 0.7019 - val_loss: 0.5671 - val_accuracy: 0.7074\n",
      "Epoch 168/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5707 - accuracy: 0.7004 - val_loss: 0.5670 - val_accuracy: 0.7060\n",
      "Epoch 169/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5673 - accuracy: 0.7031 - val_loss: 0.5679 - val_accuracy: 0.7055\n",
      "Epoch 170/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5678 - accuracy: 0.7006 - val_loss: 0.5676 - val_accuracy: 0.7083\n",
      "Epoch 171/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5687 - accuracy: 0.7005 - val_loss: 0.5675 - val_accuracy: 0.7040\n",
      "Epoch 172/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5691 - accuracy: 0.7008 - val_loss: 0.5680 - val_accuracy: 0.7043\n",
      "Epoch 173/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5663 - accuracy: 0.7031 - val_loss: 0.5674 - val_accuracy: 0.7055\n",
      "Epoch 174/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5667 - accuracy: 0.7041 - val_loss: 0.5671 - val_accuracy: 0.7066\n",
      "Epoch 175/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5668 - accuracy: 0.7014 - val_loss: 0.5668 - val_accuracy: 0.7057\n",
      "Epoch 176/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5670 - accuracy: 0.6999 - val_loss: 0.5676 - val_accuracy: 0.7080\n",
      "Epoch 177/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5691 - accuracy: 0.6987 - val_loss: 0.5675 - val_accuracy: 0.7060\n",
      "Epoch 178/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5672 - accuracy: 0.6972 - val_loss: 0.5676 - val_accuracy: 0.7024\n",
      "Epoch 179/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5674 - accuracy: 0.6993 - val_loss: 0.5675 - val_accuracy: 0.7029\n",
      "Epoch 180/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5677 - accuracy: 0.7028 - val_loss: 0.5673 - val_accuracy: 0.7055\n",
      "Epoch 181/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5676 - accuracy: 0.7054 - val_loss: 0.5672 - val_accuracy: 0.7055\n",
      "Epoch 182/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5720 - accuracy: 0.6936 - val_loss: 0.5688 - val_accuracy: 0.7066\n",
      "Epoch 183/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5700 - accuracy: 0.7010 - val_loss: 0.5697 - val_accuracy: 0.7029\n",
      "Epoch 184/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5689 - accuracy: 0.6994 - val_loss: 0.5676 - val_accuracy: 0.7052\n",
      "Epoch 185/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5711 - accuracy: 0.6992 - val_loss: 0.5720 - val_accuracy: 0.7010\n",
      "Epoch 186/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5672 - accuracy: 0.6989 - val_loss: 0.5667 - val_accuracy: 0.7063\n",
      "Epoch 187/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5690 - accuracy: 0.6999 - val_loss: 0.5663 - val_accuracy: 0.7066\n",
      "Epoch 188/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5692 - accuracy: 0.7000 - val_loss: 0.5680 - val_accuracy: 0.7049\n",
      "Epoch 189/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5680 - accuracy: 0.7008 - val_loss: 0.5671 - val_accuracy: 0.7069\n",
      "Epoch 190/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5686 - accuracy: 0.6995 - val_loss: 0.5674 - val_accuracy: 0.7046\n",
      "Epoch 191/200\n",
      "260/260 [==============================] - 0s 1ms/step - loss: 0.5671 - accuracy: 0.7018 - val_loss: 0.5675 - val_accuracy: 0.7055\n",
      "371/371 [==============================] - 0s 1ms/step - loss: 0.5698 - accuracy: 0.6999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\myjr\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:430: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
      "  warnings.warn('`model.predict_proba()` is deprecated and '\n",
      "C:\\Users\\myjr\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:430: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
      "  warnings.warn('`model.predict_proba()` is deprecated and '\n",
      "C:\\Users\\myjr\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:430: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
      "  warnings.warn('`model.predict_proba()` is deprecated and '\n",
      "C:\\Users\\myjr\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:430: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
      "  warnings.warn('`model.predict_proba()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP train Brier score: 0.18977180160723062 \n",
      " test Brier score: 0.19587808978363402 \n",
      " AUROC: 0.6817445004462644\n",
      "Xgboost best layer size: {'max_depth': 10, 'n_estimators': 100, 'reg_alpha': 0.001, 'reg_lambda': 1000} \n",
      " best train score: 0.7039127810136172 \n",
      " test score: 0.7030103718694662 \n",
      " Xgboost train Brier score: 0.16887619509860682 \n",
      " test Brier score: 0.19520220967475316 \n",
      " AUROC: 0.6859977352673052\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression,LogisticRegression\n",
    "\n",
    "\n",
    "X_train1,X_test1,y_train1,y_test1=train_test_split(raw_extend[raw_extend['Gender']==1].drop(['ID','Recidivism_Within_3years','Recidivism_Arrest_Year1','Recidivism_Arrest_Year2','Recidivism_Arrest_Year3'],axis=1),raw_extend[raw_extend['Gender']==1]['Recidivism_Arrest_Year1'])\n",
    "fpr_list,tpr_list,auc_list=dict(),dict(),dict()\n",
    "\n",
    "\n",
    "logistic=LogisticRegression(max_iter=1000)\n",
    "logistic.fit(X_train1.fillna(0),y_train1)\n",
    "fpr_list[0], tpr_list[0], _ = roc_curve(y_test1, y_roc(logistic,X_test1.fillna(0)))\n",
    "print('Logistic regression train score:',\n",
    "      logistic.score(X_train1.fillna(0),y_train1),'\\n test score:',logistic.score(X_test1.fillna(0),y_test1.fillna(0)),'\\n Logistic regression train Brier score:',\n",
    "      brier_score(logistic.predict_proba(X_train1.fillna(0)),y_train1),'\\n test Brier score:',brier_score(logistic.predict_proba(X_test1.fillna(0)),y_test1),\n",
    "     '\\n AUROC:',roc_auc_score(y_test1, y_roc(logistic,X_test1.fillna(0))))\n",
    "\n",
    "RF=RandomForestClassifier(n_estimators=150,min_samples_split=2)\n",
    "RF.fit(X_train1.fillna(0),y_train1)\n",
    "fpr_list[1], tpr_list[1], _ = roc_curve(y_test1, y_roc(RF,X_test1.fillna(0)))\n",
    "print('Random forest train score:',\n",
    "      RF.score(X_train1.fillna(0),y_train1),'\\n test score:',RF.score(X_test1.fillna(0),y_test1.fillna(0)),'\\n Random forest  train Brier score:',\n",
    "      brier_score(RF.predict_proba(X_train1.fillna(0)),y_train1),'\\n test Brier score:',brier_score(RF.predict_proba(X_test1.fillna(0)),y_test1),\n",
    "     '\\n AUROC:',roc_auc_score(y_test1, y_roc(RF,X_test1.fillna(0))))\n",
    "\n",
    "GBDT=GradientBoostingClassifier()\n",
    "params_SGD={'n_estimators':[150],'min_samples_split':[2,4]}\n",
    "grid_SGD=GridSearchCV(GBDT,param_grid=params_SGD,cv=3)\n",
    "grid_SGD.fit(X_train1.fillna(0),y_train1.fillna(0))\n",
    "fpr_list[2], tpr_list[2], _ = roc_curve(y_test1, y_roc(grid_SGD.best_estimator_,X_test1.fillna(0)))\n",
    "print('SGD best layer size:',grid_SGD.best_params_,'\\n best train score:',\n",
    "      grid_SGD.best_score_,'\\n test score:',grid_SGD.score(X_test1.fillna(0),y_test1.fillna(0)),'\\n SGD  train Brier score:',\n",
    "      brier_score(grid_SGD.predict_proba(X_train1.fillna(0)),y_train1),'\\n test Brier score:',brier_score(grid_SGD.predict_proba(X_test1.fillna(0)),y_test1),\n",
    "     '\\n AUROC:',roc_auc_score(y_test1, y_roc(grid_SGD.best_estimator_,X_test1.fillna(0))))\n",
    "\n",
    "pipe = Sequential()\n",
    "n_cols = X_train1.shape[1]\n",
    "pipe.add(BatchNormalization(input_shape=(n_cols,)))\n",
    "pipe.add(Dense(70, activation= 'linear'))\n",
    "pipe.add(Dropout(0.3))\n",
    "pipe.add(Dense(50, activation= 'relu'))\n",
    "pipe.add(Dropout(0.3))\n",
    "pipe.add(Dense(50, activation= 'relu'))\n",
    "pipe.add(Dropout(0.3))\n",
    "#model.add(Dense(100, activation='relu'))\n",
    "#model.add(Dropout(0.3))\n",
    "#model.add(Dense(100, activation='linear'))\n",
    "pipe.add(BatchNormalization())\n",
    "pipe.add(Dense(2, activation='softmax'))\n",
    "    #model.compile(\n",
    "        #optimizer='Adam',\n",
    "        #loss='mean_squared_error',\n",
    "        #metrics=['accuracy'])\n",
    "early_stopping_monitor = EarlyStopping(patience=50)\n",
    "sgd = keras.optimizers.SGD(lr=.001, decay=2e-4, momentum=0.9, nesterov=True)\n",
    "pipe.compile(loss = 'sparse_categorical_crossentropy',optimizer = 'sgd', metrics=['accuracy'])\n",
    "history=pipe.fit(X_train1.fillna(0).astype('float32'), y_train1, validation_split=0.3, epochs=200, callbacks=[early_stopping_monitor])\n",
    "#history=model.fit(X_train, y_train, validation_split=0.2, epochs=25)\n",
    "score = pipe.evaluate(X_test1.fillna(0).astype('float32'), y_test1, verbose=0)\n",
    "pipe.fit(X_train1.fillna(0).astype('float32'),y_train1.fillna(0))\n",
    "fpr_list[3], tpr_list[3], _ = roc_curve(y_test1, y_roc(pipe,X_test1.fillna(0).astype('float32')))\n",
    "print('MLP train Brier score:',\n",
    "      brier_score(pipe.predict_proba(X_train1.fillna(0).astype('float32')),y_train1),'\\n test Brier score:',brier_score(pipe.predict_proba(X_test1.fillna(0).astype('float32')),y_test1),\n",
    "     '\\n AUROC:',roc_auc_score(y_test1, y_roc(pipe,X_test1.fillna(0).astype('float32'))))\n",
    "\n",
    "\n",
    "XGB=XGBClassifier()\n",
    "params_XGB={'n_estimators':[100,200],'max_depth':[8,10],'reg_alpha':[0.001],'reg_lambda':[10000,1000,500]}\n",
    "grid_XGB=GridSearchCV(XGB,param_grid=params_XGB,cv=3)\n",
    "grid_XGB.fit(X_train1.fillna(0),y_train1.fillna(0))\n",
    "fpr_list[4], tpr_list[4], _ = roc_curve(y_test1, y_roc(grid_XGB.best_estimator_,X_test1.fillna(0)))\n",
    "print('Xgboost best layer size:',grid_XGB.best_params_,'\\n best train score:',\n",
    "      grid_XGB.best_score_,'\\n test score:',grid_XGB.score(X_test1.fillna(0),y_test1.fillna(0)),'\\n Xgboost train Brier score:',\n",
    "      brier_score(grid_XGB.predict_proba(X_train1.fillna(0)),y_train1),'\\n test Brier score:',brier_score(grid_XGB.predict_proba(X_test1.fillna(0)),y_test1),\n",
    "     '\\n AUROC:',roc_auc_score(y_test1, y_roc(grid_XGB.best_estimator_,X_test1.fillna(0))))\n",
    "\n",
    "\n",
    "\n",
    "colors = cycle(['aqua', 'red', 'green','orange','blue'])\n",
    "labels=['Logistic Regression','Random Forest','Gradient Boosting','MLP','Xgboost']\n",
    "for i, label, color in zip(range(len(fpr_list)), labels, colors):\n",
    "    legend= label + ' (area = {1:0.2f})'''.format(i, auc(fpr_list[i], tpr_list[i]))\n",
    "    plt.plot(fpr_list[i], tpr_list[i], color=color, lw=2,label=legend)\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC curve for different methods on Recidivism_1Year')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('ROC_plot.png',dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7ef03ae8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Race</th>\n",
       "      <th>Age_at_Release</th>\n",
       "      <th>Residence_PUMA</th>\n",
       "      <th>Gang_Affiliated</th>\n",
       "      <th>Supervision_Risk_Score_First</th>\n",
       "      <th>Supervision_Level_First</th>\n",
       "      <th>Education_Level</th>\n",
       "      <th>Dependents</th>\n",
       "      <th>...</th>\n",
       "      <th>FSMXSP</th>\n",
       "      <th>FSTOVP</th>\n",
       "      <th>FTAXP</th>\n",
       "      <th>FTELP</th>\n",
       "      <th>FTENP</th>\n",
       "      <th>FTOILP</th>\n",
       "      <th>FVACSP</th>\n",
       "      <th>FVALP</th>\n",
       "      <th>FVEHP</th>\n",
       "      <th>FWATP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.733539</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.100330</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.286229</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.258442</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.654887</td>\n",
       "      <td>0.481108</td>\n",
       "      <td>0.457411</td>\n",
       "      <td>3.631924</td>\n",
       "      <td>-1.068371</td>\n",
       "      <td>-0.302085</td>\n",
       "      <td>-1.402483</td>\n",
       "      <td>0.231283</td>\n",
       "      <td>-0.668677</td>\n",
       "      <td>-0.620890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.733409</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.039825</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.027176</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.389139</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.654887</td>\n",
       "      <td>0.481108</td>\n",
       "      <td>0.457411</td>\n",
       "      <td>3.631924</td>\n",
       "      <td>-1.068371</td>\n",
       "      <td>-0.302085</td>\n",
       "      <td>-1.402483</td>\n",
       "      <td>0.231283</td>\n",
       "      <td>-0.668677</td>\n",
       "      <td>-0.620890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.733150</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.570077</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0.392508</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.389139</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.654887</td>\n",
       "      <td>0.481108</td>\n",
       "      <td>0.457411</td>\n",
       "      <td>3.631924</td>\n",
       "      <td>-1.068371</td>\n",
       "      <td>-0.302085</td>\n",
       "      <td>-1.402483</td>\n",
       "      <td>0.231283</td>\n",
       "      <td>-0.668677</td>\n",
       "      <td>-0.620890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.733021</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.039825</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.866544</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.258442</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.654887</td>\n",
       "      <td>0.481108</td>\n",
       "      <td>0.457411</td>\n",
       "      <td>3.631924</td>\n",
       "      <td>-1.068371</td>\n",
       "      <td>-0.302085</td>\n",
       "      <td>-1.402483</td>\n",
       "      <td>0.231283</td>\n",
       "      <td>-0.668677</td>\n",
       "      <td>-0.620890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.732373</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.100330</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.446860</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.258442</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.654887</td>\n",
       "      <td>0.481108</td>\n",
       "      <td>0.457411</td>\n",
       "      <td>3.631924</td>\n",
       "      <td>-1.068371</td>\n",
       "      <td>-0.302085</td>\n",
       "      <td>-1.402483</td>\n",
       "      <td>0.231283</td>\n",
       "      <td>-0.668677</td>\n",
       "      <td>-0.620890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18023</th>\n",
       "      <td>1.718110</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.020680</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.812193</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.389139</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.780227</td>\n",
       "      <td>0.805995</td>\n",
       "      <td>-0.148737</td>\n",
       "      <td>-0.195273</td>\n",
       "      <td>-0.945059</td>\n",
       "      <td>1.380755</td>\n",
       "      <td>2.569246</td>\n",
       "      <td>0.621093</td>\n",
       "      <td>-1.121273</td>\n",
       "      <td>-0.501324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18024</th>\n",
       "      <td>1.721218</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.490428</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.446860</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.258442</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.780227</td>\n",
       "      <td>0.805995</td>\n",
       "      <td>-0.148737</td>\n",
       "      <td>-0.195273</td>\n",
       "      <td>-0.945059</td>\n",
       "      <td>1.380755</td>\n",
       "      <td>2.569246</td>\n",
       "      <td>0.621093</td>\n",
       "      <td>-1.121273</td>\n",
       "      <td>-0.501324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18025</th>\n",
       "      <td>1.727305</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.570077</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.125597</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.434651</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.780227</td>\n",
       "      <td>0.805995</td>\n",
       "      <td>-0.148737</td>\n",
       "      <td>-0.195273</td>\n",
       "      <td>-0.945059</td>\n",
       "      <td>1.380755</td>\n",
       "      <td>2.569246</td>\n",
       "      <td>0.621093</td>\n",
       "      <td>-1.121273</td>\n",
       "      <td>-0.501324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18026</th>\n",
       "      <td>1.728341</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.100330</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.286229</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.434651</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.780227</td>\n",
       "      <td>0.805995</td>\n",
       "      <td>-0.148737</td>\n",
       "      <td>-0.195273</td>\n",
       "      <td>-0.945059</td>\n",
       "      <td>1.380755</td>\n",
       "      <td>2.569246</td>\n",
       "      <td>0.621093</td>\n",
       "      <td>-1.121273</td>\n",
       "      <td>-0.501324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18027</th>\n",
       "      <td>1.730543</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.630582</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.446860</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.258442</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.780227</td>\n",
       "      <td>0.805995</td>\n",
       "      <td>-0.148737</td>\n",
       "      <td>-0.195273</td>\n",
       "      <td>-0.945059</td>\n",
       "      <td>1.380755</td>\n",
       "      <td>2.569246</td>\n",
       "      <td>0.621093</td>\n",
       "      <td>-1.121273</td>\n",
       "      <td>-0.501324</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15811 rows × 179 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             ID  Gender  Race  Age_at_Release  Residence_PUMA  \\\n",
       "0     -1.733539       1     0        1.100330              15   \n",
       "1     -1.733409       1     0        0.039825              15   \n",
       "2     -1.733150       1     1        0.570077              15   \n",
       "3     -1.733021       1     1        0.039825              15   \n",
       "4     -1.732373       1     0        1.100330              15   \n",
       "...         ...     ...   ...             ...             ...   \n",
       "18023  1.718110       1     0       -1.020680               6   \n",
       "18024  1.721218       1     1       -0.490428               6   \n",
       "18025  1.727305       1     1        0.570077               6   \n",
       "18026  1.728341       1     1        1.100330               6   \n",
       "18027  1.730543       1     1        1.630582               6   \n",
       "\n",
       "       Gang_Affiliated  Supervision_Risk_Score_First  Supervision_Level_First  \\\n",
       "0                    0                     -1.286229                        2   \n",
       "1                    0                     -0.027176                        1   \n",
       "2                    0                      0.392508                        0   \n",
       "3                    0                     -0.866544                        1   \n",
       "4                    0                     -0.446860                        2   \n",
       "...                ...                           ...                      ...   \n",
       "18023                0                      0.812193                        0   \n",
       "18024                0                     -0.446860                        2   \n",
       "18025                0                     -2.125597                        2   \n",
       "18026                0                     -1.286229                        2   \n",
       "18027                0                     -0.446860                        2   \n",
       "\n",
       "       Education_Level  Dependents  ...    FSMXSP    FSTOVP     FTAXP  \\\n",
       "0                    0    1.258442  ... -0.654887  0.481108  0.457411   \n",
       "1                    2   -0.389139  ... -0.654887  0.481108  0.457411   \n",
       "2                    2   -0.389139  ... -0.654887  0.481108  0.457411   \n",
       "3                    2    1.258442  ... -0.654887  0.481108  0.457411   \n",
       "4                    1    1.258442  ... -0.654887  0.481108  0.457411   \n",
       "...                ...         ...  ...       ...       ...       ...   \n",
       "18023                1   -0.389139  ... -0.780227  0.805995 -0.148737   \n",
       "18024                0    1.258442  ... -0.780227  0.805995 -0.148737   \n",
       "18025                1    0.434651  ... -0.780227  0.805995 -0.148737   \n",
       "18026                1    0.434651  ... -0.780227  0.805995 -0.148737   \n",
       "18027                0    1.258442  ... -0.780227  0.805995 -0.148737   \n",
       "\n",
       "          FTELP     FTENP    FTOILP    FVACSP     FVALP     FVEHP     FWATP  \n",
       "0      3.631924 -1.068371 -0.302085 -1.402483  0.231283 -0.668677 -0.620890  \n",
       "1      3.631924 -1.068371 -0.302085 -1.402483  0.231283 -0.668677 -0.620890  \n",
       "2      3.631924 -1.068371 -0.302085 -1.402483  0.231283 -0.668677 -0.620890  \n",
       "3      3.631924 -1.068371 -0.302085 -1.402483  0.231283 -0.668677 -0.620890  \n",
       "4      3.631924 -1.068371 -0.302085 -1.402483  0.231283 -0.668677 -0.620890  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "18023 -0.195273 -0.945059  1.380755  2.569246  0.621093 -1.121273 -0.501324  \n",
       "18024 -0.195273 -0.945059  1.380755  2.569246  0.621093 -1.121273 -0.501324  \n",
       "18025 -0.195273 -0.945059  1.380755  2.569246  0.621093 -1.121273 -0.501324  \n",
       "18026 -0.195273 -0.945059  1.380755  2.569246  0.621093 -1.121273 -0.501324  \n",
       "18027 -0.195273 -0.945059  1.380755  2.569246  0.621093 -1.121273 -0.501324  \n",
       "\n",
       "[15811 rows x 179 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_extend[raw_extend['Gender']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "85230c7e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14789770544363473"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brier_score(logistic.predict_proba(X_train1[X_train1['Gender']==0].fillna(0)),y_train1[X_train1['Gender']==0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
