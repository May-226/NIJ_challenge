{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ignored-bryan",
   "metadata": {},
   "source": [
    "### Util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "desperate-charity",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key savefig.frameon in file C:\\Users\\myjr\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle, line 421 ('savefig.frameon : True')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.4.2/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key verbose.level in file C:\\Users\\myjr\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle, line 472 ('verbose.level  : silent      # one of silent, helpful, debug, debug-annoying')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.4.2/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key verbose.fileo in file C:\\Users\\myjr\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle, line 473 ('verbose.fileo  : sys.stdout  # a log filename, sys.stdout or sys.stderr')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.4.2/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "In C:\\Users\\myjr\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The text.latex.preview rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\myjr\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The mathtext.fallback_to_cm rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\myjr\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: Support for setting the 'mathtext.fallback_to_cm' rcParam is deprecated since 3.3 and will be removed two minor releases later; use 'mathtext.fallback : 'cm' instead.\n",
      "In C:\\Users\\myjr\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The validate_bool_maybe_none function was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\myjr\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The savefig.jpeg_quality rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\myjr\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The keymap.all_axes rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\myjr\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The animation.avconv_path rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\myjr\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The animation.avconv_args rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import interp\n",
    "#import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from IPython.display import Image\n",
    "import pydotplus\n",
    "import graphviz\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression,LogisticRegression,Ridge\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier,RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV,train_test_split,cross_val_score,cross_validate\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_squared_error, roc_auc_score,roc_curve, auc, brier_score_loss\n",
    "from sklearn import tree\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from itertools import cycle\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, BatchNormalization, Activation, Embedding\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "forced-chester",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "def y_roc(estimator,X):\n",
    "    y_scores=[]\n",
    "    for list in estimator.predict_proba(X):\n",
    "        y_scores.append(list[1])\n",
    "    return y_scores\n",
    "def y_roc_regression(estimator,X):\n",
    "    y_scores=[]\n",
    "    for list in estimator.predict(X):\n",
    "        y_scores.append(list)\n",
    "    return y_scores\n",
    "def cv_roc_plot(estimator,X,y):\n",
    "    cv = StratifiedKFold(n_splits=4,shuffle=False)\n",
    "    tprs = []\n",
    "    aucs = []\n",
    "    mean_fpr = np.linspace(0,1,100)\n",
    "    for train,test in cv.split(X,y):\n",
    "        prediction = estimator.fit(X.iloc[train],y.iloc[train]).predict_proba(X.iloc[test])\n",
    "        fpr, tpr, t = roc_curve(y.iloc[test], prediction[:, 1])\n",
    "        tpr[0]=0\n",
    "        tpr[-1]=1\n",
    "        tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        aucs.append(roc_auc)\n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "    return mean_fpr, mean_tpr, mean_auc\n",
    "def brier_score(y_prob_raw,y_true):\n",
    "    y_prob=[prob[1] for prob in y_prob_raw]\n",
    "    if len(y_prob)!=len(y_true):\n",
    "        print('Error: two lists must have same length')\n",
    "        return\n",
    "    out = 0\n",
    "    for prob_1,y in zip(y_prob,y_true):\n",
    "        out+=(prob_1-y)**2\n",
    "    return out/len(y_prob)\n",
    "def get_prob_1(y_prob_raw):\n",
    "    return [prob[1] for prob in y_prob_raw]\n",
    "def aver_prob(prob_lists):\n",
    "    n=len(prob_lists)\n",
    "    return np.sum(np.array(prob_lists),0)/n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accredited-moldova",
   "metadata": {},
   "source": [
    "### PUMA related data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "genetic-convert",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    7607\n",
       "True     1791\n",
       "Name: Recidivism_Arrest_Year3, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train.Recidivism_Arrest_Year3.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "combined-leeds",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'Gender', 'Race', 'Age_at_Release', 'Residence_PUMA',\n",
       "       'Gang_Affiliated', 'Supervision_Risk_Score_First',\n",
       "       'Supervision_Level_First', 'Education_Level', 'Dependents',\n",
       "       'Prison_Offense', 'Prison_Years', 'Prior_Arrest_Episodes_Felony',\n",
       "       'Prior_Arrest_Episodes_Misd', 'Prior_Arrest_Episodes_Violent',\n",
       "       'Prior_Arrest_Episodes_Property', 'Prior_Arrest_Episodes_Drug',\n",
       "       'Prior_Arrest_Episodes_PPViolationCharges',\n",
       "       'Prior_Arrest_Episodes_DVCharges', 'Prior_Arrest_Episodes_GunCharges',\n",
       "       'Prior_Conviction_Episodes_Felony', 'Prior_Conviction_Episodes_Misd',\n",
       "       'Prior_Conviction_Episodes_Viol', 'Prior_Conviction_Episodes_Prop',\n",
       "       'Prior_Conviction_Episodes_Drug',\n",
       "       'Prior_Conviction_Episodes_PPViolationCharges',\n",
       "       'Prior_Conviction_Episodes_DomesticViolenceCharges',\n",
       "       'Prior_Conviction_Episodes_GunCharges', 'Prior_Revocations_Parole',\n",
       "       'Prior_Revocations_Probation', 'Condition_MH_SA', 'Condition_Cog_Ed',\n",
       "       'Condition_Other', 'Violations_ElectronicMonitoring',\n",
       "       'Violations_Instruction', 'Violations_FailToReport',\n",
       "       'Violations_MoveWithoutPermission', 'Delinquency_Reports',\n",
       "       'Program_Attendances', 'Program_UnexcusedAbsences', 'Residence_Changes',\n",
       "       'Avg_Days_per_DrugTest', 'DrugTests_THC_Positive',\n",
       "       'DrugTests_Cocaine_Positive', 'DrugTests_Meth_Positive',\n",
       "       'DrugTests_Other_Positive', 'Percent_Days_Employed', 'Jobs_Per_Year',\n",
       "       'Employment_Exempt', 'Recidivism_Within_3years',\n",
       "       'Recidivism_Arrest_Year1', 'Recidivism_Arrest_Year2',\n",
       "       'Recidivism_Arrest_Year3'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All features are available\n",
    "raw_train=pd.read_csv(r'.\\data\\NIJ_s_Recidivism_Challenge_Training_Dataset.csv')\n",
    "raw_train=raw_train.drop(raw_train[raw_train.Recidivism_Arrest_Year1==True].index)\n",
    "raw_train=raw_train.drop(raw_train[raw_train.Recidivism_Arrest_Year2==True].index)\n",
    "raw_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acknowledged-bumper",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RT</th>\n",
       "      <th>SERIALNO</th>\n",
       "      <th>DIVISION</th>\n",
       "      <th>PUMA</th>\n",
       "      <th>REGION</th>\n",
       "      <th>ST</th>\n",
       "      <th>ADJHSG</th>\n",
       "      <th>ADJINC</th>\n",
       "      <th>WGTP</th>\n",
       "      <th>NP</th>\n",
       "      <th>...</th>\n",
       "      <th>wgtp72</th>\n",
       "      <th>wgtp73</th>\n",
       "      <th>wgtp74</th>\n",
       "      <th>wgtp75</th>\n",
       "      <th>wgtp76</th>\n",
       "      <th>wgtp77</th>\n",
       "      <th>wgtp78</th>\n",
       "      <th>wgtp79</th>\n",
       "      <th>wgtp80</th>\n",
       "      <th>Code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H</td>\n",
       "      <td>127</td>\n",
       "      <td>5</td>\n",
       "      <td>1400</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>1000000</td>\n",
       "      <td>1007549</td>\n",
       "      <td>139</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>242</td>\n",
       "      <td>41</td>\n",
       "      <td>124</td>\n",
       "      <td>132</td>\n",
       "      <td>224</td>\n",
       "      <td>157</td>\n",
       "      <td>237</td>\n",
       "      <td>280</td>\n",
       "      <td>142</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>H</td>\n",
       "      <td>131</td>\n",
       "      <td>5</td>\n",
       "      <td>700</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>1000000</td>\n",
       "      <td>1007549</td>\n",
       "      <td>60</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>95</td>\n",
       "      <td>96</td>\n",
       "      <td>19</td>\n",
       "      <td>70</td>\n",
       "      <td>53</td>\n",
       "      <td>12</td>\n",
       "      <td>61</td>\n",
       "      <td>18</td>\n",
       "      <td>111</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>H</td>\n",
       "      <td>136</td>\n",
       "      <td>5</td>\n",
       "      <td>2200</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>1000000</td>\n",
       "      <td>1007549</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>23</td>\n",
       "      <td>33</td>\n",
       "      <td>40</td>\n",
       "      <td>38</td>\n",
       "      <td>19</td>\n",
       "      <td>33</td>\n",
       "      <td>19</td>\n",
       "      <td>8</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>H</td>\n",
       "      <td>151</td>\n",
       "      <td>5</td>\n",
       "      <td>1100</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>1000000</td>\n",
       "      <td>1007549</td>\n",
       "      <td>213</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>55</td>\n",
       "      <td>175</td>\n",
       "      <td>62</td>\n",
       "      <td>53</td>\n",
       "      <td>313</td>\n",
       "      <td>200</td>\n",
       "      <td>330</td>\n",
       "      <td>252</td>\n",
       "      <td>239</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>H</td>\n",
       "      <td>294</td>\n",
       "      <td>5</td>\n",
       "      <td>3400</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>1000000</td>\n",
       "      <td>1007549</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>59</td>\n",
       "      <td>19</td>\n",
       "      <td>61</td>\n",
       "      <td>109</td>\n",
       "      <td>102</td>\n",
       "      <td>99</td>\n",
       "      <td>74</td>\n",
       "      <td>73</td>\n",
       "      <td>97</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45693</th>\n",
       "      <td>H</td>\n",
       "      <td>1492766</td>\n",
       "      <td>5</td>\n",
       "      <td>5002</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>1000000</td>\n",
       "      <td>1007549</td>\n",
       "      <td>69</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>60</td>\n",
       "      <td>95</td>\n",
       "      <td>23</td>\n",
       "      <td>114</td>\n",
       "      <td>81</td>\n",
       "      <td>64</td>\n",
       "      <td>126</td>\n",
       "      <td>67</td>\n",
       "      <td>99</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45694</th>\n",
       "      <td>H</td>\n",
       "      <td>1492793</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>1000000</td>\n",
       "      <td>1007549</td>\n",
       "      <td>53</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>132</td>\n",
       "      <td>51</td>\n",
       "      <td>60</td>\n",
       "      <td>106</td>\n",
       "      <td>39</td>\n",
       "      <td>77</td>\n",
       "      <td>57</td>\n",
       "      <td>49</td>\n",
       "      <td>79</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45695</th>\n",
       "      <td>H</td>\n",
       "      <td>1492802</td>\n",
       "      <td>5</td>\n",
       "      <td>700</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>1000000</td>\n",
       "      <td>1007549</td>\n",
       "      <td>48</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>53</td>\n",
       "      <td>59</td>\n",
       "      <td>80</td>\n",
       "      <td>40</td>\n",
       "      <td>68</td>\n",
       "      <td>85</td>\n",
       "      <td>76</td>\n",
       "      <td>105</td>\n",
       "      <td>43</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45696</th>\n",
       "      <td>H</td>\n",
       "      <td>1492814</td>\n",
       "      <td>5</td>\n",
       "      <td>2800</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>1000000</td>\n",
       "      <td>1007549</td>\n",
       "      <td>109</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>84</td>\n",
       "      <td>114</td>\n",
       "      <td>101</td>\n",
       "      <td>79</td>\n",
       "      <td>35</td>\n",
       "      <td>203</td>\n",
       "      <td>117</td>\n",
       "      <td>108</td>\n",
       "      <td>182</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45697</th>\n",
       "      <td>H</td>\n",
       "      <td>1492830</td>\n",
       "      <td>5</td>\n",
       "      <td>4000</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>1000000</td>\n",
       "      <td>1007549</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>42</td>\n",
       "      <td>9</td>\n",
       "      <td>26</td>\n",
       "      <td>28</td>\n",
       "      <td>9</td>\n",
       "      <td>30</td>\n",
       "      <td>28</td>\n",
       "      <td>8</td>\n",
       "      <td>47</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45698 rows × 232 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      RT  SERIALNO  DIVISION  PUMA  REGION  ST   ADJHSG   ADJINC  WGTP  NP  \\\n",
       "0      H       127         5  1400       3  13  1000000  1007549   139   1   \n",
       "1      H       131         5   700       3  13  1000000  1007549    60   3   \n",
       "2      H       136         5  2200       3  13  1000000  1007549    22   1   \n",
       "3      H       151         5  1100       3  13  1000000  1007549   213   2   \n",
       "4      H       294         5  3400       3  13  1000000  1007549    64   2   \n",
       "...   ..       ...       ...   ...     ...  ..      ...      ...   ...  ..   \n",
       "45693  H   1492766         5  5002       3  13  1000000  1007549    69   1   \n",
       "45694  H   1492793         5   100       3  13  1000000  1007549    53   2   \n",
       "45695  H   1492802         5   700       3  13  1000000  1007549    48   2   \n",
       "45696  H   1492814         5  2800       3  13  1000000  1007549   109   2   \n",
       "45697  H   1492830         5  4000       3  13  1000000  1007549    28   1   \n",
       "\n",
       "       ...  wgtp72  wgtp73  wgtp74  wgtp75  wgtp76  wgtp77  wgtp78  wgtp79  \\\n",
       "0      ...     242      41     124     132     224     157     237     280   \n",
       "1      ...      95      96      19      70      53      12      61      18   \n",
       "2      ...       7      23      33      40      38      19      33      19   \n",
       "3      ...      55     175      62      53     313     200     330     252   \n",
       "4      ...      59      19      61     109     102      99      74      73   \n",
       "...    ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "45693  ...      60      95      23     114      81      64     126      67   \n",
       "45694  ...     132      51      60     106      39      77      57      49   \n",
       "45695  ...      53      59      80      40      68      85      76     105   \n",
       "45696  ...      84     114     101      79      35     203     117     108   \n",
       "45697  ...      42       9      26      28       9      30      28       8   \n",
       "\n",
       "       wgtp80  Code  \n",
       "0         142     4  \n",
       "1         111    18  \n",
       "2           8    22  \n",
       "3         239    19  \n",
       "4          97    12  \n",
       "...       ...   ...  \n",
       "45693      99    10  \n",
       "45694      79     7  \n",
       "45695      43    18  \n",
       "45696     182    17  \n",
       "45697      47     8  \n",
       "\n",
       "[45698 rows x 232 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_PUMA=pd.read_csv(r'.\\data\\ss13hga_13.csv')\n",
    "pdata_PUMA=pd.read_csv(r'.\\data\\ss13pga_13.csv')\n",
    "data_PUMA['Code']=None\n",
    "pdata_PUMA['Code']=None\n",
    "data_PUMA['PUMA']=data_PUMA['PUMA'].astype(str)\n",
    "pdata_PUMA['PUMA']=pdata_PUMA['PUMA'].astype(str)\n",
    "PUMA_code_map={'1003':1,'4400':1,'1008':2,'4300':2,'1200':3, '1300':3,'1400':4,'1500':4,'1600':4,'1700':5,'1800':5,\n",
    "              '2001':6,'2002':6,'2003':6,'4005':6,'100':7,'200':7,'500':7,'4000':8,'4100':8,'4200':8,'5001':9,'6001':9,'6002':9,\n",
    "              '2400':10,'5002':10,'1001':11,'3004':11,'4600':11,'1002':12, '1005':12, '3300':12, '3400':12, '4001':12, '4002':12,\n",
    "              '4006':12,'3101':13,'3102':13,'1900':14, '3900':14, '4003':14, '4004':14,'3001':15, '3002':15, '3003':15, '3005':15,\n",
    "              '2500':16, '4500':16,'2800':17, '2900':17, '3200':17, '3500':17,'600':18, '700':18, '800':18,'900':19, '1100':19,\n",
    "              '300':20, '401':20, '402':20, '1004':21, '2100':21 ,'2200':22, '2300':22, '1006':23, '1007':23, '2004':23,'2600':24, \n",
    "              '2700':24,'3600':25, '3700':25, '3800':25}\n",
    "data_PUMA['Code']=data_PUMA['PUMA'].map(PUMA_code_map)\n",
    "pdata_PUMA['Code']=pdata_PUMA['PUMA'].map(PUMA_code_map)\n",
    "data_PUMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "rising-australia",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>County</th>\n",
       "      <th>Total law\\nenforcement\\nemployees</th>\n",
       "      <th>Total\\nofficers</th>\n",
       "      <th>Total\\ncivilians</th>\n",
       "      <th>Murder</th>\n",
       "      <th>Rape</th>\n",
       "      <th>Robbery</th>\n",
       "      <th>Assault</th>\n",
       "      <th>Burglary</th>\n",
       "      <th>Larceny</th>\n",
       "      <th>Vehicle Theft</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Barrow</td>\n",
       "      <td>190</td>\n",
       "      <td>129</td>\n",
       "      <td>61</td>\n",
       "      <td>2</td>\n",
       "      <td>33</td>\n",
       "      <td>20</td>\n",
       "      <td>302</td>\n",
       "      <td>547</td>\n",
       "      <td>1551</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bartow</td>\n",
       "      <td>234</td>\n",
       "      <td>197</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>32</td>\n",
       "      <td>329</td>\n",
       "      <td>806</td>\n",
       "      <td>2263</td>\n",
       "      <td>264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bibb</td>\n",
       "      <td>361</td>\n",
       "      <td>296</td>\n",
       "      <td>65</td>\n",
       "      <td>19</td>\n",
       "      <td>67</td>\n",
       "      <td>296</td>\n",
       "      <td>416</td>\n",
       "      <td>2500</td>\n",
       "      <td>6297</td>\n",
       "      <td>715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Brantley</td>\n",
       "      <td>43</td>\n",
       "      <td>19</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "      <td>191</td>\n",
       "      <td>243</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Brooks</td>\n",
       "      <td>51</td>\n",
       "      <td>22</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>45</td>\n",
       "      <td>114</td>\n",
       "      <td>291</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>Webster</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>White</td>\n",
       "      <td>73</td>\n",
       "      <td>40</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>35</td>\n",
       "      <td>153</td>\n",
       "      <td>328</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>Wilcox</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>72</td>\n",
       "      <td>93</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>Wilkes</td>\n",
       "      <td>27</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>29</td>\n",
       "      <td>106</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>Wilkinson</td>\n",
       "      <td>28</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>47</td>\n",
       "      <td>75</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>125 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        County  Total law\\nenforcement\\nemployees  Total\\nofficers  \\\n",
       "0       Barrow                                190              129   \n",
       "1       Bartow                                234              197   \n",
       "2         Bibb                                361              296   \n",
       "3     Brantley                                 43               19   \n",
       "4       Brooks                                 51               22   \n",
       "..         ...                                ...              ...   \n",
       "120    Webster                                  6                5   \n",
       "121      White                                 73               40   \n",
       "122     Wilcox                                 16               10   \n",
       "123     Wilkes                                 27               14   \n",
       "124  Wilkinson                                 28               16   \n",
       "\n",
       "     Total\\ncivilians Murder Rape Robbery Assault Burglary Larceny  \\\n",
       "0                  61      2   33      20     302      547    1551   \n",
       "1                  37      0   13      32     329      806    2263   \n",
       "2                  65     19   67     296     416     2500    6297   \n",
       "3                  24      0    1       7      16      191     243   \n",
       "4                  29      2    2       9      45      114     291   \n",
       "..                ...    ...  ...     ...     ...      ...     ...   \n",
       "120                 1      0    0       1       0        8       7   \n",
       "121                33      0    3       4      35      153     328   \n",
       "122                 6      0    5       2      16       72      93   \n",
       "123                13      0    0       0      32       29     106   \n",
       "124                12      0    0       0       7       47      75   \n",
       "\n",
       "    Vehicle Theft  \n",
       "0             145  \n",
       "1             264  \n",
       "2             715  \n",
       "3              10  \n",
       "4              31  \n",
       "..            ...  \n",
       "120             3  \n",
       "121            15  \n",
       "122            10  \n",
       "123            11  \n",
       "124             3  \n",
       "\n",
       "[125 rows x 11 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Law_enforcement=pd.read_excel(r'.\\data\\table_80_full_time_law_enforcement_employees_georgia_by_metropolitan_and_nonmetropolitan_counties_2012.xls')\n",
    "crime_rates=pd.read_excel(r'.\\data\\georgia crime rates.xlsx',header=1)\n",
    "crime_rates=crime_rates.groupby(by='County').sum().iloc[11:,:]\n",
    "crime_len=pd.merge(Law_enforcement,crime_rates,left_on='County',right_on='County')\n",
    "crime_len=crime_len.drop(columns=crime_len.columns[crime_len.isnull().any()])\n",
    "crime_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "nominated-cameroon",
   "metadata": {},
   "outputs": [],
   "source": [
    "crime_len=crime_len.set_index(keys='County')\n",
    "crime_len['PUMA']=None\n",
    "crime_len.loc['Barrow','PUMA']=3800\n",
    "crime_len.loc['Bartow','PUMA']=2900\n",
    "crime_len.loc['Bibb','PUMA']=1700\n",
    "crime_len.loc['Bryan','PUMA']=200\n",
    "crime_len.loc['Brooks','PUMA']=700\n",
    "crime_len.loc['Butts','PUMA']=1900\n",
    "crime_len.loc['Brantley','PUMA']=500\n",
    "crime_len.loc['Carroll','PUMA']=2300\n",
    "crime_len.loc['Catoosa','PUMA']=2600\n",
    "crime_len.loc['Chatham','PUMA']=401\n",
    "crime_len.loc['Chattahoochee','PUMA']=1700\n",
    "crime_len.loc['Cherokee','PUMA']=3101\n",
    "crime_len.loc['Clarke','PUMA']=3600\n",
    "crime_len.loc['Clayton','PUMA']=5001\n",
    "crime_len.loc['Cobb','PUMA']=3001\n",
    "crime_len.loc['Columbia','PUMA']=4100\n",
    "crime_len.loc['Coweta','PUMA']=2100\n",
    "crime_len.loc['Dade','PUMA']=2600\n",
    "crime_len.loc['Dawson','PUMA']=3200\n",
    "crime_len.loc['Dougherty','PUMA']=900\n",
    "crime_len.loc['Effingham','PUMA']=300\n",
    "crime_len.loc['Floyd','PUMA']=2500\n",
    "crime_len.loc['Forsyth','PUMA']=3300\n",
    "crime_len.loc['Fulton','PUMA']=4600\n",
    "crime_len.loc['Glynn','PUMA']=100\n",
    "crime_len.loc['Hall','PUMA']=3400\n",
    "crime_len.loc['Haralson','PUMA']=2500\n",
    "crime_len.loc['Harris','PUMA']=1800\n",
    "crime_len.loc['Heard','PUMA']=2200\n",
    "crime_len.loc['Henry','PUMA']=600\n",
    "crime_len.loc['Houston','PUMA']=1500\n",
    "crime_len.loc['Jones','PUMA']=1600\n",
    "crime_len.loc['Lamar','PUMA']=1900\n",
    "crime_len.loc['Lanier','PUMA']=500\n",
    "crime_len.loc['Long','PUMA']=200\n",
    "crime_len.loc['Lowndes','PUMA']=600\n",
    "crime_len.loc['Madison','PUMA']=3700\n",
    "crime_len.loc['McDuffie','PUMA']=4200\n",
    "crime_len.loc['McIntosh','PUMA']=100\n",
    "crime_len.loc['Morgan','PUMA']=3900\n",
    "crime_len.loc['Monroe','PUMA']=1600\n",
    "crime_len.loc['Oconee','PUMA']=3700\n",
    "crime_len.loc['Oglethorpe','PUMA']=3700\n",
    "crime_len.loc['Pickens','PUMA']=2800\n",
    "crime_len.loc['Spalding','PUMA']=1900\n",
    "crime_len.loc['Twiggs','PUMA']=1600\n",
    "crime_len.loc['Appling','PUMA']=1200\n",
    "crime_len.loc['Baldwin','PUMA']=1600\n",
    "crime_len.loc['Ben Hill','PUMA']=700\n",
    "crime_len.loc['Berrien','PUMA']=700\n",
    "crime_len.loc['Newton','PUMA']=4300\n",
    "crime_len.loc['Paulding','PUMA']=4500\n",
    "crime_len.loc['Pulaski','PUMA']=1500\n",
    "crime_len.loc['Rockdale','PUMA']=4300\n",
    "crime_len.loc['Walker','PUMA']=2600\n",
    "crime_len.loc['Walton','PUMA']=3900\n",
    "crime_len.loc['Whitfield','PUMA']=2700\n",
    "crime_len.loc['Worth','PUMA']=800\n",
    "crime_len.loc['Bulloch','PUMA']=300\n",
    "crime_len.loc['Camden','PUMA']=100\n",
    "crime_len.loc['Chattooga','PUMA']=2600\n",
    "crime_len.loc['Decatur','PUMA']=2002\n",
    "crime_len.loc['Dodge','PUMA']=1300\n",
    "crime_len.loc['Dooly','PUMA']=2600\n",
    "crime_len.loc['Early','PUMA']=1100\n",
    "crime_len.loc['Elbert','PUMA']=3700\n",
    "crime_len.loc['Emanuel','PUMA']=1300\n",
    "crime_len.loc['Fannin','PUMA']=2800\n",
    "crime_len.loc['Franklin','PUMA']=3500\n",
    "crime_len.loc['Gilmer','PUMA']=2800\n",
    "crime_len.loc['Gordon','PUMA']=2800\n",
    "crime_len.loc['Grady','PUMA']=1100\n",
    "crime_len.loc['Greene','PUMA']=3300\n",
    "crime_len.loc['Habersham','PUMA']=3500\n",
    "crime_len.loc['Hancock','PUMA']=4200\n",
    "crime_len.loc['Hart','PUMA']=3500\n",
    "crime_len.loc['Irwin','PUMA']=1600\n",
    "crime_len.loc['Jackson','PUMA']=1900\n",
    "crime_len.loc['Jefferson','PUMA']=4200\n",
    "crime_len.loc['Bleckley','PUMA']=1300\n",
    "crime_len.loc['Charlton','PUMA']=500\n",
    "crime_len.loc['Clinch','PUMA']=500\n",
    "crime_len.loc['Coffee','PUMA']=500\n",
    "crime_len.loc['Cook','PUMA']=700\n",
    "crime_len.loc['Crisp','PUMA']=1800\n",
    "crime_len.loc['Johnson','PUMA']=1300\n",
    "crime_len.loc['Lumpkin','PUMA']=1800\n",
    "crime_len.loc['Miller','PUMA']=1100\n",
    "crime_len.loc['Mitchell','PUMA']=1100\n",
    "crime_len.loc['Miller','PUMA']=1100\n",
    "crime_len.loc['Pierce','PUMA']=500\n",
    "crime_len.loc['Polk','PUMA']=2500\n",
    "crime_len.loc['Rabun','PUMA']=3200\n",
    "crime_len.loc['Schley','PUMA']=1800\n",
    "crime_len.loc['Seminole','PUMA']=1100\n",
    "crime_len.loc['Stephens','PUMA']=3500\n",
    "crime_len.loc['Stewart','PUMA']=200\n",
    "crime_len.loc['Sumter','PUMA']=1800\n",
    "crime_len.loc['Talbot','PUMA']=1800\n",
    "crime_len.loc['Taliaferro','PUMA']=4200\n",
    "crime_len.loc['Taylor','PUMA']=1800\n",
    "crime_len.loc['Tift','PUMA']=700\n",
    "crime_len.loc['Towns','PUMA']=3200\n",
    "crime_len.loc['Treutlen','PUMA']=1300\n",
    "crime_len.loc['Upson','PUMA']=1900\n",
    "crime_len.loc['Macon','PUMA']=1400\n",
    "crime_len.loc['Polk','PUMA']=2500\n",
    "crime_len.loc['Screven','PUMA']=300\n",
    "crime_len.loc['Thomas','PUMA']=800\n",
    "crime_len.loc['Troup','PUMA']=2200\n",
    "crime_len.loc['Union','PUMA']=1004\n",
    "crime_len.loc['Ware','PUMA']=500\n",
    "crime_len.loc['Warren','PUMA']=4200\n",
    "crime_len.loc['Washington','PUMA']=4200\n",
    "crime_len.loc['Wayne','PUMA']=1200\n",
    "crime_len.loc['Webster','PUMA']=1800\n",
    "crime_len.loc['White','PUMA']=3200\n",
    "crime_len.loc['Wilcox','PUMA']=1300\n",
    "crime_len.loc['Wilkes','PUMA']=4200\n",
    "crime_len.loc['Wilkinson','PUMA']=1600\n",
    "crime_len.drop(index=crime_len[crime_len.PUMA.isnull()].index,inplace=True)\n",
    "crime_len['Code']=(crime_len['PUMA']).astype(str).map(PUMA_code_map)\n",
    "crime_len=crime_len.astype(int)\n",
    "crime_len=crime_len.groupby('Code').mean()\n",
    "crime_len.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "desperate-listing",
   "metadata": {},
   "outputs": [],
   "source": [
    "PUMA_group=data_PUMA.groupby(by='Code',as_index=False).mean()\n",
    "PUMA_group=PUMA_group.iloc[:,:-81].drop(columns=['SERIALNO', 'DIVISION', 'REGION', 'ST', 'ADJHSG', 'ADJINC',\n",
    "       'WGTP'])\n",
    "PUMA_group['TEN']=data_PUMA[['Code','TEN']].groupby(by='Code',as_index=False).agg(lambda x:x.value_counts().index[0])['TEN']\n",
    "PUMA_new=pd.merge(PUMA_group,crime_len,left_on='Code',right_on='Code',how='outer')\n",
    "PUMA_new=PUMA_new.fillna(PUMA_new.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "absolute-degree",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_extend=pd.merge(raw_train,PUMA_new,left_on='Residence_PUMA',right_on='Code',how='left')\n",
    "\n",
    "# Change the dtype of the feature from object to intagar\n",
    "raw_extend['TEN']=raw_extend['TEN'].astype('category')\n",
    "raw_extend['TEN']=raw_extend['TEN'].cat.codes\n",
    "raw_extend['Residence_PUMA']=raw_extend['Residence_PUMA'].astype('category')\n",
    "raw_extend['Residence_PUMA']=raw_extend['Residence_PUMA'].cat.codes\n",
    "raw_extend['Age_at_Release']=raw_extend['Age_at_Release'].apply(lambda x: int(x[:2]))\n",
    "raw_extend['Dependents']=raw_extend['Dependents'].apply(lambda x: int(x[:1]))\n",
    "raw_extend['Prior_Arrest_Episodes_Felony']=raw_extend['Prior_Arrest_Episodes_Felony'].apply(lambda x: int(x[:2]))\n",
    "raw_extend['Prior_Arrest_Episodes_Drug']=raw_extend['Prior_Arrest_Episodes_Drug'].apply(lambda x: int(x[:2]))\n",
    "raw_extend['Prior_Arrest_Episodes_Misd']=raw_extend['Prior_Arrest_Episodes_Misd'].apply(lambda x: int(x[:2]))\n",
    "raw_extend['Prior_Arrest_Episodes_Violent']=raw_extend['Prior_Arrest_Episodes_Violent'].apply(lambda x: int(x[:2]))\n",
    "raw_extend['Prior_Arrest_Episodes_Property']=raw_extend['Prior_Arrest_Episodes_Property'].apply(lambda x: int(x[:2]))\n",
    "raw_extend['Prior_Arrest_Episodes_PPViolationCharges']=raw_extend['Prior_Arrest_Episodes_PPViolationCharges'].apply(lambda x: int(x[:2]))\n",
    "raw_extend['Prior_Conviction_Episodes_Felony']=raw_extend['Prior_Conviction_Episodes_Felony'].apply(lambda x: int(x[:1]))\n",
    "raw_extend['Prior_Conviction_Episodes_Misd']=raw_extend['Prior_Conviction_Episodes_Misd'].apply(lambda x: int(x[:1]))\n",
    "raw_extend['Prior_Conviction_Episodes_Prop']=raw_extend['Prior_Conviction_Episodes_Prop'].apply(lambda x: int(x[:1]))\n",
    "raw_extend['Prior_Conviction_Episodes_Drug']=raw_extend['Prior_Conviction_Episodes_Drug'].apply(lambda x: int(x[:1]))\n",
    "#raw_extend['Delinquency_Reports']=raw_extend['Delinquency_Reports'].apply(lambda x: int(x[:1]))\n",
    "#raw_extend['Program_Attendances']=raw_extend['Program_Attendances'].apply(lambda x: int(x[:2]))\n",
    "#raw_extend['Program_UnexcusedAbsences']=raw_extend['Program_UnexcusedAbsences'].apply(lambda x: int(x[:1]))\n",
    "#raw_extend['Residence_Changes']=raw_extend['Residence_Changes'].apply(lambda x: int(x[:1]))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaling_set=[]\n",
    "for column in raw_extend.columns:\n",
    "    if raw_extend[column].dtype == object:\n",
    "        raw_extend[column]=raw_extend[column].astype('category')\n",
    "        raw_extend[column]=raw_extend[column].cat.codes\n",
    "    elif raw_extend[column].dtype in ['int64','float32','float64'] :\n",
    "        scaling_set+=[column]\n",
    "raw_extend[scaling_set]=scaler.fit_transform(raw_extend[scaling_set].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "intermediate-ability",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression train score: 0.8114358683314415 \n",
      " test score: 0.7991489361702128 \n",
      " Logistic regression train Brier score: 0.1416992281466853 \n",
      " test Brier score: 0.15199506250450368 \n",
      " AUROC: 0.6641206428248075\n",
      "Random forest train score: 0.9998581157775256 \n",
      " test score: 0.7991489361702128 \n",
      " Random forest  train Brier score: 0.020619882709042796 \n",
      " test Brier score: 0.15464026477541468 \n",
      " AUROC: 0.6387007695789949\n",
      "SGD best layer size: {'min_samples_split': 4, 'n_estimators': 100} \n",
      " best train score: -0.14580163748480732 \n",
      " test score: -0.1492601799757902 \n",
      " SGD  train Brier score: 0.1232099557807449 \n",
      " test Brier score: 0.1492601799757901 \n",
      " AUROC: 0.6873562698053418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\myjr\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "155/155 [==============================] - 1s 2ms/step - loss: 0.7212 - accuracy: 0.6424 - val_loss: 0.4956 - val_accuracy: 0.8142\n",
      "Epoch 2/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.5770 - accuracy: 0.7592 - val_loss: 0.4837 - val_accuracy: 0.8142\n",
      "Epoch 3/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.5376 - accuracy: 0.7871 - val_loss: 0.4809 - val_accuracy: 0.8142\n",
      "Epoch 4/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.5163 - accuracy: 0.7975 - val_loss: 0.4769 - val_accuracy: 0.8142\n",
      "Epoch 5/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.5076 - accuracy: 0.8060 - val_loss: 0.4747 - val_accuracy: 0.8142\n",
      "Epoch 6/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4980 - accuracy: 0.8052 - val_loss: 0.4745 - val_accuracy: 0.8142\n",
      "Epoch 7/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4888 - accuracy: 0.8101 - val_loss: 0.4745 - val_accuracy: 0.8142\n",
      "Epoch 8/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4890 - accuracy: 0.8107 - val_loss: 0.4743 - val_accuracy: 0.8142\n",
      "Epoch 9/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4894 - accuracy: 0.8109 - val_loss: 0.4730 - val_accuracy: 0.8142\n",
      "Epoch 10/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4804 - accuracy: 0.8107 - val_loss: 0.4720 - val_accuracy: 0.8142\n",
      "Epoch 11/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4800 - accuracy: 0.8109 - val_loss: 0.4720 - val_accuracy: 0.8142\n",
      "Epoch 12/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4802 - accuracy: 0.8117 - val_loss: 0.4707 - val_accuracy: 0.8142\n",
      "Epoch 13/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4757 - accuracy: 0.8123 - val_loss: 0.4712 - val_accuracy: 0.8142\n",
      "Epoch 14/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4734 - accuracy: 0.8115 - val_loss: 0.4699 - val_accuracy: 0.8142\n",
      "Epoch 15/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4745 - accuracy: 0.8117 - val_loss: 0.4695 - val_accuracy: 0.8142\n",
      "Epoch 16/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4763 - accuracy: 0.8117 - val_loss: 0.4692 - val_accuracy: 0.8142\n",
      "Epoch 17/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4793 - accuracy: 0.8117 - val_loss: 0.4691 - val_accuracy: 0.8142\n",
      "Epoch 18/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4754 - accuracy: 0.8115 - val_loss: 0.4687 - val_accuracy: 0.8142\n",
      "Epoch 19/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4737 - accuracy: 0.8121 - val_loss: 0.4680 - val_accuracy: 0.8142\n",
      "Epoch 20/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4697 - accuracy: 0.8121 - val_loss: 0.4676 - val_accuracy: 0.8142\n",
      "Epoch 21/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4738 - accuracy: 0.8119 - val_loss: 0.4670 - val_accuracy: 0.8142\n",
      "Epoch 22/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4697 - accuracy: 0.8117 - val_loss: 0.4657 - val_accuracy: 0.8142\n",
      "Epoch 23/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4729 - accuracy: 0.8113 - val_loss: 0.4659 - val_accuracy: 0.8142\n",
      "Epoch 24/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4683 - accuracy: 0.8119 - val_loss: 0.4650 - val_accuracy: 0.8142\n",
      "Epoch 25/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4679 - accuracy: 0.8121 - val_loss: 0.4642 - val_accuracy: 0.8142\n",
      "Epoch 26/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4678 - accuracy: 0.8113 - val_loss: 0.4641 - val_accuracy: 0.8142\n",
      "Epoch 27/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4627 - accuracy: 0.8125 - val_loss: 0.4635 - val_accuracy: 0.8142\n",
      "Epoch 28/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4669 - accuracy: 0.8119 - val_loss: 0.4632 - val_accuracy: 0.8142\n",
      "Epoch 29/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4670 - accuracy: 0.8115 - val_loss: 0.4631 - val_accuracy: 0.8142\n",
      "Epoch 30/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4656 - accuracy: 0.8113 - val_loss: 0.4626 - val_accuracy: 0.8142\n",
      "Epoch 31/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4642 - accuracy: 0.8115 - val_loss: 0.4622 - val_accuracy: 0.8142\n",
      "Epoch 32/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4672 - accuracy: 0.8121 - val_loss: 0.4625 - val_accuracy: 0.8142\n",
      "Epoch 33/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4640 - accuracy: 0.8119 - val_loss: 0.4620 - val_accuracy: 0.8142\n",
      "Epoch 34/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4651 - accuracy: 0.8117 - val_loss: 0.4625 - val_accuracy: 0.8142\n",
      "Epoch 35/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4635 - accuracy: 0.8115 - val_loss: 0.4621 - val_accuracy: 0.8142\n",
      "Epoch 36/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4655 - accuracy: 0.8105 - val_loss: 0.4619 - val_accuracy: 0.8142\n",
      "Epoch 37/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4651 - accuracy: 0.8121 - val_loss: 0.4617 - val_accuracy: 0.8142\n",
      "Epoch 38/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4630 - accuracy: 0.8117 - val_loss: 0.4615 - val_accuracy: 0.8142\n",
      "Epoch 39/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4644 - accuracy: 0.8109 - val_loss: 0.4609 - val_accuracy: 0.8142\n",
      "Epoch 40/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4591 - accuracy: 0.8117 - val_loss: 0.4612 - val_accuracy: 0.8142\n",
      "Epoch 41/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4630 - accuracy: 0.8113 - val_loss: 0.4607 - val_accuracy: 0.8142\n",
      "Epoch 42/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4630 - accuracy: 0.8113 - val_loss: 0.4601 - val_accuracy: 0.8142\n",
      "Epoch 43/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4645 - accuracy: 0.8119 - val_loss: 0.4599 - val_accuracy: 0.8142\n",
      "Epoch 44/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4637 - accuracy: 0.8105 - val_loss: 0.4597 - val_accuracy: 0.8142\n",
      "Epoch 45/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4633 - accuracy: 0.8121 - val_loss: 0.4598 - val_accuracy: 0.8142\n",
      "Epoch 46/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4636 - accuracy: 0.8127 - val_loss: 0.4595 - val_accuracy: 0.8142\n",
      "Epoch 47/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4567 - accuracy: 0.8113 - val_loss: 0.4600 - val_accuracy: 0.8142\n",
      "Epoch 48/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4603 - accuracy: 0.8115 - val_loss: 0.4595 - val_accuracy: 0.8142\n",
      "Epoch 49/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4618 - accuracy: 0.8111 - val_loss: 0.4592 - val_accuracy: 0.8142\n",
      "Epoch 50/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4593 - accuracy: 0.8115 - val_loss: 0.4598 - val_accuracy: 0.8142\n",
      "Epoch 51/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4617 - accuracy: 0.8105 - val_loss: 0.4593 - val_accuracy: 0.8142\n",
      "Epoch 52/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4613 - accuracy: 0.8123 - val_loss: 0.4593 - val_accuracy: 0.8142\n",
      "Epoch 53/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4619 - accuracy: 0.8121 - val_loss: 0.4592 - val_accuracy: 0.8142\n",
      "Epoch 54/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4585 - accuracy: 0.8119 - val_loss: 0.4590 - val_accuracy: 0.8142\n",
      "Epoch 55/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4587 - accuracy: 0.8121 - val_loss: 0.4587 - val_accuracy: 0.8142\n",
      "Epoch 56/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4587 - accuracy: 0.8127 - val_loss: 0.4592 - val_accuracy: 0.8142\n",
      "Epoch 57/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4571 - accuracy: 0.8119 - val_loss: 0.4589 - val_accuracy: 0.8142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4585 - accuracy: 0.8123 - val_loss: 0.4591 - val_accuracy: 0.8142\n",
      "Epoch 59/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4577 - accuracy: 0.8119 - val_loss: 0.4589 - val_accuracy: 0.8142\n",
      "Epoch 60/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4596 - accuracy: 0.8113 - val_loss: 0.4589 - val_accuracy: 0.8142\n",
      "Epoch 61/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4560 - accuracy: 0.8117 - val_loss: 0.4586 - val_accuracy: 0.8142\n",
      "Epoch 62/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4567 - accuracy: 0.8123 - val_loss: 0.4587 - val_accuracy: 0.8142\n",
      "Epoch 63/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4598 - accuracy: 0.8111 - val_loss: 0.4584 - val_accuracy: 0.8142\n",
      "Epoch 64/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4602 - accuracy: 0.8115 - val_loss: 0.4590 - val_accuracy: 0.8142\n",
      "Epoch 65/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4577 - accuracy: 0.8115 - val_loss: 0.4590 - val_accuracy: 0.8142\n",
      "Epoch 66/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4554 - accuracy: 0.8129 - val_loss: 0.4585 - val_accuracy: 0.8142\n",
      "Epoch 67/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4608 - accuracy: 0.8119 - val_loss: 0.4580 - val_accuracy: 0.8142\n",
      "Epoch 68/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4577 - accuracy: 0.8119 - val_loss: 0.4578 - val_accuracy: 0.8142\n",
      "Epoch 69/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4581 - accuracy: 0.8105 - val_loss: 0.4575 - val_accuracy: 0.8142\n",
      "Epoch 70/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4606 - accuracy: 0.8125 - val_loss: 0.4579 - val_accuracy: 0.8142\n",
      "Epoch 71/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4573 - accuracy: 0.8123 - val_loss: 0.4575 - val_accuracy: 0.8142\n",
      "Epoch 72/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4549 - accuracy: 0.8109 - val_loss: 0.4572 - val_accuracy: 0.8142\n",
      "Epoch 73/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4564 - accuracy: 0.8115 - val_loss: 0.4571 - val_accuracy: 0.8142\n",
      "Epoch 74/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4533 - accuracy: 0.8127 - val_loss: 0.4571 - val_accuracy: 0.8142\n",
      "Epoch 75/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4576 - accuracy: 0.8121 - val_loss: 0.4571 - val_accuracy: 0.8142\n",
      "Epoch 76/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4582 - accuracy: 0.8117 - val_loss: 0.4568 - val_accuracy: 0.8142\n",
      "Epoch 77/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4563 - accuracy: 0.8115 - val_loss: 0.4574 - val_accuracy: 0.8142\n",
      "Epoch 78/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4553 - accuracy: 0.8129 - val_loss: 0.4577 - val_accuracy: 0.8142\n",
      "Epoch 79/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4547 - accuracy: 0.8123 - val_loss: 0.4578 - val_accuracy: 0.8142\n",
      "Epoch 80/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4577 - accuracy: 0.8115 - val_loss: 0.4584 - val_accuracy: 0.8142\n",
      "Epoch 81/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4559 - accuracy: 0.8115 - val_loss: 0.4576 - val_accuracy: 0.8137\n",
      "Epoch 82/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4535 - accuracy: 0.8117 - val_loss: 0.4571 - val_accuracy: 0.8142\n",
      "Epoch 83/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4535 - accuracy: 0.8113 - val_loss: 0.4571 - val_accuracy: 0.8137\n",
      "Epoch 84/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4566 - accuracy: 0.8111 - val_loss: 0.4562 - val_accuracy: 0.8142\n",
      "Epoch 85/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4529 - accuracy: 0.8115 - val_loss: 0.4565 - val_accuracy: 0.8142\n",
      "Epoch 86/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4536 - accuracy: 0.8117 - val_loss: 0.4566 - val_accuracy: 0.8142\n",
      "Epoch 87/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4540 - accuracy: 0.8131 - val_loss: 0.4567 - val_accuracy: 0.8142\n",
      "Epoch 88/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4523 - accuracy: 0.8111 - val_loss: 0.4570 - val_accuracy: 0.8142\n",
      "Epoch 89/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4579 - accuracy: 0.8119 - val_loss: 0.4567 - val_accuracy: 0.8142\n",
      "Epoch 90/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4575 - accuracy: 0.8113 - val_loss: 0.4560 - val_accuracy: 0.8142\n",
      "Epoch 91/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4552 - accuracy: 0.8103 - val_loss: 0.4560 - val_accuracy: 0.8142\n",
      "Epoch 92/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4554 - accuracy: 0.8109 - val_loss: 0.4564 - val_accuracy: 0.8142\n",
      "Epoch 93/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4556 - accuracy: 0.8105 - val_loss: 0.4560 - val_accuracy: 0.8142\n",
      "Epoch 94/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4543 - accuracy: 0.8121 - val_loss: 0.4559 - val_accuracy: 0.8142\n",
      "Epoch 95/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4539 - accuracy: 0.8127 - val_loss: 0.4560 - val_accuracy: 0.8142\n",
      "Epoch 96/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4541 - accuracy: 0.8115 - val_loss: 0.4569 - val_accuracy: 0.8147\n",
      "Epoch 97/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4547 - accuracy: 0.8117 - val_loss: 0.4569 - val_accuracy: 0.8142\n",
      "Epoch 98/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4549 - accuracy: 0.8113 - val_loss: 0.4561 - val_accuracy: 0.8142\n",
      "Epoch 99/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4573 - accuracy: 0.8125 - val_loss: 0.4563 - val_accuracy: 0.8142\n",
      "Epoch 100/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4542 - accuracy: 0.8117 - val_loss: 0.4570 - val_accuracy: 0.8137\n",
      "Epoch 101/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4561 - accuracy: 0.8107 - val_loss: 0.4570 - val_accuracy: 0.8142\n",
      "Epoch 102/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4554 - accuracy: 0.8117 - val_loss: 0.4569 - val_accuracy: 0.8142\n",
      "Epoch 103/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4531 - accuracy: 0.8113 - val_loss: 0.4573 - val_accuracy: 0.8137\n",
      "Epoch 104/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4534 - accuracy: 0.8121 - val_loss: 0.4574 - val_accuracy: 0.8132\n",
      "Epoch 105/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4562 - accuracy: 0.8111 - val_loss: 0.4571 - val_accuracy: 0.8142\n",
      "Epoch 106/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4548 - accuracy: 0.8103 - val_loss: 0.4562 - val_accuracy: 0.8142\n",
      "Epoch 107/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4547 - accuracy: 0.8111 - val_loss: 0.4565 - val_accuracy: 0.8142\n",
      "Epoch 108/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4537 - accuracy: 0.8119 - val_loss: 0.4566 - val_accuracy: 0.8142\n",
      "Epoch 109/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4528 - accuracy: 0.8125 - val_loss: 0.4571 - val_accuracy: 0.8142\n",
      "Epoch 110/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4532 - accuracy: 0.8137 - val_loss: 0.4568 - val_accuracy: 0.8142\n",
      "Epoch 111/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4523 - accuracy: 0.8113 - val_loss: 0.4569 - val_accuracy: 0.8147\n",
      "Epoch 112/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4557 - accuracy: 0.8121 - val_loss: 0.4568 - val_accuracy: 0.8142\n",
      "Epoch 113/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4557 - accuracy: 0.8123 - val_loss: 0.4560 - val_accuracy: 0.8142\n",
      "Epoch 114/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4555 - accuracy: 0.8133 - val_loss: 0.4561 - val_accuracy: 0.8142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4505 - accuracy: 0.8133 - val_loss: 0.4564 - val_accuracy: 0.8142\n",
      "Epoch 116/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4548 - accuracy: 0.8117 - val_loss: 0.4572 - val_accuracy: 0.8113\n",
      "Epoch 117/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4544 - accuracy: 0.8088 - val_loss: 0.4567 - val_accuracy: 0.8132\n",
      "Epoch 118/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4494 - accuracy: 0.8111 - val_loss: 0.4573 - val_accuracy: 0.8142\n",
      "Epoch 119/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4512 - accuracy: 0.8119 - val_loss: 0.4570 - val_accuracy: 0.8137\n",
      "Epoch 120/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4517 - accuracy: 0.8111 - val_loss: 0.4561 - val_accuracy: 0.8147\n",
      "Epoch 121/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4565 - accuracy: 0.8119 - val_loss: 0.4559 - val_accuracy: 0.8142\n",
      "Epoch 122/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4505 - accuracy: 0.8119 - val_loss: 0.4563 - val_accuracy: 0.8132\n",
      "Epoch 123/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4486 - accuracy: 0.8125 - val_loss: 0.4562 - val_accuracy: 0.8137\n",
      "Epoch 124/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4565 - accuracy: 0.8119 - val_loss: 0.4556 - val_accuracy: 0.8142\n",
      "Epoch 125/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4524 - accuracy: 0.8115 - val_loss: 0.4560 - val_accuracy: 0.8142\n",
      "Epoch 126/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4529 - accuracy: 0.8115 - val_loss: 0.4555 - val_accuracy: 0.8142\n",
      "Epoch 127/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4526 - accuracy: 0.8123 - val_loss: 0.4558 - val_accuracy: 0.8142\n",
      "Epoch 128/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4570 - accuracy: 0.8123 - val_loss: 0.4555 - val_accuracy: 0.8142\n",
      "Epoch 129/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4502 - accuracy: 0.8113 - val_loss: 0.4564 - val_accuracy: 0.8142\n",
      "Epoch 130/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4499 - accuracy: 0.8137 - val_loss: 0.4561 - val_accuracy: 0.8142\n",
      "Epoch 131/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4541 - accuracy: 0.8117 - val_loss: 0.4558 - val_accuracy: 0.8142\n",
      "Epoch 132/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4529 - accuracy: 0.8117 - val_loss: 0.4558 - val_accuracy: 0.8142\n",
      "Epoch 133/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4542 - accuracy: 0.8125 - val_loss: 0.4559 - val_accuracy: 0.8142\n",
      "Epoch 134/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4533 - accuracy: 0.8125 - val_loss: 0.4556 - val_accuracy: 0.8142\n",
      "Epoch 135/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4507 - accuracy: 0.8103 - val_loss: 0.4563 - val_accuracy: 0.8128\n",
      "Epoch 136/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4507 - accuracy: 0.8113 - val_loss: 0.4558 - val_accuracy: 0.8137\n",
      "Epoch 137/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4514 - accuracy: 0.8119 - val_loss: 0.4558 - val_accuracy: 0.8137\n",
      "Epoch 138/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4524 - accuracy: 0.8115 - val_loss: 0.4556 - val_accuracy: 0.8137\n",
      "Epoch 139/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4525 - accuracy: 0.8117 - val_loss: 0.4554 - val_accuracy: 0.8128\n",
      "Epoch 140/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4506 - accuracy: 0.8115 - val_loss: 0.4551 - val_accuracy: 0.8142\n",
      "Epoch 141/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4527 - accuracy: 0.8115 - val_loss: 0.4548 - val_accuracy: 0.8142\n",
      "Epoch 142/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4483 - accuracy: 0.8129 - val_loss: 0.4555 - val_accuracy: 0.8132\n",
      "Epoch 143/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4505 - accuracy: 0.8115 - val_loss: 0.4560 - val_accuracy: 0.8137\n",
      "Epoch 144/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4510 - accuracy: 0.8117 - val_loss: 0.4556 - val_accuracy: 0.8142\n",
      "Epoch 145/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4497 - accuracy: 0.8119 - val_loss: 0.4558 - val_accuracy: 0.8142\n",
      "Epoch 146/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4518 - accuracy: 0.8127 - val_loss: 0.4560 - val_accuracy: 0.8142\n",
      "Epoch 147/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4520 - accuracy: 0.8103 - val_loss: 0.4559 - val_accuracy: 0.8142\n",
      "Epoch 148/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4515 - accuracy: 0.8115 - val_loss: 0.4560 - val_accuracy: 0.8142\n",
      "Epoch 149/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4500 - accuracy: 0.8127 - val_loss: 0.4560 - val_accuracy: 0.8132\n",
      "Epoch 150/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4520 - accuracy: 0.8123 - val_loss: 0.4562 - val_accuracy: 0.8142\n",
      "Epoch 151/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4440 - accuracy: 0.8121 - val_loss: 0.4559 - val_accuracy: 0.8142\n",
      "Epoch 152/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4492 - accuracy: 0.8129 - val_loss: 0.4559 - val_accuracy: 0.8142\n",
      "Epoch 153/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4499 - accuracy: 0.8113 - val_loss: 0.4559 - val_accuracy: 0.8147\n",
      "Epoch 154/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4512 - accuracy: 0.8121 - val_loss: 0.4560 - val_accuracy: 0.8147\n",
      "Epoch 155/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4504 - accuracy: 0.8121 - val_loss: 0.4566 - val_accuracy: 0.8151\n",
      "Epoch 156/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4492 - accuracy: 0.8117 - val_loss: 0.4553 - val_accuracy: 0.8142\n",
      "Epoch 157/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4511 - accuracy: 0.8111 - val_loss: 0.4557 - val_accuracy: 0.8137\n",
      "Epoch 158/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4504 - accuracy: 0.8105 - val_loss: 0.4558 - val_accuracy: 0.8137\n",
      "Epoch 159/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4504 - accuracy: 0.8127 - val_loss: 0.4558 - val_accuracy: 0.8142\n",
      "Epoch 160/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4491 - accuracy: 0.8131 - val_loss: 0.4550 - val_accuracy: 0.8137\n",
      "Epoch 161/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4539 - accuracy: 0.8125 - val_loss: 0.4556 - val_accuracy: 0.8142\n",
      "Epoch 162/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4497 - accuracy: 0.8127 - val_loss: 0.4555 - val_accuracy: 0.8137\n",
      "Epoch 163/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4489 - accuracy: 0.8127 - val_loss: 0.4553 - val_accuracy: 0.8142\n",
      "Epoch 164/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4502 - accuracy: 0.8125 - val_loss: 0.4557 - val_accuracy: 0.8142\n",
      "Epoch 165/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4524 - accuracy: 0.8115 - val_loss: 0.4553 - val_accuracy: 0.8142\n",
      "Epoch 166/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4491 - accuracy: 0.8107 - val_loss: 0.4561 - val_accuracy: 0.8137\n",
      "Epoch 167/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4498 - accuracy: 0.8123 - val_loss: 0.4561 - val_accuracy: 0.8137\n",
      "Epoch 168/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4513 - accuracy: 0.8125 - val_loss: 0.4557 - val_accuracy: 0.8142\n",
      "Epoch 169/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4509 - accuracy: 0.8103 - val_loss: 0.4553 - val_accuracy: 0.8137\n",
      "Epoch 170/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4516 - accuracy: 0.8139 - val_loss: 0.4555 - val_accuracy: 0.8137\n",
      "Epoch 171/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4508 - accuracy: 0.8131 - val_loss: 0.4552 - val_accuracy: 0.8137\n",
      "Epoch 172/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4510 - accuracy: 0.8119 - val_loss: 0.4555 - val_accuracy: 0.8142\n",
      "Epoch 173/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4474 - accuracy: 0.8111 - val_loss: 0.4557 - val_accuracy: 0.8151\n",
      "Epoch 174/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4524 - accuracy: 0.8115 - val_loss: 0.4551 - val_accuracy: 0.8142\n",
      "Epoch 175/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4485 - accuracy: 0.8129 - val_loss: 0.4555 - val_accuracy: 0.8156\n",
      "Epoch 176/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4502 - accuracy: 0.8105 - val_loss: 0.4549 - val_accuracy: 0.8147\n",
      "Epoch 177/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4499 - accuracy: 0.8115 - val_loss: 0.4549 - val_accuracy: 0.8137\n",
      "Epoch 178/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4503 - accuracy: 0.8137 - val_loss: 0.4550 - val_accuracy: 0.8137\n",
      "Epoch 179/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4505 - accuracy: 0.8101 - val_loss: 0.4553 - val_accuracy: 0.8137\n",
      "Epoch 180/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4520 - accuracy: 0.8107 - val_loss: 0.4547 - val_accuracy: 0.8137\n",
      "Epoch 181/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4487 - accuracy: 0.8119 - val_loss: 0.4552 - val_accuracy: 0.8147\n",
      "Epoch 182/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4495 - accuracy: 0.8131 - val_loss: 0.4562 - val_accuracy: 0.8113\n",
      "Epoch 183/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4488 - accuracy: 0.8107 - val_loss: 0.4551 - val_accuracy: 0.8147\n",
      "Epoch 184/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4500 - accuracy: 0.8113 - val_loss: 0.4549 - val_accuracy: 0.8137\n",
      "Epoch 185/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4458 - accuracy: 0.8117 - val_loss: 0.4547 - val_accuracy: 0.8142\n",
      "Epoch 186/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4487 - accuracy: 0.8115 - val_loss: 0.4550 - val_accuracy: 0.8132\n",
      "Epoch 187/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4487 - accuracy: 0.8107 - val_loss: 0.4549 - val_accuracy: 0.8142\n",
      "Epoch 188/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4471 - accuracy: 0.8117 - val_loss: 0.4556 - val_accuracy: 0.8147\n",
      "Epoch 189/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4522 - accuracy: 0.8113 - val_loss: 0.4554 - val_accuracy: 0.8142\n",
      "Epoch 190/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4504 - accuracy: 0.8115 - val_loss: 0.4553 - val_accuracy: 0.8142\n",
      "Epoch 191/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4466 - accuracy: 0.8125 - val_loss: 0.4551 - val_accuracy: 0.8142\n",
      "Epoch 192/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4500 - accuracy: 0.8109 - val_loss: 0.4552 - val_accuracy: 0.8137\n",
      "Epoch 193/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4471 - accuracy: 0.8145 - val_loss: 0.4558 - val_accuracy: 0.8128\n",
      "Epoch 194/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4526 - accuracy: 0.8135 - val_loss: 0.4562 - val_accuracy: 0.8118\n",
      "Epoch 195/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4493 - accuracy: 0.8111 - val_loss: 0.4556 - val_accuracy: 0.8132\n",
      "Epoch 196/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4485 - accuracy: 0.8111 - val_loss: 0.4559 - val_accuracy: 0.8128\n",
      "Epoch 197/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4484 - accuracy: 0.8107 - val_loss: 0.4559 - val_accuracy: 0.8147\n",
      "Epoch 198/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4473 - accuracy: 0.8121 - val_loss: 0.4556 - val_accuracy: 0.8142\n",
      "Epoch 199/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4495 - accuracy: 0.8113 - val_loss: 0.4569 - val_accuracy: 0.8142\n",
      "Epoch 200/200\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.4500 - accuracy: 0.8131 - val_loss: 0.4555 - val_accuracy: 0.8137\n",
      "221/221 [==============================] - 0s 925us/step - loss: 0.4567 - accuracy: 0.8130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\myjr\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:430: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
      "  warnings.warn('`model.predict_proba()` is deprecated and '\n",
      "C:\\Users\\myjr\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:430: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
      "  warnings.warn('`model.predict_proba()` is deprecated and '\n",
      "C:\\Users\\myjr\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:430: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
      "  warnings.warn('`model.predict_proba()` is deprecated and '\n",
      "C:\\Users\\myjr\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:430: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
      "  warnings.warn('`model.predict_proba()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP train Brier score: 0.14116996387262903 \n",
      " test Brier score: 0.15254556203429823 \n",
      " AUROC: 0.6557322317790856\n",
      "Xgboost best layer size: {'colsample_bytree': 0.8, 'max_depth': 2, 'min_child_weight': 2, 'n_estimators': 100, 'reg_lambda': 1000, 'subsample': 0.9} \n",
      " best train score: -0.14435204358302955 \n",
      " test score: -0.15137479648932078 \n",
      " Xgboost train Brier score: 0.13930614349398035 \n",
      " test Brier score: 0.15137479646251997 \n",
      " AUROC: 0.6688048890900861\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression,LogisticRegression\n",
    "\n",
    "\n",
    "X_train1,X_test1,y_train1,y_test1=train_test_split(raw_extend.drop(['ID','Recidivism_Within_3years','Recidivism_Arrest_Year1','Recidivism_Arrest_Year2','Recidivism_Arrest_Year3'],axis=1),raw_extend['Recidivism_Arrest_Year3'])\n",
    "fpr_list,tpr_list,auc_list=dict(),dict(),dict()\n",
    "\n",
    "\n",
    "logistic=LogisticRegression(max_iter=1000)\n",
    "logistic.fit(X_train1.fillna(0),y_train1)\n",
    "fpr_list[0], tpr_list[0], _ = roc_curve(y_test1, y_roc(logistic,X_test1.fillna(0)))\n",
    "print('Logistic regression train score:',\n",
    "      logistic.score(X_train1.fillna(0),y_train1),'\\n test score:',logistic.score(X_test1.fillna(0),y_test1.fillna(0)),'\\n Logistic regression train Brier score:',\n",
    "      brier_score(logistic.predict_proba(X_train1.fillna(0)),y_train1),'\\n test Brier score:',brier_score(logistic.predict_proba(X_test1.fillna(0)),y_test1),\n",
    "     '\\n AUROC:',roc_auc_score(y_test1, y_roc(logistic,X_test1.fillna(0))))\n",
    "\n",
    "RF=RandomForestClassifier(n_estimators=150,min_samples_split=2)\n",
    "RF.fit(X_train1.fillna(0),y_train1)\n",
    "fpr_list[1], tpr_list[1], _ = roc_curve(y_test1, y_roc(RF,X_test1.fillna(0)))\n",
    "print('Random forest train score:',\n",
    "      RF.score(X_train1.fillna(0),y_train1),'\\n test score:',RF.score(X_test1.fillna(0),y_test1.fillna(0)),'\\n Random forest  train Brier score:',\n",
    "      brier_score(RF.predict_proba(X_train1.fillna(0)),y_train1),'\\n test Brier score:',brier_score(RF.predict_proba(X_test1.fillna(0)),y_test1),\n",
    "     '\\n AUROC:',roc_auc_score(y_test1, y_roc(RF,X_test1.fillna(0))))\n",
    "\n",
    "GBDT=GradientBoostingClassifier()\n",
    "params_SGD={'n_estimators':[150,100],'min_samples_split':[2,4]}\n",
    "grid_SGD=GridSearchCV(GBDT,param_grid=params_SGD, scoring='neg_brier_score',cv=3)\n",
    "grid_SGD.fit(X_train1.fillna(0),y_train1.fillna(0))\n",
    "fpr_list[2], tpr_list[2], _ = roc_curve(y_test1, y_roc(grid_SGD.best_estimator_,X_test1.fillna(0)))\n",
    "print('SGD best layer size:',grid_SGD.best_params_,'\\n best train score:',\n",
    "      grid_SGD.best_score_,'\\n test score:',grid_SGD.score(X_test1.fillna(0),y_test1.fillna(0)),'\\n SGD  train Brier score:',\n",
    "      brier_score(grid_SGD.predict_proba(X_train1.fillna(0)),y_train1),'\\n test Brier score:',brier_score(grid_SGD.predict_proba(X_test1.fillna(0)),y_test1),\n",
    "     '\\n AUROC:',roc_auc_score(y_test1, y_roc(grid_SGD.best_estimator_,X_test1.fillna(0))))\n",
    "\n",
    "pipe = Sequential()\n",
    "n_cols = X_train1.shape[1]\n",
    "pipe.add(BatchNormalization(input_shape=(n_cols,)))\n",
    "pipe.add(Dense(70, activation= 'linear'))\n",
    "pipe.add(Dropout(0.3))\n",
    "pipe.add(Dense(50, activation= 'relu'))\n",
    "pipe.add(Dropout(0.3))\n",
    "pipe.add(Dense(50, activation= 'relu'))\n",
    "pipe.add(Dropout(0.3))\n",
    "#model.add(Dense(100, activation='relu'))\n",
    "#model.add(Dropout(0.3))\n",
    "#model.add(Dense(100, activation='linear'))\n",
    "pipe.add(BatchNormalization())\n",
    "pipe.add(Dense(2, activation='softmax'))\n",
    "    #model.compile(\n",
    "        #optimizer='Adam',\n",
    "        #loss='mean_squared_error',\n",
    "        #metrics=['accuracy'])\n",
    "early_stopping_monitor = EarlyStopping(patience=50)\n",
    "sgd = keras.optimizers.SGD(lr=.001, decay=2e-4, momentum=0.9, nesterov=True)\n",
    "pipe.compile(loss = 'sparse_categorical_crossentropy',optimizer = 'sgd', metrics=['accuracy'])\n",
    "history=pipe.fit(X_train1.fillna(0).astype('float32'), y_train1, validation_split=0.3, epochs=200, callbacks=[early_stopping_monitor])\n",
    "#history=model.fit(X_train, y_train, validation_split=0.2, epochs=25)\n",
    "score = pipe.evaluate(X_test1.fillna(0).astype('float32'), y_test1, verbose=0)\n",
    "pipe.fit(X_train1.fillna(0).astype('float32'),y_train1.fillna(0))\n",
    "fpr_list[3], tpr_list[3], _ = roc_curve(y_test1, y_roc(pipe,X_test1.fillna(0).astype('float32')))\n",
    "print('MLP train Brier score:',\n",
    "      brier_score(pipe.predict_proba(X_train1.fillna(0).astype('float32')),y_train1),'\\n test Brier score:',brier_score(pipe.predict_proba(X_test1.fillna(0).astype('float32')),y_test1),\n",
    "     '\\n AUROC:',roc_auc_score(y_test1, y_roc(pipe,X_test1.fillna(0).astype('float32'))))\n",
    "\n",
    "\n",
    "XGB=XGBClassifier()\n",
    "params_XGB={    'n_estimators': [50, 100],\n",
    "    'max_depth': [2, 3],\n",
    "    'min_child_weight': [2,4,6],\n",
    "    'subsample': [0.9],\n",
    "    'colsample_bytree': [0.8],'reg_lambda':[1000]}\n",
    "grid_XGB=GridSearchCV(XGB,param_grid=params_XGB, scoring='neg_brier_score',cv=3)\n",
    "grid_XGB.fit(X_train1.fillna(0),y_train1.fillna(0))\n",
    "fpr_list[4], tpr_list[4], _ = roc_curve(y_test1, y_roc(grid_XGB.best_estimator_,X_test1.fillna(0)))\n",
    "print('Xgboost best layer size:',grid_XGB.best_params_,'\\n best train score:',\n",
    "      grid_XGB.best_score_,'\\n test score:',grid_XGB.score(X_test1.fillna(0),y_test1.fillna(0)),'\\n Xgboost train Brier score:',\n",
    "      brier_score(grid_XGB.predict_proba(X_train1.fillna(0)),y_train1),'\\n test Brier score:',brier_score(grid_XGB.predict_proba(X_test1.fillna(0)),y_test1),\n",
    "     '\\n AUROC:',roc_auc_score(y_test1, y_roc(grid_XGB.best_estimator_,X_test1.fillna(0))))\n",
    "\n",
    "\n",
    "\n",
    "colors = cycle(['aqua', 'red', 'green','orange','blue'])\n",
    "labels=['Logistic Regression','Random Forest','Gradient Boosting','MLP','Xgboost']\n",
    "for i, label, color in zip(range(len(fpr_list)), labels, colors):\n",
    "    legend= label + ' (area = {1:0.2f})'''.format(i, auc(fpr_list[i], tpr_list[i]))\n",
    "    plt.plot(fpr_list[i], tpr_list[i], color=color, lw=2,label=legend)\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC curve for different methods on Recidivism_1Year')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('ROC_plot.png',dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "simplified-wildlife",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Percent_Days_Employed</td>\n",
       "      <td>0.105911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Jobs_Per_Year</td>\n",
       "      <td>0.090166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Avg_Days_per_DrugTest</td>\n",
       "      <td>0.087627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Prior_Arrest_Episodes_PPViolationCharges</td>\n",
       "      <td>0.067799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Age_at_Release</td>\n",
       "      <td>0.066556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>WATP</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>VEH</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>VACS</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>TOIL</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>PUMA</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>201 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Feature  importance\n",
       "45                      Percent_Days_Employed    0.105911\n",
       "46                              Jobs_Per_Year    0.090166\n",
       "40                      Avg_Days_per_DrugTest    0.087627\n",
       "16   Prior_Arrest_Episodes_PPViolationCharges    0.067799\n",
       "2                              Age_at_Release    0.066556\n",
       "..                                        ...         ...\n",
       "95                                       WATP    0.000000\n",
       "94                                        VEH    0.000000\n",
       "92                                       VACS    0.000000\n",
       "91                                       TOIL    0.000000\n",
       "200                                      PUMA    0.000000\n",
       "\n",
       "[201 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GBDT_importance=pd.DataFrame()\n",
    "GBDT_importance['Feature']=raw_extend.drop(['ID','Recidivism_Within_3years','Recidivism_Arrest_Year1','Recidivism_Arrest_Year2','Recidivism_Arrest_Year3'],axis=1).columns\n",
    "GBDT_importance['importance']=grid_SGD.best_estimator_.feature_importances_\n",
    "GBDT_importance=GBDT_importance.sort_values(by='importance',ascending=False)\n",
    "GBDT_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "emerging-trader",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['PUMA'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-e2053f0d5dc4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mraw_extend_remove\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mraw_extend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGBDT_importance\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mGBDT_importance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimportance\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFeature\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mraw_extend_remove\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mraw_extend_remove\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'PUMA'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4313\u001b[0m             \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4314\u001b[0m             \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4315\u001b[1;33m             \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4316\u001b[0m         )\n\u001b[0;32m   4317\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4151\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4152\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4153\u001b[1;33m                 \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4155\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[1;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[0;32m   4186\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4187\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4188\u001b[1;33m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4189\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   5589\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5590\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5591\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{labels[mask]} not found in axis\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5592\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5593\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['PUMA'] not found in axis\""
     ]
    }
   ],
   "source": [
    "raw_extend_remove=raw_extend.drop(list(GBDT_importance[GBDT_importance.importance==0].Feature),axis=1)\n",
    "raw_extend_remove=raw_extend_remove.drop(columns=['PUMA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "abroad-setting",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic_t regression train score: 0.7474704890387859 \n",
      " test score: 0.7413847613025608 \n",
      " logistic_t regression train Brier score: 0.17316677698794947 \n",
      " test Brier score: 0.1733842314181451 \n",
      " AUROC: 0.7061798958415116\n",
      "Random forest train score: 1.0 \n",
      " test score: 0.7407524502055011 \n",
      " Random forest  train Brier score: 0.024287961401535964 \n",
      " test Brier score: 0.17402142832051218 \n",
      " AUROC: 0.702015126534244\n",
      "SGD best layer size: {'min_samples_split': 4, 'n_estimators': 100} \n",
      " best train score: -0.17000461902078176 \n",
      " test score: -0.16864156061796565 \n",
      " SGD  train Brier score: 0.1525993611187537 \n",
      " test Brier score: 0.16864156061796584 \n",
      " AUROC: 0.7300798199471356\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\myjr\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208/208 [==============================] - 1s 3ms/step - loss: 0.7816 - accuracy: 0.6138 - val_loss: 0.5570 - val_accuracy: 0.7482\n",
      "Epoch 2/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.6229 - accuracy: 0.6916 - val_loss: 0.5526 - val_accuracy: 0.7492\n",
      "Epoch 3/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5940 - accuracy: 0.7198 - val_loss: 0.5504 - val_accuracy: 0.7496\n",
      "Epoch 4/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5758 - accuracy: 0.7336 - val_loss: 0.5493 - val_accuracy: 0.7492\n",
      "Epoch 5/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5712 - accuracy: 0.7366 - val_loss: 0.5490 - val_accuracy: 0.7489\n",
      "Epoch 6/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5679 - accuracy: 0.7378 - val_loss: 0.5482 - val_accuracy: 0.7499\n",
      "Epoch 7/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5670 - accuracy: 0.7401 - val_loss: 0.5481 - val_accuracy: 0.7496\n",
      "Epoch 8/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5635 - accuracy: 0.7387 - val_loss: 0.5467 - val_accuracy: 0.7499\n",
      "Epoch 9/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5616 - accuracy: 0.7413 - val_loss: 0.5452 - val_accuracy: 0.7503\n",
      "Epoch 10/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5614 - accuracy: 0.7393 - val_loss: 0.5442 - val_accuracy: 0.7499\n",
      "Epoch 11/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5612 - accuracy: 0.7396 - val_loss: 0.5435 - val_accuracy: 0.7510\n",
      "Epoch 12/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5601 - accuracy: 0.7409 - val_loss: 0.5418 - val_accuracy: 0.7503\n",
      "Epoch 13/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5607 - accuracy: 0.7412 - val_loss: 0.5426 - val_accuracy: 0.7499\n",
      "Epoch 14/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5610 - accuracy: 0.7384 - val_loss: 0.5413 - val_accuracy: 0.7503\n",
      "Epoch 15/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5573 - accuracy: 0.7390 - val_loss: 0.5405 - val_accuracy: 0.7499\n",
      "Epoch 16/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5612 - accuracy: 0.7399 - val_loss: 0.5398 - val_accuracy: 0.7496\n",
      "Epoch 17/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5550 - accuracy: 0.7386 - val_loss: 0.5371 - val_accuracy: 0.7492\n",
      "Epoch 18/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5544 - accuracy: 0.7350 - val_loss: 0.5372 - val_accuracy: 0.7492\n",
      "Epoch 19/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5557 - accuracy: 0.7387 - val_loss: 0.5359 - val_accuracy: 0.7496\n",
      "Epoch 20/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5541 - accuracy: 0.7374 - val_loss: 0.5369 - val_accuracy: 0.7503\n",
      "Epoch 21/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5523 - accuracy: 0.7398 - val_loss: 0.5352 - val_accuracy: 0.7496\n",
      "Epoch 22/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5479 - accuracy: 0.7406 - val_loss: 0.5334 - val_accuracy: 0.7496\n",
      "Epoch 23/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5549 - accuracy: 0.7380 - val_loss: 0.5359 - val_accuracy: 0.7489\n",
      "Epoch 24/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5498 - accuracy: 0.7407 - val_loss: 0.5333 - val_accuracy: 0.7492\n",
      "Epoch 25/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5510 - accuracy: 0.7390 - val_loss: 0.5319 - val_accuracy: 0.7468\n",
      "Epoch 26/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5491 - accuracy: 0.7389 - val_loss: 0.5319 - val_accuracy: 0.7475\n",
      "Epoch 27/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5500 - accuracy: 0.7393 - val_loss: 0.5323 - val_accuracy: 0.7499\n",
      "Epoch 28/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5487 - accuracy: 0.7410 - val_loss: 0.5316 - val_accuracy: 0.7489\n",
      "Epoch 29/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5487 - accuracy: 0.7404 - val_loss: 0.5322 - val_accuracy: 0.7482\n",
      "Epoch 30/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5476 - accuracy: 0.7401 - val_loss: 0.5308 - val_accuracy: 0.7475\n",
      "Epoch 31/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5476 - accuracy: 0.7398 - val_loss: 0.5321 - val_accuracy: 0.7489\n",
      "Epoch 32/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5465 - accuracy: 0.7393 - val_loss: 0.5300 - val_accuracy: 0.7485\n",
      "Epoch 33/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5475 - accuracy: 0.7387 - val_loss: 0.5290 - val_accuracy: 0.7475\n",
      "Epoch 34/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5454 - accuracy: 0.7387 - val_loss: 0.5276 - val_accuracy: 0.7482\n",
      "Epoch 35/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5445 - accuracy: 0.7410 - val_loss: 0.5284 - val_accuracy: 0.7468\n",
      "Epoch 36/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5444 - accuracy: 0.7366 - val_loss: 0.5289 - val_accuracy: 0.7471\n",
      "Epoch 37/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5440 - accuracy: 0.7406 - val_loss: 0.5293 - val_accuracy: 0.7482\n",
      "Epoch 38/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5429 - accuracy: 0.7402 - val_loss: 0.5274 - val_accuracy: 0.7457\n",
      "Epoch 39/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5430 - accuracy: 0.7372 - val_loss: 0.5283 - val_accuracy: 0.7460\n",
      "Epoch 40/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5441 - accuracy: 0.7380 - val_loss: 0.5272 - val_accuracy: 0.7482\n",
      "Epoch 41/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5406 - accuracy: 0.7378 - val_loss: 0.5260 - val_accuracy: 0.7460\n",
      "Epoch 42/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5427 - accuracy: 0.7418 - val_loss: 0.5268 - val_accuracy: 0.7485\n",
      "Epoch 43/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5416 - accuracy: 0.7402 - val_loss: 0.5265 - val_accuracy: 0.7464\n",
      "Epoch 44/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5446 - accuracy: 0.7392 - val_loss: 0.5273 - val_accuracy: 0.7464\n",
      "Epoch 45/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5386 - accuracy: 0.7416 - val_loss: 0.5264 - val_accuracy: 0.7457\n",
      "Epoch 46/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5429 - accuracy: 0.7390 - val_loss: 0.5258 - val_accuracy: 0.7460\n",
      "Epoch 47/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5418 - accuracy: 0.7431 - val_loss: 0.5263 - val_accuracy: 0.7464\n",
      "Epoch 48/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5384 - accuracy: 0.7402 - val_loss: 0.5250 - val_accuracy: 0.7460\n",
      "Epoch 49/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5397 - accuracy: 0.7402 - val_loss: 0.5251 - val_accuracy: 0.7475\n",
      "Epoch 50/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5418 - accuracy: 0.7428 - val_loss: 0.5263 - val_accuracy: 0.7478\n",
      "Epoch 51/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5403 - accuracy: 0.7413 - val_loss: 0.5242 - val_accuracy: 0.7475\n",
      "Epoch 52/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5387 - accuracy: 0.7415 - val_loss: 0.5247 - val_accuracy: 0.7471\n",
      "Epoch 53/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5389 - accuracy: 0.7427 - val_loss: 0.5244 - val_accuracy: 0.7460\n",
      "Epoch 54/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5384 - accuracy: 0.7425 - val_loss: 0.5245 - val_accuracy: 0.7446\n",
      "Epoch 55/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5363 - accuracy: 0.7413 - val_loss: 0.5232 - val_accuracy: 0.7446\n",
      "Epoch 56/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5391 - accuracy: 0.7430 - val_loss: 0.5235 - val_accuracy: 0.7453\n",
      "Epoch 57/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5365 - accuracy: 0.7399 - val_loss: 0.5240 - val_accuracy: 0.7468\n",
      "Epoch 58/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5357 - accuracy: 0.7443 - val_loss: 0.5234 - val_accuracy: 0.7464\n",
      "Epoch 59/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5354 - accuracy: 0.7419 - val_loss: 0.5239 - val_accuracy: 0.7471\n",
      "Epoch 60/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5369 - accuracy: 0.7413 - val_loss: 0.5237 - val_accuracy: 0.7471\n",
      "Epoch 61/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5345 - accuracy: 0.7395 - val_loss: 0.5247 - val_accuracy: 0.7492\n",
      "Epoch 62/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5374 - accuracy: 0.7415 - val_loss: 0.5231 - val_accuracy: 0.7475\n",
      "Epoch 63/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5374 - accuracy: 0.7434 - val_loss: 0.5228 - val_accuracy: 0.7457\n",
      "Epoch 64/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5341 - accuracy: 0.7458 - val_loss: 0.5232 - val_accuracy: 0.7468\n",
      "Epoch 65/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5323 - accuracy: 0.7390 - val_loss: 0.5235 - val_accuracy: 0.7460\n",
      "Epoch 66/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5352 - accuracy: 0.7428 - val_loss: 0.5229 - val_accuracy: 0.7457\n",
      "Epoch 67/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5387 - accuracy: 0.7381 - val_loss: 0.5240 - val_accuracy: 0.7464\n",
      "Epoch 68/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5362 - accuracy: 0.7455 - val_loss: 0.5228 - val_accuracy: 0.7468\n",
      "Epoch 69/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5387 - accuracy: 0.7398 - val_loss: 0.5226 - val_accuracy: 0.7482\n",
      "Epoch 70/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5373 - accuracy: 0.7421 - val_loss: 0.5229 - val_accuracy: 0.7457\n",
      "Epoch 71/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5360 - accuracy: 0.7390 - val_loss: 0.5219 - val_accuracy: 0.7475\n",
      "Epoch 72/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5348 - accuracy: 0.7458 - val_loss: 0.5217 - val_accuracy: 0.7485\n",
      "Epoch 73/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5328 - accuracy: 0.7433 - val_loss: 0.5226 - val_accuracy: 0.7468\n",
      "Epoch 74/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5349 - accuracy: 0.7424 - val_loss: 0.5222 - val_accuracy: 0.7464\n",
      "Epoch 75/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5319 - accuracy: 0.7412 - val_loss: 0.5209 - val_accuracy: 0.7485\n",
      "Epoch 76/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5310 - accuracy: 0.7439 - val_loss: 0.5212 - val_accuracy: 0.7471\n",
      "Epoch 77/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5342 - accuracy: 0.7427 - val_loss: 0.5211 - val_accuracy: 0.7485\n",
      "Epoch 78/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5323 - accuracy: 0.7380 - val_loss: 0.5217 - val_accuracy: 0.7482\n",
      "Epoch 79/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5316 - accuracy: 0.7433 - val_loss: 0.5213 - val_accuracy: 0.7468\n",
      "Epoch 80/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5340 - accuracy: 0.7433 - val_loss: 0.5219 - val_accuracy: 0.7496\n",
      "Epoch 81/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5340 - accuracy: 0.7436 - val_loss: 0.5218 - val_accuracy: 0.7468\n",
      "Epoch 82/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5332 - accuracy: 0.7428 - val_loss: 0.5215 - val_accuracy: 0.7464\n",
      "Epoch 83/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5343 - accuracy: 0.7402 - val_loss: 0.5213 - val_accuracy: 0.7475\n",
      "Epoch 84/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5314 - accuracy: 0.7451 - val_loss: 0.5214 - val_accuracy: 0.7460\n",
      "Epoch 85/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5333 - accuracy: 0.7437 - val_loss: 0.5216 - val_accuracy: 0.7457\n",
      "Epoch 86/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5294 - accuracy: 0.7458 - val_loss: 0.5215 - val_accuracy: 0.7478\n",
      "Epoch 87/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5346 - accuracy: 0.7413 - val_loss: 0.5210 - val_accuracy: 0.7468\n",
      "Epoch 88/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5320 - accuracy: 0.7451 - val_loss: 0.5210 - val_accuracy: 0.7471\n",
      "Epoch 89/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5318 - accuracy: 0.7436 - val_loss: 0.5212 - val_accuracy: 0.7475\n",
      "Epoch 90/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5322 - accuracy: 0.7431 - val_loss: 0.5212 - val_accuracy: 0.7478\n",
      "Epoch 91/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5306 - accuracy: 0.7443 - val_loss: 0.5206 - val_accuracy: 0.7468\n",
      "Epoch 92/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5322 - accuracy: 0.7404 - val_loss: 0.5216 - val_accuracy: 0.7464\n",
      "Epoch 93/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5350 - accuracy: 0.7440 - val_loss: 0.5210 - val_accuracy: 0.7475\n",
      "Epoch 94/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5319 - accuracy: 0.7387 - val_loss: 0.5213 - val_accuracy: 0.7475\n",
      "Epoch 95/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5340 - accuracy: 0.7418 - val_loss: 0.5218 - val_accuracy: 0.7485\n",
      "Epoch 96/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5356 - accuracy: 0.7439 - val_loss: 0.5219 - val_accuracy: 0.7460\n",
      "Epoch 97/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5308 - accuracy: 0.7433 - val_loss: 0.5215 - val_accuracy: 0.7468\n",
      "Epoch 98/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5333 - accuracy: 0.7381 - val_loss: 0.5213 - val_accuracy: 0.7464\n",
      "Epoch 99/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5341 - accuracy: 0.7448 - val_loss: 0.5213 - val_accuracy: 0.7471\n",
      "Epoch 100/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5328 - accuracy: 0.7415 - val_loss: 0.5205 - val_accuracy: 0.7446\n",
      "Epoch 101/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5321 - accuracy: 0.7421 - val_loss: 0.5205 - val_accuracy: 0.7464\n",
      "Epoch 102/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5299 - accuracy: 0.7476 - val_loss: 0.5202 - val_accuracy: 0.7471\n",
      "Epoch 103/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5277 - accuracy: 0.7430 - val_loss: 0.5216 - val_accuracy: 0.7506\n",
      "Epoch 104/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5289 - accuracy: 0.7457 - val_loss: 0.5212 - val_accuracy: 0.7492\n",
      "Epoch 105/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5308 - accuracy: 0.7422 - val_loss: 0.5208 - val_accuracy: 0.7464\n",
      "Epoch 106/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5302 - accuracy: 0.7428 - val_loss: 0.5212 - val_accuracy: 0.7460\n",
      "Epoch 107/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5317 - accuracy: 0.7442 - val_loss: 0.5218 - val_accuracy: 0.7464\n",
      "Epoch 108/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5320 - accuracy: 0.7431 - val_loss: 0.5210 - val_accuracy: 0.7471\n",
      "Epoch 109/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5307 - accuracy: 0.7424 - val_loss: 0.5206 - val_accuracy: 0.7450\n",
      "Epoch 110/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5323 - accuracy: 0.7404 - val_loss: 0.5205 - val_accuracy: 0.7485\n",
      "Epoch 111/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5310 - accuracy: 0.7401 - val_loss: 0.5202 - val_accuracy: 0.7475\n",
      "Epoch 112/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5319 - accuracy: 0.7410 - val_loss: 0.5206 - val_accuracy: 0.7468\n",
      "Epoch 113/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5300 - accuracy: 0.7425 - val_loss: 0.5203 - val_accuracy: 0.7460\n",
      "Epoch 114/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5308 - accuracy: 0.7419 - val_loss: 0.5202 - val_accuracy: 0.7485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5288 - accuracy: 0.7389 - val_loss: 0.5202 - val_accuracy: 0.7460\n",
      "Epoch 116/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5314 - accuracy: 0.7425 - val_loss: 0.5204 - val_accuracy: 0.7475\n",
      "Epoch 117/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5305 - accuracy: 0.7431 - val_loss: 0.5211 - val_accuracy: 0.7443\n",
      "Epoch 118/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5287 - accuracy: 0.7436 - val_loss: 0.5209 - val_accuracy: 0.7489\n",
      "Epoch 119/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5311 - accuracy: 0.7446 - val_loss: 0.5208 - val_accuracy: 0.7464\n",
      "Epoch 120/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5289 - accuracy: 0.7439 - val_loss: 0.5209 - val_accuracy: 0.7464\n",
      "Epoch 121/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5294 - accuracy: 0.7430 - val_loss: 0.5208 - val_accuracy: 0.7475\n",
      "Epoch 122/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5278 - accuracy: 0.7410 - val_loss: 0.5209 - val_accuracy: 0.7499\n",
      "Epoch 123/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5271 - accuracy: 0.7460 - val_loss: 0.5211 - val_accuracy: 0.7496\n",
      "Epoch 124/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5290 - accuracy: 0.7418 - val_loss: 0.5201 - val_accuracy: 0.7492\n",
      "Epoch 125/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5314 - accuracy: 0.7398 - val_loss: 0.5201 - val_accuracy: 0.7446\n",
      "Epoch 126/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5287 - accuracy: 0.7434 - val_loss: 0.5208 - val_accuracy: 0.7468\n",
      "Epoch 127/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5305 - accuracy: 0.7419 - val_loss: 0.5210 - val_accuracy: 0.7460\n",
      "Epoch 128/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5261 - accuracy: 0.7467 - val_loss: 0.5208 - val_accuracy: 0.7471\n",
      "Epoch 129/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5287 - accuracy: 0.7415 - val_loss: 0.5207 - val_accuracy: 0.7460\n",
      "Epoch 130/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5304 - accuracy: 0.7410 - val_loss: 0.5204 - val_accuracy: 0.7453\n",
      "Epoch 131/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5273 - accuracy: 0.7455 - val_loss: 0.5201 - val_accuracy: 0.7471\n",
      "Epoch 132/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5322 - accuracy: 0.7390 - val_loss: 0.5204 - val_accuracy: 0.7460\n",
      "Epoch 133/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5266 - accuracy: 0.7460 - val_loss: 0.5204 - val_accuracy: 0.7464\n",
      "Epoch 134/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5285 - accuracy: 0.7406 - val_loss: 0.5204 - val_accuracy: 0.7453\n",
      "Epoch 135/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5304 - accuracy: 0.7404 - val_loss: 0.5204 - val_accuracy: 0.7468\n",
      "Epoch 136/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5305 - accuracy: 0.7419 - val_loss: 0.5201 - val_accuracy: 0.7468\n",
      "Epoch 137/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5303 - accuracy: 0.7401 - val_loss: 0.5201 - val_accuracy: 0.7471\n",
      "Epoch 138/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5292 - accuracy: 0.7395 - val_loss: 0.5206 - val_accuracy: 0.7460\n",
      "Epoch 139/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5276 - accuracy: 0.7406 - val_loss: 0.5203 - val_accuracy: 0.7464\n",
      "Epoch 140/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5273 - accuracy: 0.7472 - val_loss: 0.5206 - val_accuracy: 0.7478\n",
      "Epoch 141/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5287 - accuracy: 0.7422 - val_loss: 0.5207 - val_accuracy: 0.7492\n",
      "Epoch 142/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5311 - accuracy: 0.7419 - val_loss: 0.5207 - val_accuracy: 0.7471\n",
      "Epoch 143/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5300 - accuracy: 0.7410 - val_loss: 0.5203 - val_accuracy: 0.7496\n",
      "Epoch 144/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5309 - accuracy: 0.7412 - val_loss: 0.5212 - val_accuracy: 0.7468\n",
      "Epoch 145/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5253 - accuracy: 0.7452 - val_loss: 0.5209 - val_accuracy: 0.7460\n",
      "Epoch 146/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5266 - accuracy: 0.7424 - val_loss: 0.5203 - val_accuracy: 0.7468\n",
      "Epoch 147/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5291 - accuracy: 0.7445 - val_loss: 0.5205 - val_accuracy: 0.7478\n",
      "Epoch 148/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5283 - accuracy: 0.7409 - val_loss: 0.5202 - val_accuracy: 0.7450\n",
      "Epoch 149/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5276 - accuracy: 0.7419 - val_loss: 0.5200 - val_accuracy: 0.7453\n",
      "Epoch 150/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5294 - accuracy: 0.7418 - val_loss: 0.5201 - val_accuracy: 0.7485\n",
      "Epoch 151/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5261 - accuracy: 0.7455 - val_loss: 0.5200 - val_accuracy: 0.7489\n",
      "Epoch 152/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5296 - accuracy: 0.7418 - val_loss: 0.5207 - val_accuracy: 0.7457\n",
      "Epoch 153/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5282 - accuracy: 0.7437 - val_loss: 0.5199 - val_accuracy: 0.7471\n",
      "Epoch 154/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5304 - accuracy: 0.7398 - val_loss: 0.5206 - val_accuracy: 0.7460\n",
      "Epoch 155/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5240 - accuracy: 0.7437 - val_loss: 0.5203 - val_accuracy: 0.7468\n",
      "Epoch 156/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5266 - accuracy: 0.7455 - val_loss: 0.5203 - val_accuracy: 0.7457\n",
      "Epoch 157/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5258 - accuracy: 0.7464 - val_loss: 0.5213 - val_accuracy: 0.7471\n",
      "Epoch 158/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5316 - accuracy: 0.7424 - val_loss: 0.5203 - val_accuracy: 0.7457\n",
      "Epoch 159/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5292 - accuracy: 0.7433 - val_loss: 0.5206 - val_accuracy: 0.7482\n",
      "Epoch 160/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5279 - accuracy: 0.7443 - val_loss: 0.5204 - val_accuracy: 0.7471\n",
      "Epoch 161/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5317 - accuracy: 0.7416 - val_loss: 0.5205 - val_accuracy: 0.7478\n",
      "Epoch 162/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5240 - accuracy: 0.7445 - val_loss: 0.5207 - val_accuracy: 0.7471\n",
      "Epoch 163/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5303 - accuracy: 0.7449 - val_loss: 0.5203 - val_accuracy: 0.7492\n",
      "Epoch 164/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5238 - accuracy: 0.7440 - val_loss: 0.5196 - val_accuracy: 0.7492\n",
      "Epoch 165/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5261 - accuracy: 0.7449 - val_loss: 0.5204 - val_accuracy: 0.7464\n",
      "Epoch 166/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5301 - accuracy: 0.7415 - val_loss: 0.5195 - val_accuracy: 0.7471\n",
      "Epoch 167/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5256 - accuracy: 0.7404 - val_loss: 0.5201 - val_accuracy: 0.7464\n",
      "Epoch 168/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5265 - accuracy: 0.7458 - val_loss: 0.5209 - val_accuracy: 0.7457\n",
      "Epoch 169/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5273 - accuracy: 0.7440 - val_loss: 0.5201 - val_accuracy: 0.7478\n",
      "Epoch 170/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5278 - accuracy: 0.7419 - val_loss: 0.5209 - val_accuracy: 0.7450\n",
      "Epoch 171/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5263 - accuracy: 0.7446 - val_loss: 0.5196 - val_accuracy: 0.7482\n",
      "Epoch 172/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5273 - accuracy: 0.7424 - val_loss: 0.5198 - val_accuracy: 0.7457\n",
      "Epoch 173/200\n",
      "208/208 [==============================] - 1s 2ms/step - loss: 0.5245 - accuracy: 0.7437 - val_loss: 0.5208 - val_accuracy: 0.7464\n",
      "Epoch 174/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5274 - accuracy: 0.7442 - val_loss: 0.5206 - val_accuracy: 0.7468\n",
      "Epoch 175/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5269 - accuracy: 0.7428 - val_loss: 0.5201 - val_accuracy: 0.7471\n",
      "Epoch 176/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5242 - accuracy: 0.7431 - val_loss: 0.5201 - val_accuracy: 0.7517\n",
      "Epoch 177/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5284 - accuracy: 0.7424 - val_loss: 0.5202 - val_accuracy: 0.7468\n",
      "Epoch 178/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5264 - accuracy: 0.7424 - val_loss: 0.5204 - val_accuracy: 0.7468\n",
      "Epoch 179/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5247 - accuracy: 0.7452 - val_loss: 0.5202 - val_accuracy: 0.7489\n",
      "Epoch 180/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5261 - accuracy: 0.7402 - val_loss: 0.5194 - val_accuracy: 0.7478\n",
      "Epoch 181/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5254 - accuracy: 0.7451 - val_loss: 0.5210 - val_accuracy: 0.7460\n",
      "Epoch 182/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5252 - accuracy: 0.7455 - val_loss: 0.5203 - val_accuracy: 0.7485\n",
      "Epoch 183/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5254 - accuracy: 0.7443 - val_loss: 0.5205 - val_accuracy: 0.7460\n",
      "Epoch 184/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5261 - accuracy: 0.7439 - val_loss: 0.5201 - val_accuracy: 0.7468\n",
      "Epoch 185/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5281 - accuracy: 0.7413 - val_loss: 0.5196 - val_accuracy: 0.7468\n",
      "Epoch 186/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5266 - accuracy: 0.7409 - val_loss: 0.5200 - val_accuracy: 0.7460\n",
      "Epoch 187/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5265 - accuracy: 0.7418 - val_loss: 0.5197 - val_accuracy: 0.7471\n",
      "Epoch 188/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5257 - accuracy: 0.7398 - val_loss: 0.5202 - val_accuracy: 0.7485\n",
      "Epoch 189/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5249 - accuracy: 0.7451 - val_loss: 0.5202 - val_accuracy: 0.7489\n",
      "Epoch 190/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5265 - accuracy: 0.7428 - val_loss: 0.5202 - val_accuracy: 0.7499\n",
      "Epoch 191/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5256 - accuracy: 0.7476 - val_loss: 0.5205 - val_accuracy: 0.7464\n",
      "Epoch 192/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5268 - accuracy: 0.7422 - val_loss: 0.5206 - val_accuracy: 0.7492\n",
      "Epoch 193/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5239 - accuracy: 0.7431 - val_loss: 0.5197 - val_accuracy: 0.7468\n",
      "Epoch 194/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5262 - accuracy: 0.7409 - val_loss: 0.5199 - val_accuracy: 0.7468\n",
      "Epoch 195/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5239 - accuracy: 0.7404 - val_loss: 0.5192 - val_accuracy: 0.7468\n",
      "Epoch 196/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5270 - accuracy: 0.7410 - val_loss: 0.5192 - val_accuracy: 0.7485\n",
      "Epoch 197/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5248 - accuracy: 0.7407 - val_loss: 0.5192 - val_accuracy: 0.7485\n",
      "Epoch 198/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5259 - accuracy: 0.7412 - val_loss: 0.5202 - val_accuracy: 0.7464\n",
      "Epoch 199/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5258 - accuracy: 0.7404 - val_loss: 0.5198 - val_accuracy: 0.7460\n",
      "Epoch 200/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5294 - accuracy: 0.7443 - val_loss: 0.5206 - val_accuracy: 0.7468\n",
      "297/297 [==============================] - 0s 2ms/step - loss: 0.5238 - accuracy: 0.7506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\myjr\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:430: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
      "  warnings.warn('`model.predict_proba()` is deprecated and '\n",
      "C:\\Users\\myjr\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:430: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
      "  warnings.warn('`model.predict_proba()` is deprecated and '\n",
      "C:\\Users\\myjr\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:430: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
      "  warnings.warn('`model.predict_proba()` is deprecated and '\n",
      "C:\\Users\\myjr\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:430: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
      "  warnings.warn('`model.predict_proba()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP train Brier score: 0.17112599446814658 \n",
      " test Brier score: 0.17388917553753533 \n",
      " AUROC: 0.7044222867760592\n",
      "Xgboost best layer size: {'colsample_bytree': 0.8, 'max_depth': 3, 'min_child_weight': 4, 'n_estimators': 100, 'reg_lambda': 1000, 'subsample': 0.9} \n",
      " best train score: -0.17122557885284376 \n",
      " test score: -0.16920286951037405 \n",
      " Xgboost train Brier score: 0.16209568199803753 \n",
      " test Brier score: 0.16920286941518003 \n",
      " AUROC: 0.7280128758734395\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression,LogisticRegression\n",
    "\n",
    "\n",
    "X_train1,X_test1,y_train1,y_test1=train_test_split(raw_extend_remove.drop(['ID','Recidivism_Within_3years','Recidivism_Arrest_Year1','Recidivism_Arrest_Year2','Recidivism_Arrest_Year3'],axis=1),raw_extend_remove['Recidivism_Arrest_Year2'])\n",
    "fpr_list,tpr_list,auc_list=dict(),dict(),dict()\n",
    "\n",
    "\n",
    "logistic_t=LogisticRegression(max_iter=1000)\n",
    "logistic_t.fit(X_train1.fillna(0),y_train1)\n",
    "fpr_list[0], tpr_list[0], _ = roc_curve(y_test1, y_roc(logistic_t,X_test1.fillna(0)))\n",
    "print('logistic_t regression train score:',\n",
    "      logistic_t.score(X_train1.fillna(0),y_train1),'\\n test score:',logistic_t.score(X_test1.fillna(0),y_test1.fillna(0)),'\\n logistic_t regression train Brier score:',\n",
    "      brier_score(logistic_t.predict_proba(X_train1.fillna(0)),y_train1),'\\n test Brier score:',brier_score(logistic_t.predict_proba(X_test1.fillna(0)),y_test1),\n",
    "     '\\n AUROC:',roc_auc_score(y_test1, y_roc(logistic_t,X_test1.fillna(0))))\n",
    "\n",
    "RF=RandomForestClassifier(n_estimators=150,min_samples_split=2)\n",
    "RF.fit(X_train1.fillna(0),y_train1)\n",
    "fpr_list[1], tpr_list[1], _ = roc_curve(y_test1, y_roc(RF,X_test1.fillna(0)))\n",
    "print('Random forest train score:',\n",
    "      RF.score(X_train1.fillna(0),y_train1),'\\n test score:',RF.score(X_test1.fillna(0),y_test1.fillna(0)),'\\n Random forest  train Brier score:',\n",
    "      brier_score(RF.predict_proba(X_train1.fillna(0)),y_train1),'\\n test Brier score:',brier_score(RF.predict_proba(X_test1.fillna(0)),y_test1),\n",
    "     '\\n AUROC:',roc_auc_score(y_test1, y_roc(RF,X_test1.fillna(0))))\n",
    "\n",
    "GBDT=GradientBoostingClassifier()\n",
    "params_SGD={'n_estimators':[150,100],'min_samples_split':[2,4]}\n",
    "grid_SGD_t=GridSearchCV(GBDT,param_grid=params_SGD, scoring='neg_brier_score',cv=3)\n",
    "grid_SGD_t.fit(X_train1.fillna(0),y_train1.fillna(0))\n",
    "fpr_list[2], tpr_list[2], _ = roc_curve(y_test1, y_roc(grid_SGD_t.best_estimator_,X_test1.fillna(0)))\n",
    "print('SGD best layer size:',grid_SGD_t.best_params_,'\\n best train score:',\n",
    "      grid_SGD_t.best_score_,'\\n test score:',grid_SGD_t.score(X_test1.fillna(0),y_test1.fillna(0)),'\\n SGD  train Brier score:',\n",
    "      brier_score(grid_SGD_t.predict_proba(X_train1.fillna(0)),y_train1),'\\n test Brier score:',brier_score(grid_SGD_t.predict_proba(X_test1.fillna(0)),y_test1),\n",
    "     '\\n AUROC:',roc_auc_score(y_test1, y_roc(grid_SGD_t.best_estimator_,X_test1.fillna(0))))\n",
    "\n",
    "pipe_t = Sequential()\n",
    "n_cols = X_train1.shape[1]\n",
    "pipe_t.add(BatchNormalization(input_shape=(n_cols,)))\n",
    "pipe_t.add(Dense(70, activation= 'linear'))\n",
    "pipe_t.add(Dropout(0.3))\n",
    "pipe_t.add(Dense(50, activation= 'relu'))\n",
    "pipe_t.add(Dropout(0.3))\n",
    "pipe_t.add(Dense(50, activation= 'relu'))\n",
    "pipe_t.add(Dropout(0.3))\n",
    "#model.add(Dense(100, activation='relu'))\n",
    "#model.add(Dropout(0.3))\n",
    "#model.add(Dense(100, activation='linear'))\n",
    "pipe_t.add(BatchNormalization())\n",
    "pipe_t.add(Dense(2, activation='softmax'))\n",
    "    #model.compile(\n",
    "        #optimizer='Adam',\n",
    "        #loss='mean_squared_error',\n",
    "        #metrics=['accuracy'])\n",
    "early_stopping_monitor = EarlyStopping(patience=50)\n",
    "sgd = keras.optimizers.SGD(lr=.001, decay=2e-4, momentum=0.9, nesterov=True)\n",
    "pipe_t.compile(loss = 'sparse_categorical_crossentropy',optimizer = 'sgd', metrics=['accuracy'])\n",
    "history=pipe_t.fit(X_train1.fillna(0).astype('float32'), y_train1, validation_split=0.3, epochs=200, callbacks=[early_stopping_monitor])\n",
    "#history=model.fit(X_train, y_train, validation_split=0.2, epochs=25)\n",
    "score = pipe_t.evaluate(X_test1.fillna(0).astype('float32'), y_test1, verbose=0)\n",
    "pipe_t.fit(X_train1.fillna(0).astype('float32'),y_train1.fillna(0))\n",
    "fpr_list[3], tpr_list[3], _ = roc_curve(y_test1, y_roc(pipe_t,X_test1.fillna(0).astype('float32')))\n",
    "print('MLP train Brier score:',\n",
    "      brier_score(pipe_t.predict_proba(X_train1.fillna(0).astype('float32')),y_train1),'\\n test Brier score:',brier_score(pipe_t.predict_proba(X_test1.fillna(0).astype('float32')),y_test1),\n",
    "     '\\n AUROC:',roc_auc_score(y_test1, y_roc(pipe_t,X_test1.fillna(0).astype('float32'))))\n",
    "\n",
    "\n",
    "XGB=XGBClassifier()\n",
    "params_XGB={    'n_estimators': [50, 100],\n",
    "    'max_depth': [2, 3],\n",
    "    'min_child_weight': [2,4,6],\n",
    "    'subsample': [0.9],\n",
    "    'colsample_bytree': [0.8],'reg_lambda':[1000]}\n",
    "grid_XGB_t=GridSearchCV(XGB,param_grid=params_XGB, scoring='neg_brier_score',cv=3)\n",
    "grid_XGB_t.fit(X_train1.fillna(0),y_train1.fillna(0))\n",
    "fpr_list[4], tpr_list[4], _ = roc_curve(y_test1, y_roc(grid_XGB_t.best_estimator_,X_test1.fillna(0)))\n",
    "print('Xgboost best layer size:',grid_XGB_t.best_params_,'\\n best train score:',\n",
    "      grid_XGB_t.best_score_,'\\n test score:',grid_XGB_t.score(X_test1.fillna(0),y_test1.fillna(0)),'\\n Xgboost train Brier score:',\n",
    "      brier_score(grid_XGB_t.predict_proba(X_train1.fillna(0)),y_train1),'\\n test Brier score:',brier_score(grid_XGB_t.predict_proba(X_test1.fillna(0)),y_test1),\n",
    "     '\\n AUROC:',roc_auc_score(y_test1, y_roc(grid_XGB_t.best_estimator_,X_test1.fillna(0))))\n",
    "\n",
    "\n",
    "\n",
    "colors = cycle(['aqua', 'red', 'green','orange','blue'])\n",
    "labels=['logistic_t Regression','Random Forest','Gradient Boosting','MLP','Xgboost']\n",
    "for i, label, color in zip(range(len(fpr_list)), labels, colors):\n",
    "    legend= label + ' (area = {1:0.2f})'''.format(i, auc(fpr_list[i], tpr_list[i]))\n",
    "    plt.plot(fpr_list[i], tpr_list[i], color=color, lw=2,label=legend)\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC curve for different methods on Recidivism_1Year')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('ROC_plot.png',dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "familiar-advocacy",
   "metadata": {},
   "source": [
    "### Year2 Recidivism\n",
    "When training, we need to discard those who recommit crime in 1st year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "collect-warrant",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'raw_extend_remove' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-3192035a00fb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mLogisticRegression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mraw_extend_year2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mraw_extend_remove\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_extend_remove\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mraw_extend_remove\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRecidivism_Arrest_Year1\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mX_train1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_test1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_extend_year2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ID'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Recidivism_Within_3years'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Recidivism_Arrest_Year1'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Recidivism_Arrest_Year2'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Recidivism_Arrest_Year3'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mraw_extend_year2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Recidivism_Arrest_Year2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mfpr_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtpr_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mauc_list\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'raw_extend_remove' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression,LogisticRegression\n",
    "raw_extend_year2=raw_extend_remove.drop(raw_extend_remove[raw_extend_remove.Recidivism_Arrest_Year1==True].index)\n",
    "\n",
    "X_train1,X_test1,y_train1,y_test1=train_test_split(raw_extend_year2.drop(['ID','Recidivism_Within_3years','Recidivism_Arrest_Year1','Recidivism_Arrest_Year2','Recidivism_Arrest_Year3'],axis=1),raw_extend_year2['Recidivism_Arrest_Year2'])\n",
    "fpr_list,tpr_list,auc_list=dict(),dict(),dict()\n",
    "\n",
    "\n",
    "logistic=LogisticRegression(max_iter=1000)\n",
    "logistic.fit(X_train1.fillna(0),y_train1)\n",
    "fpr_list[0], tpr_list[0], _ = roc_curve(y_test1, y_roc(logistic,X_test1.fillna(0)))\n",
    "print('Logistic regression train score:',\n",
    "      logistic.score(X_train1.fillna(0),y_train1),'\\n test score:',logistic.score(X_test1.fillna(0),y_test1.fillna(0)),'\\n Logistic regression train Brier score:',\n",
    "      brier_score(logistic.predict_proba(X_train1.fillna(0)),y_train1),'\\n test Brier score:',brier_score(logistic.predict_proba(X_test1.fillna(0)),y_test1),\n",
    "     '\\n AUROC:',roc_auc_score(y_test1, y_roc(logistic,X_test1.fillna(0))))\n",
    "\n",
    "RF=RandomForestClassifier(n_estimators=150,min_samples_split=2)\n",
    "RF.fit(X_train1.fillna(0),y_train1)\n",
    "fpr_list[1], tpr_list[1], _ = roc_curve(y_test1, y_roc(RF,X_test1.fillna(0)))\n",
    "print('Random forest train score:',\n",
    "      RF.score(X_train1.fillna(0),y_train1),'\\n test score:',RF.score(X_test1.fillna(0),y_test1.fillna(0)),'\\n Random forest  train Brier score:',\n",
    "      brier_score(RF.predict_proba(X_train1.fillna(0)),y_train1),'\\n test Brier score:',brier_score(RF.predict_proba(X_test1.fillna(0)),y_test1),\n",
    "     '\\n AUROC:',roc_auc_score(y_test1, y_roc(RF,X_test1.fillna(0))))\n",
    "\n",
    "GBDT=GradientBoostingClassifier()\n",
    "params_SGD={'n_estimators':[150,100],'min_samples_split':[2,4]}\n",
    "grid_SGD=GridSearchCV(GBDT,param_grid=params_SGD, scoring='neg_brier_score',cv=3)\n",
    "grid_SGD.fit(X_train1.fillna(0),y_train1.fillna(0))\n",
    "fpr_list[2], tpr_list[2], _ = roc_curve(y_test1, y_roc(grid_SGD.best_estimator_,X_test1.fillna(0)))\n",
    "print('SGD best layer size:',grid_SGD.best_params_,'\\n best train score:',\n",
    "      grid_SGD.best_score_,'\\n test score:',grid_SGD.score(X_test1.fillna(0),y_test1.fillna(0)),'\\n SGD  train Brier score:',\n",
    "      brier_score(grid_SGD.predict_proba(X_train1.fillna(0)),y_train1),'\\n test Brier score:',brier_score(grid_SGD.predict_proba(X_test1.fillna(0)),y_test1),\n",
    "     '\\n AUROC:',roc_auc_score(y_test1, y_roc(grid_SGD.best_estimator_,X_test1.fillna(0))))\n",
    "\n",
    "pipe = Sequential()\n",
    "n_cols = X_train1.shape[1]\n",
    "pipe.add(BatchNormalization(input_shape=(n_cols,)))\n",
    "pipe.add(Dense(70, activation= 'linear'))\n",
    "pipe.add(Dropout(0.3))\n",
    "pipe.add(Dense(50, activation= 'relu'))\n",
    "pipe.add(Dropout(0.3))\n",
    "pipe.add(Dense(50, activation= 'relu'))\n",
    "pipe.add(Dropout(0.3))\n",
    "#model.add(Dense(100, activation='relu'))\n",
    "#model.add(Dropout(0.3))\n",
    "#model.add(Dense(100, activation='linear'))\n",
    "pipe.add(BatchNormalization())\n",
    "pipe.add(Dense(2, activation='softmax'))\n",
    "    #model.compile(\n",
    "        #optimizer='Adam',\n",
    "        #loss='mean_squared_error',\n",
    "        #metrics=['accuracy'])\n",
    "early_stopping_monitor = EarlyStopping(patience=50)\n",
    "sgd = keras.optimizers.SGD(lr=.001, decay=2e-4, momentum=0.9, nesterov=True)\n",
    "pipe.compile(loss = 'sparse_categorical_crossentropy',optimizer = 'sgd', metrics=['accuracy'])\n",
    "history=pipe.fit(X_train1.fillna(0).astype('float32'), y_train1, validation_split=0.3, epochs=200, callbacks=[early_stopping_monitor])\n",
    "#history=model.fit(X_train, y_train, validation_split=0.2, epochs=25)\n",
    "score = pipe.evaluate(X_test1.fillna(0).astype('float32'), y_test1, verbose=0)\n",
    "pipe.fit(X_train1.fillna(0).astype('float32'),y_train1.fillna(0))\n",
    "fpr_list[3], tpr_list[3], _ = roc_curve(y_test1, y_roc(pipe,X_test1.fillna(0).astype('float32')))\n",
    "print('MLP train Brier score:',\n",
    "      brier_score(pipe.predict_proba(X_train1.fillna(0).astype('float32')),y_train1),'\\n test Brier score:',brier_score(pipe.predict_proba(X_test1.fillna(0).astype('float32')),y_test1),\n",
    "     '\\n AUROC:',roc_auc_score(y_test1, y_roc(pipe,X_test1.fillna(0).astype('float32'))))\n",
    "\n",
    "\n",
    "XGB=XGBClassifier()\n",
    "params_XGB={    'n_estimators': [50, 100],\n",
    "    'max_depth': [2, 3],\n",
    "    'min_child_weight': [2,4,6],\n",
    "    'subsample': [0.9],\n",
    "    'colsample_bytree': [0.8],'reg_lambda':[1000]}\n",
    "grid_XGB=GridSearchCV(XGB,param_grid=params_XGB, scoring='neg_brier_score',cv=3)\n",
    "grid_XGB.fit(X_train1.fillna(0),y_train1.fillna(0))\n",
    "fpr_list[4], tpr_list[4], _ = roc_curve(y_test1, y_roc(grid_XGB.best_estimator_,X_test1.fillna(0)))\n",
    "print('Xgboost best layer size:',grid_XGB.best_params_,'\\n best train score:',\n",
    "      grid_XGB.best_score_,'\\n test score:',grid_XGB.score(X_test1.fillna(0),y_test1.fillna(0)),'\\n Xgboost train Brier score:',\n",
    "      brier_score(grid_XGB.predict_proba(X_train1.fillna(0)),y_train1),'\\n test Brier score:',brier_score(grid_XGB.predict_proba(X_test1.fillna(0)),y_test1),\n",
    "     '\\n AUROC:',roc_auc_score(y_test1, y_roc(grid_XGB.best_estimator_,X_test1.fillna(0))))\n",
    "\n",
    "\n",
    "\n",
    "colors = cycle(['aqua', 'red', 'green','orange','blue'])\n",
    "labels=['Logistic Regression','Random Forest','Gradient Boosting','MLP','Xgboost']\n",
    "for i, label, color in zip(range(len(fpr_list)), labels, colors):\n",
    "    legend= label + ' (area = {1:0.2f})'''.format(i, auc(fpr_list[i], tpr_list[i]))\n",
    "    plt.plot(fpr_list[i], tpr_list[i], color=color, lw=2,label=legend)\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC curve for different methods on Recidivism_1Year')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('ROC_plot.png',dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "typical-sleep",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17605078595905532"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def aver_prob(prob_lists):\n",
    "    n=len(prob_lists)\n",
    "    return np.sum(np.array(prob_lists),0)/n\n",
    "X_train1,X_test1,y_train1,y_test1=train_test_split(raw_extend.drop(['ID','Recidivism_Within_3years','Recidivism_Arrest_Year1','Recidivism_Arrest_Year2','Recidivism_Arrest_Year3'],axis=1),raw_extend_remove['Recidivism_Arrest_Year2'])\n",
    "\n",
    "X_train1_t,X_test1_t,y_train1_t,y_test1_t=train_test_split(raw_extend_remove.drop(['ID','Recidivism_Within_3years','Recidivism_Arrest_Year1','Recidivism_Arrest_Year2','Recidivism_Arrest_Year3'],axis=1),raw_extend_remove['Recidivism_Arrest_Year2'])\n",
    "\n",
    "brier_score(aver_prob([grid_SGD_t.best_estimator_.predict_proba(X_test1_t.fillna(0)),\n",
    "                       grid_SGD.best_estimator_.predict_proba(X_test1.fillna(0)),\n",
    "                       #grid_XGB_t.best_estimator_.predict_proba(X_test1_t.fillna(0)),\n",
    "                       grid_XGB.best_estimator_.predict_proba(X_test1.fillna(0)),                       \n",
    "                       ]),y_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "mounted-timothy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17137843522146545"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def aver_prob(prob_lists):\n",
    "    n=len(prob_lists)\n",
    "    return np.sum(np.array(prob_lists),0)/n\n",
    "X_train1,X_test1,y_train1,y_test1=train_test_split(raw_extend.drop(['ID','Recidivism_Within_3years','Recidivism_Arrest_Year1','Recidivism_Arrest_Year2','Recidivism_Arrest_Year3'],axis=1),raw_extend_remove['Recidivism_Arrest_Year2'])\n",
    "\n",
    "X_train1_t,X_test1_t,y_train1_t,y_test1_t=train_test_split(raw_extend_remove.drop(['ID','Recidivism_Within_3years','Recidivism_Arrest_Year1','Recidivism_Arrest_Year2','Recidivism_Arrest_Year3'],axis=1),raw_extend_remove['Recidivism_Arrest_Year2'])\n",
    "\n",
    "brier_score(aver_prob([grid_SGD_t.best_estimator_.predict_proba(X_test1_t.fillna(0)),\n",
    "                       grid_SGD.best_estimator_.predict_proba(X_test1.fillna(0)),\n",
    "                       grid_XGB_t.best_estimator_.predict_proba(X_test1_t.fillna(0)),\n",
    "                       grid_XGB.best_estimator_.predict_proba(X_test1.fillna(0)),                       \n",
    "                       ]),y_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "adjustable-buying",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1368499100621035"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def aver_prob(prob_lists):\n",
    "    n=len(prob_lists)\n",
    "    return np.sum(np.array(prob_lists),0)/n\n",
    "X_train1,X_test1,y_train1,y_test1=train_test_split(raw_extend.drop(['ID','Recidivism_Within_3years','Recidivism_Arrest_Year1','Recidivism_Arrest_Year2','Recidivism_Arrest_Year3'],axis=1),raw_extend_remove['Recidivism_Arrest_Year3'])\n",
    "\n",
    "X_train1_t,X_test1_t,y_train1_t,y_test1_t=train_test_split(raw_extend_remove.drop(['ID','Recidivism_Within_3years','Recidivism_Arrest_Year1','Recidivism_Arrest_Year2','Recidivism_Arrest_Year3'],axis=1),raw_extend_remove['Recidivism_Arrest_Year3'])\n",
    "\n",
    "brier_score(aver_prob([#grid_SGD_t.best_estimator_.predict_proba(X_test1_t.fillna(0)),\n",
    "                       grid_SGD.best_estimator_.predict_proba(X_test1.fillna(0)),\n",
    "                       #grid_XGB_t.best_estimator_.predict_proba(X_test1_t.fillna(0)),\n",
    "                       grid_XGB.best_estimator_.predict_proba(X_test1.fillna(0)),                       \n",
    "                       ]),y_test1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thorough-kitchen",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "roman-folder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Race</th>\n",
       "      <th>Age_at_Release</th>\n",
       "      <th>Residence_PUMA</th>\n",
       "      <th>Gang_Affiliated</th>\n",
       "      <th>Supervision_Risk_Score_First</th>\n",
       "      <th>Supervision_Level_First</th>\n",
       "      <th>Education_Level</th>\n",
       "      <th>Dependents</th>\n",
       "      <th>...</th>\n",
       "      <th>Total\\nofficers</th>\n",
       "      <th>Total\\ncivilians</th>\n",
       "      <th>Murder</th>\n",
       "      <th>Rape</th>\n",
       "      <th>Robbery</th>\n",
       "      <th>Assault</th>\n",
       "      <th>Burglary</th>\n",
       "      <th>Larceny</th>\n",
       "      <th>Vehicle Theft</th>\n",
       "      <th>PUMA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>M</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>38</td>\n",
       "      <td>16</td>\n",
       "      <td>False</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Standard</td>\n",
       "      <td>High School Diploma</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>60.230769</td>\n",
       "      <td>18.384615</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>4.615385</td>\n",
       "      <td>5.769231</td>\n",
       "      <td>68.769231</td>\n",
       "      <td>197.769231</td>\n",
       "      <td>587.615385</td>\n",
       "      <td>43.538462</td>\n",
       "      <td>3146.153846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>M</td>\n",
       "      <td>BLACK</td>\n",
       "      <td>38</td>\n",
       "      <td>15</td>\n",
       "      <td>False</td>\n",
       "      <td>5.0</td>\n",
       "      <td>High</td>\n",
       "      <td>High School Diploma</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>111.750000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>36.750000</td>\n",
       "      <td>158.250000</td>\n",
       "      <td>541.250000</td>\n",
       "      <td>1707.750000</td>\n",
       "      <td>150.750000</td>\n",
       "      <td>3000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>M</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>33</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Standard</td>\n",
       "      <td>Less than HS diploma</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>55.700000</td>\n",
       "      <td>15.700000</td>\n",
       "      <td>2.100000</td>\n",
       "      <td>9.100000</td>\n",
       "      <td>37.900000</td>\n",
       "      <td>72.900000</td>\n",
       "      <td>376.300000</td>\n",
       "      <td>867.800000</td>\n",
       "      <td>82.400000</td>\n",
       "      <td>1780.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>M</td>\n",
       "      <td>BLACK</td>\n",
       "      <td>33</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Standard</td>\n",
       "      <td>Less than HS diploma</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>15.750000</td>\n",
       "      <td>16.375000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.125000</td>\n",
       "      <td>7.250000</td>\n",
       "      <td>40.625000</td>\n",
       "      <td>135.875000</td>\n",
       "      <td>357.125000</td>\n",
       "      <td>13.625000</td>\n",
       "      <td>1275.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23</td>\n",
       "      <td>F</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>48</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>High School Diploma</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>55.700000</td>\n",
       "      <td>15.700000</td>\n",
       "      <td>2.100000</td>\n",
       "      <td>9.100000</td>\n",
       "      <td>37.900000</td>\n",
       "      <td>72.900000</td>\n",
       "      <td>376.300000</td>\n",
       "      <td>867.800000</td>\n",
       "      <td>82.400000</td>\n",
       "      <td>1780.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4141</th>\n",
       "      <td>26728</td>\n",
       "      <td>M</td>\n",
       "      <td>BLACK</td>\n",
       "      <td>38</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Standard</td>\n",
       "      <td>At least some college</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>233.000000</td>\n",
       "      <td>592.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>2002.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4142</th>\n",
       "      <td>26732</td>\n",
       "      <td>M</td>\n",
       "      <td>BLACK</td>\n",
       "      <td>38</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Standard</td>\n",
       "      <td>High School Diploma</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>55.700000</td>\n",
       "      <td>15.700000</td>\n",
       "      <td>2.100000</td>\n",
       "      <td>9.100000</td>\n",
       "      <td>37.900000</td>\n",
       "      <td>72.900000</td>\n",
       "      <td>376.300000</td>\n",
       "      <td>867.800000</td>\n",
       "      <td>82.400000</td>\n",
       "      <td>1780.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4143</th>\n",
       "      <td>26744</td>\n",
       "      <td>M</td>\n",
       "      <td>BLACK</td>\n",
       "      <td>48</td>\n",
       "      <td>20</td>\n",
       "      <td>False</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Standard</td>\n",
       "      <td>Less than HS diploma</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>40.500000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>354.500000</td>\n",
       "      <td>984.000000</td>\n",
       "      <td>64.500000</td>\n",
       "      <td>1552.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4144</th>\n",
       "      <td>26746</td>\n",
       "      <td>M</td>\n",
       "      <td>BLACK</td>\n",
       "      <td>43</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Standard</td>\n",
       "      <td>At least some college</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>233.000000</td>\n",
       "      <td>592.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>2002.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4145</th>\n",
       "      <td>26755</td>\n",
       "      <td>M</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>33</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Standard</td>\n",
       "      <td>At least some college</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>55.700000</td>\n",
       "      <td>15.700000</td>\n",
       "      <td>2.100000</td>\n",
       "      <td>9.100000</td>\n",
       "      <td>37.900000</td>\n",
       "      <td>72.900000</td>\n",
       "      <td>376.300000</td>\n",
       "      <td>867.800000</td>\n",
       "      <td>82.400000</td>\n",
       "      <td>1780.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4146 rows × 202 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID Gender   Race  Age_at_Release  Residence_PUMA Gang_Affiliated  \\\n",
       "0         6      M  WHITE              38              16           False   \n",
       "1         8      M  BLACK              38              15           False   \n",
       "2        15      M  WHITE              33               4           False   \n",
       "3        16      M  BLACK              33               2           False   \n",
       "4        23      F  WHITE              48               4             NaN   \n",
       "...     ...    ...    ...             ...             ...             ...   \n",
       "4141  26728      M  BLACK              38               5           False   \n",
       "4142  26732      M  BLACK              38               4           False   \n",
       "4143  26744      M  BLACK              48              20           False   \n",
       "4144  26746      M  BLACK              43               5           False   \n",
       "4145  26755      M  WHITE              33               4           False   \n",
       "\n",
       "      Supervision_Risk_Score_First Supervision_Level_First  \\\n",
       "0                              5.0                Standard   \n",
       "1                              5.0                    High   \n",
       "2                              7.0                Standard   \n",
       "3                              4.0                Standard   \n",
       "4                              4.0                     NaN   \n",
       "...                            ...                     ...   \n",
       "4141                           5.0                Standard   \n",
       "4142                           5.0                Standard   \n",
       "4143                           7.0                Standard   \n",
       "4144                           5.0                Standard   \n",
       "4145                           5.0                Standard   \n",
       "\n",
       "            Education_Level  Dependents  ... Total\\nofficers Total\\ncivilians  \\\n",
       "0       High School Diploma           0  ...       60.230769        18.384615   \n",
       "1       High School Diploma           3  ...      111.750000        24.000000   \n",
       "2      Less than HS diploma           1  ...       55.700000        15.700000   \n",
       "3      Less than HS diploma           3  ...       15.750000        16.375000   \n",
       "4       High School Diploma           0  ...       55.700000        15.700000   \n",
       "...                     ...         ...  ...             ...              ...   \n",
       "4141  At least some college           0  ...       31.000000         5.000000   \n",
       "4142    High School Diploma           0  ...       55.700000        15.700000   \n",
       "4143   Less than HS diploma           0  ...       94.000000        40.500000   \n",
       "4144  At least some college           3  ...       31.000000         5.000000   \n",
       "4145  At least some college           3  ...       55.700000        15.700000   \n",
       "\n",
       "        Murder       Rape    Robbery     Assault    Burglary      Larceny  \\\n",
       "0     0.384615   4.615385   5.769231   68.769231  197.769231   587.615385   \n",
       "1     1.500000  15.000000  36.750000  158.250000  541.250000  1707.750000   \n",
       "2     2.100000   9.100000  37.900000   72.900000  376.300000   867.800000   \n",
       "3     1.000000   2.125000   7.250000   40.625000  135.875000   357.125000   \n",
       "4     2.100000   9.100000  37.900000   72.900000  376.300000   867.800000   \n",
       "...        ...        ...        ...         ...         ...          ...   \n",
       "4141  1.000000   3.000000   9.000000  118.000000  233.000000   592.000000   \n",
       "4142  2.100000   9.100000  37.900000   72.900000  376.300000   867.800000   \n",
       "4143  2.500000  11.000000  25.000000   83.000000  354.500000   984.000000   \n",
       "4144  1.000000   3.000000   9.000000  118.000000  233.000000   592.000000   \n",
       "4145  2.100000   9.100000  37.900000   72.900000  376.300000   867.800000   \n",
       "\n",
       "      Vehicle Theft         PUMA  \n",
       "0         43.538462  3146.153846  \n",
       "1        150.750000  3000.000000  \n",
       "2         82.400000  1780.000000  \n",
       "3         13.625000  1275.000000  \n",
       "4         82.400000  1780.000000  \n",
       "...             ...          ...  \n",
       "4141      13.000000  2002.000000  \n",
       "4142      82.400000  1780.000000  \n",
       "4143      64.500000  1552.000000  \n",
       "4144      13.000000  2002.000000  \n",
       "4145      82.400000  1780.000000  \n",
       "\n",
       "[4146 rows x 202 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_test=pd.read_csv(r'.\\data\\NIJ_s_Recidivism_Challenge_Test_Dataset3.csv')\n",
    "raw_test=pd.merge(raw_test,PUMA_new,left_on='Residence_PUMA',right_on='Code',how='left')\n",
    "# Change the dtype of the feature from object to intagar\n",
    "raw_test['TEN']=raw_test['TEN'].astype('category')\n",
    "raw_test['TEN']=raw_test['TEN'].cat.codes\n",
    "raw_test['Residence_PUMA']=raw_test['Residence_PUMA'].astype('category')\n",
    "raw_test['Residence_PUMA']=raw_test['Residence_PUMA'].cat.codes\n",
    "raw_test['Age_at_Release']=raw_test['Age_at_Release'].apply(lambda x: int(x[:2]))\n",
    "raw_test['Dependents']=raw_test['Dependents'].apply(lambda x: int(x[:1]))\n",
    "raw_test['Prior_Arrest_Episodes_Felony']=raw_test['Prior_Arrest_Episodes_Felony'].apply(lambda x: int(x[:2]))\n",
    "raw_test['Prior_Arrest_Episodes_Drug']=raw_test['Prior_Arrest_Episodes_Drug'].apply(lambda x: int(x[:2]))\n",
    "raw_test['Prior_Arrest_Episodes_Misd']=raw_test['Prior_Arrest_Episodes_Misd'].apply(lambda x: int(x[:2]))\n",
    "raw_test['Prior_Arrest_Episodes_Violent']=raw_test['Prior_Arrest_Episodes_Violent'].apply(lambda x: int(x[:2]))\n",
    "raw_test['Prior_Arrest_Episodes_Property']=raw_test['Prior_Arrest_Episodes_Property'].apply(lambda x: int(x[:2]))\n",
    "raw_test['Prior_Arrest_Episodes_PPViolationCharges']=raw_test['Prior_Arrest_Episodes_PPViolationCharges'].apply(lambda x: int(x[:2]))\n",
    "raw_test['Prior_Conviction_Episodes_Felony']=raw_test['Prior_Conviction_Episodes_Felony'].apply(lambda x: int(x[:1]))\n",
    "raw_test['Prior_Conviction_Episodes_Misd']=raw_test['Prior_Conviction_Episodes_Misd'].apply(lambda x: int(x[:1]))\n",
    "raw_test['Prior_Conviction_Episodes_Prop']=raw_test['Prior_Conviction_Episodes_Prop'].apply(lambda x: int(x[:1]))\n",
    "raw_test['Prior_Conviction_Episodes_Drug']=raw_test['Prior_Conviction_Episodes_Drug'].apply(lambda x: int(x[:1]))\n",
    "#raw_test['Delinquency_Reports']=raw_test['Delinquency_Reports'].apply(lambda x: int(x[:1]))\n",
    "#raw_test['Program_Attendances']=raw_test['Program_Attendances'].apply(lambda x: int(x[:2]))\n",
    "#raw_test['Program_UnexcusedAbsences']=raw_test['Program_UnexcusedAbsences'].apply(lambda x: int(x[:1]))\n",
    "#raw_test['Residence_Changes']=raw_test['Residence_Changes'].apply(lambda x: int(x[:1]))\n",
    "\n",
    "\n",
    "raw_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "difficult-quality",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\myjr\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\myjr\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "#raw_extend_test=pd.merge(raw_test,PUMA_new,left_on='Residence_PUMA',right_on='Code',how='outer')\n",
    "raw_extend_test=raw_test.copy()\n",
    "# Change the dtype of the feature from object to intagar\n",
    "\n",
    "# scale the columns which are not bool or category\n",
    "scaler = StandardScaler()\n",
    "scaling_set=[]\n",
    "for column in raw_extend_test.columns:\n",
    "    if raw_extend_test[column].dtype == object:\n",
    "        raw_extend_test[column]=raw_extend_test[column].astype('category')\n",
    "        raw_extend_test[column]=raw_extend_test[column].cat.codes\n",
    "    elif raw_extend_test[column].dtype in ['int64','float32','float64'] :\n",
    "        scaling_set+=[column]\n",
    "raw_extend_test[scaling_set]=scaler.fit_transform(raw_extend_test[scaling_set].values)\n",
    "raw_extend_test\n",
    "#raw_extend_test=raw_extend_test.drop(index=raw_extend_test[raw_extend_test.Supervision_Risk_Score_First.isnull()].index)\n",
    "#raw_extend_test=raw_extend_test.drop(index=set(raw_extend_test[raw_extend_test.Supervision_Level_First.isnull()].index) & set(raw_extend_test[raw_extend_test.Prison_Offense .isnull()].index))\n",
    "#raw_extend_test=raw_extend_test.reset_index(drop=True)\n",
    "# impute missing value 'Supervision_Level_First' and 'Prison_Offense' with relative feature\n",
    "for missing_column in ['Supervision_Level_First','Prison_Offense']:\n",
    "    test_index=raw_extend_test[raw_extend_test[missing_column]==-1].index\n",
    "    train_index=raw_extend_test[raw_extend_test[missing_column]!=-1].index\n",
    "    X=raw_extend_test.drop(columns=[missing_column])\n",
    "    y=raw_extend_test[missing_column]\n",
    "    logreg = LogisticRegression()\n",
    "    logreg.fit(X.iloc[train_index,:].fillna(0),y[train_index])\n",
    "    raw_extend_test.loc[test_index,missing_column]=logreg.predict(X.iloc[test_index,:].fillna(0))\n",
    "    raw_extend_test[missing_column]=raw_extend_test[missing_column].astype('category')\n",
    "    raw_extend_test[missing_column]=raw_extend_test[missing_column].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "saved-times",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Race</th>\n",
       "      <th>Age_at_Release</th>\n",
       "      <th>Gang_Affiliated</th>\n",
       "      <th>Supervision_Risk_Score_First</th>\n",
       "      <th>Supervision_Level_First</th>\n",
       "      <th>Education_Level</th>\n",
       "      <th>Dependents</th>\n",
       "      <th>Prison_Offense</th>\n",
       "      <th>...</th>\n",
       "      <th>FTELP</th>\n",
       "      <th>FVACSP</th>\n",
       "      <th>FVEHP</th>\n",
       "      <th>Total law\\nenforcement\\nemployees</th>\n",
       "      <th>Total\\nofficers</th>\n",
       "      <th>Total\\ncivilians</th>\n",
       "      <th>Burglary</th>\n",
       "      <th>Larceny</th>\n",
       "      <th>Vehicle Theft</th>\n",
       "      <th>PUMA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.682777</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.507782</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.385252</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.219910</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.170101</td>\n",
       "      <td>-0.440234</td>\n",
       "      <td>-0.754932</td>\n",
       "      <td>-0.442168</td>\n",
       "      <td>-0.389742</td>\n",
       "      <td>-0.512988</td>\n",
       "      <td>-0.371568</td>\n",
       "      <td>-0.350737</td>\n",
       "      <td>-0.293308</td>\n",
       "      <td>0.556955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.682520</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.507782</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.385252</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.234595</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.582757</td>\n",
       "      <td>-1.400531</td>\n",
       "      <td>-0.651192</td>\n",
       "      <td>-0.216168</td>\n",
       "      <td>-0.123506</td>\n",
       "      <td>-0.433774</td>\n",
       "      <td>-0.210058</td>\n",
       "      <td>-0.173294</td>\n",
       "      <td>-0.213081</td>\n",
       "      <td>0.443348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.681620</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.033508</td>\n",
       "      <td>0</td>\n",
       "      <td>0.459367</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.401742</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000267</td>\n",
       "      <td>-0.262700</td>\n",
       "      <td>0.129668</td>\n",
       "      <td>-0.470708</td>\n",
       "      <td>-0.413155</td>\n",
       "      <td>-0.550859</td>\n",
       "      <td>-0.287621</td>\n",
       "      <td>-0.306352</td>\n",
       "      <td>-0.264228</td>\n",
       "      <td>-0.504963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.681492</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.033508</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.807562</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.234595</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.090767</td>\n",
       "      <td>1.752553</td>\n",
       "      <td>0.835333</td>\n",
       "      <td>-0.626063</td>\n",
       "      <td>-0.619605</td>\n",
       "      <td>-0.541337</td>\n",
       "      <td>-0.400672</td>\n",
       "      <td>-0.387249</td>\n",
       "      <td>-0.315693</td>\n",
       "      <td>-0.897502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.680592</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.590364</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.807562</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.219910</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000267</td>\n",
       "      <td>-0.262700</td>\n",
       "      <td>0.129668</td>\n",
       "      <td>-0.470708</td>\n",
       "      <td>-0.413155</td>\n",
       "      <td>-0.550859</td>\n",
       "      <td>-0.287621</td>\n",
       "      <td>-0.306352</td>\n",
       "      <td>-0.264228</td>\n",
       "      <td>-0.504963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5455</th>\n",
       "      <td>1.752851</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.590364</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.385252</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.234595</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.650412</td>\n",
       "      <td>-1.053449</td>\n",
       "      <td>-0.396083</td>\n",
       "      <td>0.383428</td>\n",
       "      <td>0.547864</td>\n",
       "      <td>-0.128131</td>\n",
       "      <td>-0.246853</td>\n",
       "      <td>-0.195590</td>\n",
       "      <td>-0.238336</td>\n",
       "      <td>0.702450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5456</th>\n",
       "      <td>1.753365</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.590364</td>\n",
       "      <td>0</td>\n",
       "      <td>0.459367</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.219910</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.862282</td>\n",
       "      <td>0.375927</td>\n",
       "      <td>0.316156</td>\n",
       "      <td>-0.221113</td>\n",
       "      <td>-0.215232</td>\n",
       "      <td>-0.201015</td>\n",
       "      <td>-0.297871</td>\n",
       "      <td>-0.287945</td>\n",
       "      <td>-0.277623</td>\n",
       "      <td>-0.682188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5457</th>\n",
       "      <td>1.753622</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.049073</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.385252</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.234595</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014744</td>\n",
       "      <td>-0.895153</td>\n",
       "      <td>0.365913</td>\n",
       "      <td>-0.610735</td>\n",
       "      <td>-0.540798</td>\n",
       "      <td>-0.701799</td>\n",
       "      <td>-0.355002</td>\n",
       "      <td>-0.350042</td>\n",
       "      <td>-0.316161</td>\n",
       "      <td>-0.332401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5458</th>\n",
       "      <td>1.754393</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.574799</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.385252</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.234595</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.186956</td>\n",
       "      <td>2.589765</td>\n",
       "      <td>-1.099334</td>\n",
       "      <td>-0.538927</td>\n",
       "      <td>-0.545170</td>\n",
       "      <td>-0.433774</td>\n",
       "      <td>-0.373124</td>\n",
       "      <td>-0.352102</td>\n",
       "      <td>-0.303900</td>\n",
       "      <td>-1.625477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5459</th>\n",
       "      <td>1.754779</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.033508</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.385252</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.234595</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000267</td>\n",
       "      <td>-0.262700</td>\n",
       "      <td>0.129668</td>\n",
       "      <td>-0.470708</td>\n",
       "      <td>-0.413155</td>\n",
       "      <td>-0.550859</td>\n",
       "      <td>-0.287621</td>\n",
       "      <td>-0.306352</td>\n",
       "      <td>-0.264228</td>\n",
       "      <td>-0.504963</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5460 rows × 112 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID  Gender  Race  Age_at_Release  Gang_Affiliated  \\\n",
       "0    -1.682777       1     1        0.507782                0   \n",
       "1    -1.682520       1     0        0.507782                0   \n",
       "2    -1.681620       1     1       -0.033508                0   \n",
       "3    -1.681492       1     0       -0.033508                0   \n",
       "4    -1.680592       0     1        1.590364               -1   \n",
       "...        ...     ...   ...             ...              ...   \n",
       "5455  1.752851       1     1        1.590364                0   \n",
       "5456  1.753365       1     0        1.590364                0   \n",
       "5457  1.753622       1     0        1.049073                0   \n",
       "5458  1.754393       1     0       -0.574799                0   \n",
       "5459  1.754779       1     1       -0.033508                0   \n",
       "\n",
       "      Supervision_Risk_Score_First  Supervision_Level_First  Education_Level  \\\n",
       "0                        -0.385252                        2                1   \n",
       "1                        -0.385252                        0                1   \n",
       "2                         0.459367                        2                2   \n",
       "3                        -0.807562                        2                2   \n",
       "4                        -0.807562                        2                1   \n",
       "...                            ...                      ...              ...   \n",
       "5455                     -0.385252                        2                0   \n",
       "5456                      0.459367                        2                2   \n",
       "5457                     -0.385252                        2                0   \n",
       "5458                     -0.385252                        2                0   \n",
       "5459                     -0.385252                        2                0   \n",
       "\n",
       "      Dependents  Prison_Offense  ...     FTELP    FVACSP     FVEHP  \\\n",
       "0      -1.219910               2  ...  0.170101 -0.440234 -0.754932   \n",
       "1       1.234595               0  ...  3.582757 -1.400531 -0.651192   \n",
       "2      -0.401742               3  ...  0.000267 -0.262700  0.129668   \n",
       "3       1.234595               0  ...  0.090767  1.752553  0.835333   \n",
       "4      -1.219910               3  ...  0.000267 -0.262700  0.129668   \n",
       "...          ...             ...  ...       ...       ...       ...   \n",
       "5455    1.234595               0  ... -0.650412 -1.053449 -0.396083   \n",
       "5456   -1.219910               2  ... -0.862282  0.375927  0.316156   \n",
       "5457    1.234595               3  ... -0.014744 -0.895153  0.365913   \n",
       "5458    1.234595               2  ... -0.186956  2.589765 -1.099334   \n",
       "5459    1.234595               2  ...  0.000267 -0.262700  0.129668   \n",
       "\n",
       "      Total law\\nenforcement\\nemployees  Total\\nofficers  Total\\ncivilians  \\\n",
       "0                             -0.442168        -0.389742         -0.512988   \n",
       "1                             -0.216168        -0.123506         -0.433774   \n",
       "2                             -0.470708        -0.413155         -0.550859   \n",
       "3                             -0.626063        -0.619605         -0.541337   \n",
       "4                             -0.470708        -0.413155         -0.550859   \n",
       "...                                 ...              ...               ...   \n",
       "5455                           0.383428         0.547864         -0.128131   \n",
       "5456                          -0.221113        -0.215232         -0.201015   \n",
       "5457                          -0.610735        -0.540798         -0.701799   \n",
       "5458                          -0.538927        -0.545170         -0.433774   \n",
       "5459                          -0.470708        -0.413155         -0.550859   \n",
       "\n",
       "      Burglary   Larceny  Vehicle Theft      PUMA  \n",
       "0    -0.371568 -0.350737      -0.293308  0.556955  \n",
       "1    -0.210058 -0.173294      -0.213081  0.443348  \n",
       "2    -0.287621 -0.306352      -0.264228 -0.504963  \n",
       "3    -0.400672 -0.387249      -0.315693 -0.897502  \n",
       "4    -0.287621 -0.306352      -0.264228 -0.504963  \n",
       "...        ...       ...            ...       ...  \n",
       "5455 -0.246853 -0.195590      -0.238336  0.702450  \n",
       "5456 -0.297871 -0.287945      -0.277623 -0.682188  \n",
       "5457 -0.355002 -0.350042      -0.316161 -0.332401  \n",
       "5458 -0.373124 -0.352102      -0.303900 -1.625477  \n",
       "5459 -0.287621 -0.306352      -0.264228 -0.504963  \n",
       "\n",
       "[5460 rows x 112 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_extend_test_t=raw_extend_test.drop(list(GBDT_importance[GBDT_importance.importance==0].Feature),axis=1)\n",
    "raw_extend_test_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "handled-breakdown",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def aver_prob(prob_lists):\n",
    "    n=len(prob_lists)\n",
    "    return np.sum(np.array(prob_lists),0)/n\n",
    "prob_list=aver_prob([[x[1] for x in grid_SGD.best_estimator_.predict_proba(raw_extend_test.drop(['ID'],axis=1).fillna(0))],\n",
    "          [x[1] for x in grid_XGB.best_estimator_.predict_proba(raw_extend_test.drop(['ID'],axis=1))],\n",
    "          ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "accompanied-georgia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Race</th>\n",
       "      <th>Age_at_Release</th>\n",
       "      <th>Residence_PUMA</th>\n",
       "      <th>Gang_Affiliated</th>\n",
       "      <th>Supervision_Risk_Score_First</th>\n",
       "      <th>Supervision_Level_First</th>\n",
       "      <th>Education_Level</th>\n",
       "      <th>Dependents</th>\n",
       "      <th>...</th>\n",
       "      <th>Total\\ncivilians</th>\n",
       "      <th>Murder</th>\n",
       "      <th>Rape</th>\n",
       "      <th>Robbery</th>\n",
       "      <th>Assault</th>\n",
       "      <th>Burglary</th>\n",
       "      <th>Larceny</th>\n",
       "      <th>Vehicle Theft</th>\n",
       "      <th>PUMA</th>\n",
       "      <th>Probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.456600</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.331166</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.212898</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.509441</td>\n",
       "      <td>-0.333478</td>\n",
       "      <td>-0.415806</td>\n",
       "      <td>-0.294742</td>\n",
       "      <td>-0.308652</td>\n",
       "      <td>-0.370152</td>\n",
       "      <td>-0.349091</td>\n",
       "      <td>-0.291323</td>\n",
       "      <td>0.556165</td>\n",
       "      <td>0.072272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.456600</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.331166</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.219940</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.429729</td>\n",
       "      <td>-0.290283</td>\n",
       "      <td>-0.189598</td>\n",
       "      <td>-0.250360</td>\n",
       "      <td>-0.190399</td>\n",
       "      <td>-0.206475</td>\n",
       "      <td>-0.169250</td>\n",
       "      <td>-0.209907</td>\n",
       "      <td>0.442380</td>\n",
       "      <td>0.246831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.082888</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.516934</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.401952</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.547550</td>\n",
       "      <td>-0.267047</td>\n",
       "      <td>-0.318117</td>\n",
       "      <td>-0.248712</td>\n",
       "      <td>-0.303193</td>\n",
       "      <td>-0.285078</td>\n",
       "      <td>-0.304106</td>\n",
       "      <td>-0.261812</td>\n",
       "      <td>-0.507424</td>\n",
       "      <td>0.248155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.082888</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.755215</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.219940</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.537968</td>\n",
       "      <td>-0.309646</td>\n",
       "      <td>-0.470054</td>\n",
       "      <td>-0.292621</td>\n",
       "      <td>-0.345847</td>\n",
       "      <td>-0.399646</td>\n",
       "      <td>-0.386097</td>\n",
       "      <td>-0.314039</td>\n",
       "      <td>-0.900581</td>\n",
       "      <td>0.254387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.535575</td>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.755215</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.212898</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.547550</td>\n",
       "      <td>-0.267047</td>\n",
       "      <td>-0.318117</td>\n",
       "      <td>-0.248712</td>\n",
       "      <td>-0.303193</td>\n",
       "      <td>-0.285078</td>\n",
       "      <td>-0.304106</td>\n",
       "      <td>-0.261812</td>\n",
       "      <td>-0.507424</td>\n",
       "      <td>0.121947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4141</th>\n",
       "      <td>26728</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.456600</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.331166</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.212898</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.699439</td>\n",
       "      <td>-0.309646</td>\n",
       "      <td>-0.450993</td>\n",
       "      <td>-0.290114</td>\n",
       "      <td>-0.243591</td>\n",
       "      <td>-0.353363</td>\n",
       "      <td>-0.348387</td>\n",
       "      <td>-0.314514</td>\n",
       "      <td>-0.334591</td>\n",
       "      <td>0.055274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4142</th>\n",
       "      <td>26732</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.456600</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.331166</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.212898</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.547550</td>\n",
       "      <td>-0.267047</td>\n",
       "      <td>-0.318117</td>\n",
       "      <td>-0.248712</td>\n",
       "      <td>-0.303193</td>\n",
       "      <td>-0.285078</td>\n",
       "      <td>-0.304106</td>\n",
       "      <td>-0.261812</td>\n",
       "      <td>-0.507424</td>\n",
       "      <td>0.071139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4143</th>\n",
       "      <td>26744</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.535575</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0.516934</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.212898</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.195507</td>\n",
       "      <td>-0.251556</td>\n",
       "      <td>-0.276730</td>\n",
       "      <td>-0.267192</td>\n",
       "      <td>-0.289846</td>\n",
       "      <td>-0.295466</td>\n",
       "      <td>-0.285450</td>\n",
       "      <td>-0.275405</td>\n",
       "      <td>-0.684929</td>\n",
       "      <td>0.077549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4144</th>\n",
       "      <td>26746</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.996087</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.331166</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.219940</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.699439</td>\n",
       "      <td>-0.309646</td>\n",
       "      <td>-0.450993</td>\n",
       "      <td>-0.290114</td>\n",
       "      <td>-0.243591</td>\n",
       "      <td>-0.353363</td>\n",
       "      <td>-0.348387</td>\n",
       "      <td>-0.314514</td>\n",
       "      <td>-0.334591</td>\n",
       "      <td>0.141817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4145</th>\n",
       "      <td>26755</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.082888</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.331166</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.219940</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.547550</td>\n",
       "      <td>-0.267047</td>\n",
       "      <td>-0.318117</td>\n",
       "      <td>-0.248712</td>\n",
       "      <td>-0.303193</td>\n",
       "      <td>-0.285078</td>\n",
       "      <td>-0.304106</td>\n",
       "      <td>-0.261812</td>\n",
       "      <td>-0.507424</td>\n",
       "      <td>0.057767</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4146 rows × 203 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID  Gender  Race  Age_at_Release  Residence_PUMA  Gang_Affiliated  \\\n",
       "0         6       1     1        0.456600              16                0   \n",
       "1         8       1     0        0.456600              15                0   \n",
       "2        15       1     1       -0.082888               4                0   \n",
       "3        16       1     0       -0.082888               2                0   \n",
       "4        23       0     1        1.535575               4               -1   \n",
       "...     ...     ...   ...             ...             ...              ...   \n",
       "4141  26728       1     0        0.456600               5                0   \n",
       "4142  26732       1     0        0.456600               4                0   \n",
       "4143  26744       1     0        1.535575              20                0   \n",
       "4144  26746       1     0        0.996087               5                0   \n",
       "4145  26755       1     1       -0.082888               4                0   \n",
       "\n",
       "      Supervision_Risk_Score_First  Supervision_Level_First  Education_Level  \\\n",
       "0                        -0.331166                        2                1   \n",
       "1                        -0.331166                        0                1   \n",
       "2                         0.516934                        2                2   \n",
       "3                        -0.755215                        2                2   \n",
       "4                        -0.755215                        2                1   \n",
       "...                            ...                      ...              ...   \n",
       "4141                     -0.331166                        2                0   \n",
       "4142                     -0.331166                        2                1   \n",
       "4143                      0.516934                        2                2   \n",
       "4144                     -0.331166                        2                0   \n",
       "4145                     -0.331166                        2                0   \n",
       "\n",
       "      Dependents  ...  Total\\ncivilians    Murder      Rape   Robbery  \\\n",
       "0      -1.212898  ...         -0.509441 -0.333478 -0.415806 -0.294742   \n",
       "1       1.219940  ...         -0.429729 -0.290283 -0.189598 -0.250360   \n",
       "2      -0.401952  ...         -0.547550 -0.267047 -0.318117 -0.248712   \n",
       "3       1.219940  ...         -0.537968 -0.309646 -0.470054 -0.292621   \n",
       "4      -1.212898  ...         -0.547550 -0.267047 -0.318117 -0.248712   \n",
       "...          ...  ...               ...       ...       ...       ...   \n",
       "4141   -1.212898  ...         -0.699439 -0.309646 -0.450993 -0.290114   \n",
       "4142   -1.212898  ...         -0.547550 -0.267047 -0.318117 -0.248712   \n",
       "4143   -1.212898  ...         -0.195507 -0.251556 -0.276730 -0.267192   \n",
       "4144    1.219940  ...         -0.699439 -0.309646 -0.450993 -0.290114   \n",
       "4145    1.219940  ...         -0.547550 -0.267047 -0.318117 -0.248712   \n",
       "\n",
       "       Assault  Burglary   Larceny  Vehicle Theft      PUMA  Probability  \n",
       "0    -0.308652 -0.370152 -0.349091      -0.291323  0.556165     0.072272  \n",
       "1    -0.190399 -0.206475 -0.169250      -0.209907  0.442380     0.246831  \n",
       "2    -0.303193 -0.285078 -0.304106      -0.261812 -0.507424     0.248155  \n",
       "3    -0.345847 -0.399646 -0.386097      -0.314039 -0.900581     0.254387  \n",
       "4    -0.303193 -0.285078 -0.304106      -0.261812 -0.507424     0.121947  \n",
       "...        ...       ...       ...            ...       ...          ...  \n",
       "4141 -0.243591 -0.353363 -0.348387      -0.314514 -0.334591     0.055274  \n",
       "4142 -0.303193 -0.285078 -0.304106      -0.261812 -0.507424     0.071139  \n",
       "4143 -0.289846 -0.295466 -0.285450      -0.275405 -0.684929     0.077549  \n",
       "4144 -0.243591 -0.353363 -0.348387      -0.314514 -0.334591     0.141817  \n",
       "4145 -0.303193 -0.285078 -0.304106      -0.261812 -0.507424     0.057767  \n",
       "\n",
       "[4146 rows x 203 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_extend_test['Probability']=prob_list\n",
    "raw_extend_test['ID']=raw_test.ID\n",
    "raw_extend_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aggressive-hello",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>0.072272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>0.246831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>0.248155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>0.254387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23</td>\n",
       "      <td>0.121947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4141</th>\n",
       "      <td>26728</td>\n",
       "      <td>0.055274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4142</th>\n",
       "      <td>26732</td>\n",
       "      <td>0.071139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4143</th>\n",
       "      <td>26744</td>\n",
       "      <td>0.077549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4144</th>\n",
       "      <td>26746</td>\n",
       "      <td>0.141817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4145</th>\n",
       "      <td>26755</td>\n",
       "      <td>0.057767</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4146 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID  Probability\n",
       "0         6     0.072272\n",
       "1         8     0.246831\n",
       "2        15     0.248155\n",
       "3        16     0.254387\n",
       "4        23     0.121947\n",
       "...     ...          ...\n",
       "4141  26728     0.055274\n",
       "4142  26732     0.071139\n",
       "4143  26744     0.077549\n",
       "4144  26746     0.141817\n",
       "4145  26755     0.057767\n",
       "\n",
       "[4146 rows x 2 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Final_out=raw_extend_test[['ID','Probability']]\n",
    "Final_out.to_csv(r'Recidivism_year3.csv',index=False)\n",
    "Final_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "grand-mobility",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>M</td>\n",
       "      <td>0.055036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>M</td>\n",
       "      <td>0.360398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>M</td>\n",
       "      <td>0.308977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>M</td>\n",
       "      <td>0.278147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27</td>\n",
       "      <td>M</td>\n",
       "      <td>0.266731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5455</th>\n",
       "      <td>26543</td>\n",
       "      <td>F</td>\n",
       "      <td>0.060433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5456</th>\n",
       "      <td>26604</td>\n",
       "      <td>F</td>\n",
       "      <td>0.222661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5457</th>\n",
       "      <td>26617</td>\n",
       "      <td>F</td>\n",
       "      <td>0.091506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5458</th>\n",
       "      <td>26622</td>\n",
       "      <td>F</td>\n",
       "      <td>0.116771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5459</th>\n",
       "      <td>26701</td>\n",
       "      <td>F</td>\n",
       "      <td>0.257236</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5460 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID Gender  Probability\n",
       "0         6      M     0.055036\n",
       "1         8      M     0.360398\n",
       "2        15      M     0.308977\n",
       "3        16      M     0.278147\n",
       "4        27      M     0.266731\n",
       "...     ...    ...          ...\n",
       "5455  26543      F     0.060433\n",
       "5456  26604      F     0.222661\n",
       "5457  26617      F     0.091506\n",
       "5458  26622      F     0.116771\n",
       "5459  26701      F     0.257236\n",
       "\n",
       "[5460 rows x 3 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Male\n",
    "Output_male=raw_extend_test[raw_extend_test['Gender']==1][['ID','Gender','Probability']]\n",
    "Output_male.ID=Output_male.ID.astype(int)\n",
    "Output_male.Gender='M'\n",
    "# Female\n",
    "Output_female=raw_extend_test[raw_extend_test['Gender']==0][['ID','Gender','Probability']]\n",
    "Output_female.ID=Output_female.ID.astype(int)\n",
    "Output_female.Gender='F'\n",
    "Final_out=Output_male.append(Output_female).reset_index(drop=True)\n",
    "Final_out.to_csv(r'Recidivism_year3.csv',index=False)\n",
    "Final_out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
