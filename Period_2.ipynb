{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28f3ecdd",
   "metadata": {},
   "source": [
    "### Util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31de4655",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key savefig.frameon in file C:\\Users\\myjr\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle, line 421 ('savefig.frameon : True')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.4.2/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key verbose.level in file C:\\Users\\myjr\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle, line 472 ('verbose.level  : silent      # one of silent, helpful, debug, debug-annoying')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.4.2/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key verbose.fileo in file C:\\Users\\myjr\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle, line 473 ('verbose.fileo  : sys.stdout  # a log filename, sys.stdout or sys.stderr')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.4.2/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "In C:\\Users\\myjr\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The text.latex.preview rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\myjr\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The mathtext.fallback_to_cm rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\myjr\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: Support for setting the 'mathtext.fallback_to_cm' rcParam is deprecated since 3.3 and will be removed two minor releases later; use 'mathtext.fallback : 'cm' instead.\n",
      "In C:\\Users\\myjr\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The validate_bool_maybe_none function was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\myjr\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The savefig.jpeg_quality rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\myjr\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The keymap.all_axes rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\myjr\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The animation.avconv_path rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\myjr\\AppData\\Roaming\\Python\\Python37\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The animation.avconv_args rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "C:\\Users\\myjr\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\myjr\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\myjr\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\myjr\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\myjr\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\myjr\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import interp\n",
    "#import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from IPython.display import Image\n",
    "import pydotplus\n",
    "import graphviz\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression,LogisticRegression,Ridge\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier,RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV,train_test_split,cross_val_score,cross_validate\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_squared_error, roc_auc_score,roc_curve, auc, brier_score_loss\n",
    "from sklearn import tree\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from itertools import cycle\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, BatchNormalization, Activation, Embedding\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bca399e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "def y_roc(estimator,X):\n",
    "    y_scores=[]\n",
    "    for list in estimator.predict_proba(X):\n",
    "        y_scores.append(list[1])\n",
    "    return y_scores\n",
    "def y_roc_regression(estimator,X):\n",
    "    y_scores=[]\n",
    "    for list in estimator.predict(X):\n",
    "        y_scores.append(list)\n",
    "    return y_scores\n",
    "def cv_roc_plot(estimator,X,y):\n",
    "    cv = StratifiedKFold(n_splits=4,shuffle=False)\n",
    "    tprs = []\n",
    "    aucs = []\n",
    "    mean_fpr = np.linspace(0,1,100)\n",
    "    for train,test in cv.split(X,y):\n",
    "        prediction = estimator.fit(X.iloc[train],y.iloc[train]).predict_proba(X.iloc[test])\n",
    "        fpr, tpr, t = roc_curve(y.iloc[test], prediction[:, 1])\n",
    "        tpr[0]=0\n",
    "        tpr[-1]=1\n",
    "        tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        aucs.append(roc_auc)\n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "    return mean_fpr, mean_tpr, mean_auc\n",
    "def brier_score(y_prob_raw,y_true):\n",
    "    y_prob=[prob[1] for prob in y_prob_raw]\n",
    "    if len(y_prob)!=len(y_true):\n",
    "        print('Error: two lists must have same length')\n",
    "        return\n",
    "    out = 0\n",
    "    for prob_1,y in zip(y_prob,y_true):\n",
    "        out+=(prob_1-y)**2\n",
    "    return out/len(y_prob)\n",
    "def get_prob_1(y_prob_raw):\n",
    "    return [prob[1] for prob in y_prob_raw]\n",
    "def aver_prob(prob_lists):\n",
    "    n=len(prob_lists)\n",
    "    return np.sum(np.array(prob_lists),0)/n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366396af",
   "metadata": {},
   "source": [
    "### PUMA related data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b6aa180",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'Gender', 'Race', 'Age_at_Release', 'Residence_PUMA',\n",
       "       'Gang_Affiliated', 'Supervision_Risk_Score_First',\n",
       "       'Supervision_Level_First', 'Education_Level', 'Dependents',\n",
       "       'Prison_Offense', 'Prison_Years', 'Prior_Arrest_Episodes_Felony',\n",
       "       'Prior_Arrest_Episodes_Misd', 'Prior_Arrest_Episodes_Violent',\n",
       "       'Prior_Arrest_Episodes_Property', 'Prior_Arrest_Episodes_Drug',\n",
       "       'Prior_Arrest_Episodes_PPViolationCharges',\n",
       "       'Prior_Arrest_Episodes_DVCharges', 'Prior_Arrest_Episodes_GunCharges',\n",
       "       'Prior_Conviction_Episodes_Felony', 'Prior_Conviction_Episodes_Misd',\n",
       "       'Prior_Conviction_Episodes_Viol', 'Prior_Conviction_Episodes_Prop',\n",
       "       'Prior_Conviction_Episodes_Drug',\n",
       "       'Prior_Conviction_Episodes_PPViolationCharges',\n",
       "       'Prior_Conviction_Episodes_DomesticViolenceCharges',\n",
       "       'Prior_Conviction_Episodes_GunCharges', 'Prior_Revocations_Parole',\n",
       "       'Prior_Revocations_Probation', 'Condition_MH_SA', 'Condition_Cog_Ed',\n",
       "       'Condition_Other', 'Violations_ElectronicMonitoring',\n",
       "       'Violations_Instruction', 'Violations_FailToReport',\n",
       "       'Violations_MoveWithoutPermission', 'Delinquency_Reports',\n",
       "       'Program_Attendances', 'Program_UnexcusedAbsences', 'Residence_Changes',\n",
       "       'Avg_Days_per_DrugTest', 'DrugTests_THC_Positive',\n",
       "       'DrugTests_Cocaine_Positive', 'DrugTests_Meth_Positive',\n",
       "       'DrugTests_Other_Positive', 'Percent_Days_Employed', 'Jobs_Per_Year',\n",
       "       'Employment_Exempt', 'Recidivism_Within_3years',\n",
       "       'Recidivism_Arrest_Year1', 'Recidivism_Arrest_Year2',\n",
       "       'Recidivism_Arrest_Year3'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All features are available\n",
    "raw_train=pd.read_csv(r'.\\data\\NIJ_s_Recidivism_Challenge_Training_Dataset.csv')\n",
    "raw_train=raw_train.drop(raw_train[raw_train.Recidivism_Arrest_Year1==True].index)\n",
    "raw_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78f7a52e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RT</th>\n",
       "      <th>SERIALNO</th>\n",
       "      <th>DIVISION</th>\n",
       "      <th>PUMA</th>\n",
       "      <th>REGION</th>\n",
       "      <th>ST</th>\n",
       "      <th>ADJHSG</th>\n",
       "      <th>ADJINC</th>\n",
       "      <th>WGTP</th>\n",
       "      <th>NP</th>\n",
       "      <th>...</th>\n",
       "      <th>wgtp72</th>\n",
       "      <th>wgtp73</th>\n",
       "      <th>wgtp74</th>\n",
       "      <th>wgtp75</th>\n",
       "      <th>wgtp76</th>\n",
       "      <th>wgtp77</th>\n",
       "      <th>wgtp78</th>\n",
       "      <th>wgtp79</th>\n",
       "      <th>wgtp80</th>\n",
       "      <th>Code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H</td>\n",
       "      <td>127</td>\n",
       "      <td>5</td>\n",
       "      <td>1400</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>1000000</td>\n",
       "      <td>1007549</td>\n",
       "      <td>139</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>242</td>\n",
       "      <td>41</td>\n",
       "      <td>124</td>\n",
       "      <td>132</td>\n",
       "      <td>224</td>\n",
       "      <td>157</td>\n",
       "      <td>237</td>\n",
       "      <td>280</td>\n",
       "      <td>142</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>H</td>\n",
       "      <td>131</td>\n",
       "      <td>5</td>\n",
       "      <td>700</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>1000000</td>\n",
       "      <td>1007549</td>\n",
       "      <td>60</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>95</td>\n",
       "      <td>96</td>\n",
       "      <td>19</td>\n",
       "      <td>70</td>\n",
       "      <td>53</td>\n",
       "      <td>12</td>\n",
       "      <td>61</td>\n",
       "      <td>18</td>\n",
       "      <td>111</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>H</td>\n",
       "      <td>136</td>\n",
       "      <td>5</td>\n",
       "      <td>2200</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>1000000</td>\n",
       "      <td>1007549</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>23</td>\n",
       "      <td>33</td>\n",
       "      <td>40</td>\n",
       "      <td>38</td>\n",
       "      <td>19</td>\n",
       "      <td>33</td>\n",
       "      <td>19</td>\n",
       "      <td>8</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>H</td>\n",
       "      <td>151</td>\n",
       "      <td>5</td>\n",
       "      <td>1100</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>1000000</td>\n",
       "      <td>1007549</td>\n",
       "      <td>213</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>55</td>\n",
       "      <td>175</td>\n",
       "      <td>62</td>\n",
       "      <td>53</td>\n",
       "      <td>313</td>\n",
       "      <td>200</td>\n",
       "      <td>330</td>\n",
       "      <td>252</td>\n",
       "      <td>239</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>H</td>\n",
       "      <td>294</td>\n",
       "      <td>5</td>\n",
       "      <td>3400</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>1000000</td>\n",
       "      <td>1007549</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>59</td>\n",
       "      <td>19</td>\n",
       "      <td>61</td>\n",
       "      <td>109</td>\n",
       "      <td>102</td>\n",
       "      <td>99</td>\n",
       "      <td>74</td>\n",
       "      <td>73</td>\n",
       "      <td>97</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45693</th>\n",
       "      <td>H</td>\n",
       "      <td>1492766</td>\n",
       "      <td>5</td>\n",
       "      <td>5002</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>1000000</td>\n",
       "      <td>1007549</td>\n",
       "      <td>69</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>60</td>\n",
       "      <td>95</td>\n",
       "      <td>23</td>\n",
       "      <td>114</td>\n",
       "      <td>81</td>\n",
       "      <td>64</td>\n",
       "      <td>126</td>\n",
       "      <td>67</td>\n",
       "      <td>99</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45694</th>\n",
       "      <td>H</td>\n",
       "      <td>1492793</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>1000000</td>\n",
       "      <td>1007549</td>\n",
       "      <td>53</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>132</td>\n",
       "      <td>51</td>\n",
       "      <td>60</td>\n",
       "      <td>106</td>\n",
       "      <td>39</td>\n",
       "      <td>77</td>\n",
       "      <td>57</td>\n",
       "      <td>49</td>\n",
       "      <td>79</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45695</th>\n",
       "      <td>H</td>\n",
       "      <td>1492802</td>\n",
       "      <td>5</td>\n",
       "      <td>700</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>1000000</td>\n",
       "      <td>1007549</td>\n",
       "      <td>48</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>53</td>\n",
       "      <td>59</td>\n",
       "      <td>80</td>\n",
       "      <td>40</td>\n",
       "      <td>68</td>\n",
       "      <td>85</td>\n",
       "      <td>76</td>\n",
       "      <td>105</td>\n",
       "      <td>43</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45696</th>\n",
       "      <td>H</td>\n",
       "      <td>1492814</td>\n",
       "      <td>5</td>\n",
       "      <td>2800</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>1000000</td>\n",
       "      <td>1007549</td>\n",
       "      <td>109</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>84</td>\n",
       "      <td>114</td>\n",
       "      <td>101</td>\n",
       "      <td>79</td>\n",
       "      <td>35</td>\n",
       "      <td>203</td>\n",
       "      <td>117</td>\n",
       "      <td>108</td>\n",
       "      <td>182</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45697</th>\n",
       "      <td>H</td>\n",
       "      <td>1492830</td>\n",
       "      <td>5</td>\n",
       "      <td>4000</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>1000000</td>\n",
       "      <td>1007549</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>42</td>\n",
       "      <td>9</td>\n",
       "      <td>26</td>\n",
       "      <td>28</td>\n",
       "      <td>9</td>\n",
       "      <td>30</td>\n",
       "      <td>28</td>\n",
       "      <td>8</td>\n",
       "      <td>47</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45698 rows × 232 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      RT  SERIALNO  DIVISION  PUMA  REGION  ST   ADJHSG   ADJINC  WGTP  NP  \\\n",
       "0      H       127         5  1400       3  13  1000000  1007549   139   1   \n",
       "1      H       131         5   700       3  13  1000000  1007549    60   3   \n",
       "2      H       136         5  2200       3  13  1000000  1007549    22   1   \n",
       "3      H       151         5  1100       3  13  1000000  1007549   213   2   \n",
       "4      H       294         5  3400       3  13  1000000  1007549    64   2   \n",
       "...   ..       ...       ...   ...     ...  ..      ...      ...   ...  ..   \n",
       "45693  H   1492766         5  5002       3  13  1000000  1007549    69   1   \n",
       "45694  H   1492793         5   100       3  13  1000000  1007549    53   2   \n",
       "45695  H   1492802         5   700       3  13  1000000  1007549    48   2   \n",
       "45696  H   1492814         5  2800       3  13  1000000  1007549   109   2   \n",
       "45697  H   1492830         5  4000       3  13  1000000  1007549    28   1   \n",
       "\n",
       "       ...  wgtp72  wgtp73  wgtp74  wgtp75  wgtp76  wgtp77  wgtp78  wgtp79  \\\n",
       "0      ...     242      41     124     132     224     157     237     280   \n",
       "1      ...      95      96      19      70      53      12      61      18   \n",
       "2      ...       7      23      33      40      38      19      33      19   \n",
       "3      ...      55     175      62      53     313     200     330     252   \n",
       "4      ...      59      19      61     109     102      99      74      73   \n",
       "...    ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "45693  ...      60      95      23     114      81      64     126      67   \n",
       "45694  ...     132      51      60     106      39      77      57      49   \n",
       "45695  ...      53      59      80      40      68      85      76     105   \n",
       "45696  ...      84     114     101      79      35     203     117     108   \n",
       "45697  ...      42       9      26      28       9      30      28       8   \n",
       "\n",
       "       wgtp80  Code  \n",
       "0         142     4  \n",
       "1         111    18  \n",
       "2           8    22  \n",
       "3         239    19  \n",
       "4          97    12  \n",
       "...       ...   ...  \n",
       "45693      99    10  \n",
       "45694      79     7  \n",
       "45695      43    18  \n",
       "45696     182    17  \n",
       "45697      47     8  \n",
       "\n",
       "[45698 rows x 232 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_PUMA=pd.read_csv(r'.\\data\\ss13hga_13.csv')\n",
    "pdata_PUMA=pd.read_csv(r'.\\data\\ss13pga_13.csv')\n",
    "data_PUMA['Code']=None\n",
    "pdata_PUMA['Code']=None\n",
    "data_PUMA['PUMA']=data_PUMA['PUMA'].astype(str)\n",
    "pdata_PUMA['PUMA']=pdata_PUMA['PUMA'].astype(str)\n",
    "PUMA_code_map={'1003':1,'4400':1,'1008':2,'4300':2,'1200':3, '1300':3,'1400':4,'1500':4,'1600':4,'1700':5,'1800':5,\n",
    "              '2001':6,'2002':6,'2003':6,'4005':6,'100':7,'200':7,'500':7,'4000':8,'4100':8,'4200':8,'5001':9,'6001':9,'6002':9,\n",
    "              '2400':10,'5002':10,'1001':11,'3004':11,'4600':11,'1002':12, '1005':12, '3300':12, '3400':12, '4001':12, '4002':12,\n",
    "              '4006':12,'3101':13,'3102':13,'1900':14, '3900':14, '4003':14, '4004':14,'3001':15, '3002':15, '3003':15, '3005':15,\n",
    "              '2500':16, '4500':16,'2800':17, '2900':17, '3200':17, '3500':17,'600':18, '700':18, '800':18,'900':19, '1100':19,\n",
    "              '300':20, '401':20, '402':20, '1004':21, '2100':21 ,'2200':22, '2300':22, '1006':23, '1007':23, '2004':23,'2600':24, \n",
    "              '2700':24,'3600':25, '3700':25, '3800':25}\n",
    "data_PUMA['Code']=data_PUMA['PUMA'].map(PUMA_code_map)\n",
    "pdata_PUMA['Code']=pdata_PUMA['PUMA'].map(PUMA_code_map)\n",
    "data_PUMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59191338",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>County</th>\n",
       "      <th>Total law\\nenforcement\\nemployees</th>\n",
       "      <th>Total\\nofficers</th>\n",
       "      <th>Total\\ncivilians</th>\n",
       "      <th>Murder</th>\n",
       "      <th>Rape</th>\n",
       "      <th>Robbery</th>\n",
       "      <th>Assault</th>\n",
       "      <th>Burglary</th>\n",
       "      <th>Larceny</th>\n",
       "      <th>Vehicle Theft</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Barrow</td>\n",
       "      <td>190</td>\n",
       "      <td>129</td>\n",
       "      <td>61</td>\n",
       "      <td>2</td>\n",
       "      <td>33</td>\n",
       "      <td>20</td>\n",
       "      <td>302</td>\n",
       "      <td>547</td>\n",
       "      <td>1551</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bartow</td>\n",
       "      <td>234</td>\n",
       "      <td>197</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>32</td>\n",
       "      <td>329</td>\n",
       "      <td>806</td>\n",
       "      <td>2263</td>\n",
       "      <td>264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bibb</td>\n",
       "      <td>361</td>\n",
       "      <td>296</td>\n",
       "      <td>65</td>\n",
       "      <td>19</td>\n",
       "      <td>67</td>\n",
       "      <td>296</td>\n",
       "      <td>416</td>\n",
       "      <td>2500</td>\n",
       "      <td>6297</td>\n",
       "      <td>715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Brantley</td>\n",
       "      <td>43</td>\n",
       "      <td>19</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "      <td>191</td>\n",
       "      <td>243</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Brooks</td>\n",
       "      <td>51</td>\n",
       "      <td>22</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>45</td>\n",
       "      <td>114</td>\n",
       "      <td>291</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>Webster</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>White</td>\n",
       "      <td>73</td>\n",
       "      <td>40</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>35</td>\n",
       "      <td>153</td>\n",
       "      <td>328</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>Wilcox</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>72</td>\n",
       "      <td>93</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>Wilkes</td>\n",
       "      <td>27</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>29</td>\n",
       "      <td>106</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>Wilkinson</td>\n",
       "      <td>28</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>47</td>\n",
       "      <td>75</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>125 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        County  Total law\\nenforcement\\nemployees  Total\\nofficers  \\\n",
       "0       Barrow                                190              129   \n",
       "1       Bartow                                234              197   \n",
       "2         Bibb                                361              296   \n",
       "3     Brantley                                 43               19   \n",
       "4       Brooks                                 51               22   \n",
       "..         ...                                ...              ...   \n",
       "120    Webster                                  6                5   \n",
       "121      White                                 73               40   \n",
       "122     Wilcox                                 16               10   \n",
       "123     Wilkes                                 27               14   \n",
       "124  Wilkinson                                 28               16   \n",
       "\n",
       "     Total\\ncivilians Murder Rape Robbery Assault Burglary Larceny  \\\n",
       "0                  61      2   33      20     302      547    1551   \n",
       "1                  37      0   13      32     329      806    2263   \n",
       "2                  65     19   67     296     416     2500    6297   \n",
       "3                  24      0    1       7      16      191     243   \n",
       "4                  29      2    2       9      45      114     291   \n",
       "..                ...    ...  ...     ...     ...      ...     ...   \n",
       "120                 1      0    0       1       0        8       7   \n",
       "121                33      0    3       4      35      153     328   \n",
       "122                 6      0    5       2      16       72      93   \n",
       "123                13      0    0       0      32       29     106   \n",
       "124                12      0    0       0       7       47      75   \n",
       "\n",
       "    Vehicle Theft  \n",
       "0             145  \n",
       "1             264  \n",
       "2             715  \n",
       "3              10  \n",
       "4              31  \n",
       "..            ...  \n",
       "120             3  \n",
       "121            15  \n",
       "122            10  \n",
       "123            11  \n",
       "124             3  \n",
       "\n",
       "[125 rows x 11 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Law_enforcement=pd.read_excel(r'.\\data\\table_80_full_time_law_enforcement_employees_georgia_by_metropolitan_and_nonmetropolitan_counties_2012.xls')\n",
    "crime_rates=pd.read_excel(r'.\\data\\georgia crime rates.xlsx',header=1)\n",
    "crime_rates=crime_rates.groupby(by='County').sum().iloc[11:,:]\n",
    "crime_len=pd.merge(Law_enforcement,crime_rates,left_on='County',right_on='County')\n",
    "crime_len=crime_len.drop(columns=crime_len.columns[crime_len.isnull().any()])\n",
    "crime_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c71f610b",
   "metadata": {},
   "outputs": [],
   "source": [
    "crime_len=crime_len.set_index(keys='County')\n",
    "crime_len['PUMA']=None\n",
    "crime_len.loc['Barrow','PUMA']=3800\n",
    "crime_len.loc['Bartow','PUMA']=2900\n",
    "crime_len.loc['Bibb','PUMA']=1700\n",
    "crime_len.loc['Bryan','PUMA']=200\n",
    "crime_len.loc['Brooks','PUMA']=700\n",
    "crime_len.loc['Butts','PUMA']=1900\n",
    "crime_len.loc['Brantley','PUMA']=500\n",
    "crime_len.loc['Carroll','PUMA']=2300\n",
    "crime_len.loc['Catoosa','PUMA']=2600\n",
    "crime_len.loc['Chatham','PUMA']=401\n",
    "crime_len.loc['Chattahoochee','PUMA']=1700\n",
    "crime_len.loc['Cherokee','PUMA']=3101\n",
    "crime_len.loc['Clarke','PUMA']=3600\n",
    "crime_len.loc['Clayton','PUMA']=5001\n",
    "crime_len.loc['Cobb','PUMA']=3001\n",
    "crime_len.loc['Columbia','PUMA']=4100\n",
    "crime_len.loc['Coweta','PUMA']=2100\n",
    "crime_len.loc['Dade','PUMA']=2600\n",
    "crime_len.loc['Dawson','PUMA']=3200\n",
    "crime_len.loc['Dougherty','PUMA']=900\n",
    "crime_len.loc['Effingham','PUMA']=300\n",
    "crime_len.loc['Floyd','PUMA']=2500\n",
    "crime_len.loc['Forsyth','PUMA']=3300\n",
    "crime_len.loc['Fulton','PUMA']=4600\n",
    "crime_len.loc['Glynn','PUMA']=100\n",
    "crime_len.loc['Hall','PUMA']=3400\n",
    "crime_len.loc['Haralson','PUMA']=2500\n",
    "crime_len.loc['Harris','PUMA']=1800\n",
    "crime_len.loc['Heard','PUMA']=2200\n",
    "crime_len.loc['Henry','PUMA']=600\n",
    "crime_len.loc['Houston','PUMA']=1500\n",
    "crime_len.loc['Jones','PUMA']=1600\n",
    "crime_len.loc['Lamar','PUMA']=1900\n",
    "crime_len.loc['Lanier','PUMA']=500\n",
    "crime_len.loc['Long','PUMA']=200\n",
    "crime_len.loc['Lowndes','PUMA']=600\n",
    "crime_len.loc['Madison','PUMA']=3700\n",
    "crime_len.loc['McDuffie','PUMA']=4200\n",
    "crime_len.loc['McIntosh','PUMA']=100\n",
    "crime_len.loc['Morgan','PUMA']=3900\n",
    "crime_len.loc['Monroe','PUMA']=1600\n",
    "crime_len.loc['Oconee','PUMA']=3700\n",
    "crime_len.loc['Oglethorpe','PUMA']=3700\n",
    "crime_len.loc['Pickens','PUMA']=2800\n",
    "crime_len.loc['Spalding','PUMA']=1900\n",
    "crime_len.loc['Twiggs','PUMA']=1600\n",
    "crime_len.loc['Appling','PUMA']=1200\n",
    "crime_len.loc['Baldwin','PUMA']=1600\n",
    "crime_len.loc['Ben Hill','PUMA']=700\n",
    "crime_len.loc['Berrien','PUMA']=700\n",
    "crime_len.loc['Newton','PUMA']=4300\n",
    "crime_len.loc['Paulding','PUMA']=4500\n",
    "crime_len.loc['Pulaski','PUMA']=1500\n",
    "crime_len.loc['Rockdale','PUMA']=4300\n",
    "crime_len.loc['Walker','PUMA']=2600\n",
    "crime_len.loc['Walton','PUMA']=3900\n",
    "crime_len.loc['Whitfield','PUMA']=2700\n",
    "crime_len.loc['Worth','PUMA']=800\n",
    "crime_len.loc['Bulloch','PUMA']=300\n",
    "crime_len.loc['Camden','PUMA']=100\n",
    "crime_len.loc['Chattooga','PUMA']=2600\n",
    "crime_len.loc['Decatur','PUMA']=2002\n",
    "crime_len.loc['Dodge','PUMA']=1300\n",
    "crime_len.loc['Dooly','PUMA']=2600\n",
    "crime_len.loc['Early','PUMA']=1100\n",
    "crime_len.loc['Elbert','PUMA']=3700\n",
    "crime_len.loc['Emanuel','PUMA']=1300\n",
    "crime_len.loc['Fannin','PUMA']=2800\n",
    "crime_len.loc['Franklin','PUMA']=3500\n",
    "crime_len.loc['Gilmer','PUMA']=2800\n",
    "crime_len.loc['Gordon','PUMA']=2800\n",
    "crime_len.loc['Grady','PUMA']=1100\n",
    "crime_len.loc['Greene','PUMA']=3300\n",
    "crime_len.loc['Habersham','PUMA']=3500\n",
    "crime_len.loc['Hancock','PUMA']=4200\n",
    "crime_len.loc['Hart','PUMA']=3500\n",
    "crime_len.loc['Irwin','PUMA']=1600\n",
    "crime_len.loc['Jackson','PUMA']=1900\n",
    "crime_len.loc['Jefferson','PUMA']=4200\n",
    "crime_len.loc['Bleckley','PUMA']=1300\n",
    "crime_len.loc['Charlton','PUMA']=500\n",
    "crime_len.loc['Clinch','PUMA']=500\n",
    "crime_len.loc['Coffee','PUMA']=500\n",
    "crime_len.loc['Cook','PUMA']=700\n",
    "crime_len.loc['Crisp','PUMA']=1800\n",
    "crime_len.loc['Johnson','PUMA']=1300\n",
    "crime_len.loc['Lumpkin','PUMA']=1800\n",
    "crime_len.loc['Miller','PUMA']=1100\n",
    "crime_len.loc['Mitchell','PUMA']=1100\n",
    "crime_len.loc['Miller','PUMA']=1100\n",
    "crime_len.loc['Pierce','PUMA']=500\n",
    "crime_len.loc['Polk','PUMA']=2500\n",
    "crime_len.loc['Rabun','PUMA']=3200\n",
    "crime_len.loc['Schley','PUMA']=1800\n",
    "crime_len.loc['Seminole','PUMA']=1100\n",
    "crime_len.loc['Stephens','PUMA']=3500\n",
    "crime_len.loc['Stewart','PUMA']=200\n",
    "crime_len.loc['Sumter','PUMA']=1800\n",
    "crime_len.loc['Talbot','PUMA']=1800\n",
    "crime_len.loc['Taliaferro','PUMA']=4200\n",
    "crime_len.loc['Taylor','PUMA']=1800\n",
    "crime_len.loc['Tift','PUMA']=700\n",
    "crime_len.loc['Towns','PUMA']=3200\n",
    "crime_len.loc['Treutlen','PUMA']=1300\n",
    "crime_len.loc['Upson','PUMA']=1900\n",
    "crime_len.loc['Macon','PUMA']=1400\n",
    "crime_len.loc['Polk','PUMA']=2500\n",
    "crime_len.loc['Screven','PUMA']=300\n",
    "crime_len.loc['Thomas','PUMA']=800\n",
    "crime_len.loc['Troup','PUMA']=2200\n",
    "crime_len.loc['Union','PUMA']=1004\n",
    "crime_len.loc['Ware','PUMA']=500\n",
    "crime_len.loc['Warren','PUMA']=4200\n",
    "crime_len.loc['Washington','PUMA']=4200\n",
    "crime_len.loc['Wayne','PUMA']=1200\n",
    "crime_len.loc['Webster','PUMA']=1800\n",
    "crime_len.loc['White','PUMA']=3200\n",
    "crime_len.loc['Wilcox','PUMA']=1300\n",
    "crime_len.loc['Wilkes','PUMA']=4200\n",
    "crime_len.loc['Wilkinson','PUMA']=1600\n",
    "crime_len.drop(index=crime_len[crime_len.PUMA.isnull()].index,inplace=True)\n",
    "crime_len['Code']=(crime_len['PUMA']).astype(str).map(PUMA_code_map)\n",
    "crime_len=crime_len.astype(int)\n",
    "crime_len=crime_len.groupby('Code').mean()\n",
    "crime_len.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4cba816",
   "metadata": {},
   "outputs": [],
   "source": [
    "PUMA_group=data_PUMA.groupby(by='Code',as_index=False).mean()\n",
    "PUMA_group=PUMA_group.iloc[:,:-81].drop(columns=['SERIALNO', 'DIVISION', 'REGION', 'ST', 'ADJHSG', 'ADJINC',\n",
    "       'WGTP'])\n",
    "PUMA_group['TEN']=data_PUMA[['Code','TEN']].groupby(by='Code',as_index=False).agg(lambda x:x.value_counts().index[0])['TEN']\n",
    "PUMA_new=pd.merge(PUMA_group,crime_len,left_on='Code',right_on='Code',how='outer')\n",
    "PUMA_new=PUMA_new.fillna(PUMA_new.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f89a2f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_extend=pd.merge(raw_train,PUMA_new,left_on='Residence_PUMA',right_on='Code',how='left')\n",
    "\n",
    "# Change the dtype of the feature from object to intagar\n",
    "raw_extend['TEN']=raw_extend['TEN'].astype('category')\n",
    "raw_extend['TEN']=raw_extend['TEN'].cat.codes\n",
    "raw_extend['Residence_PUMA']=raw_extend['Residence_PUMA'].astype('category')\n",
    "raw_extend['Residence_PUMA']=raw_extend['Residence_PUMA'].cat.codes\n",
    "raw_extend['Age_at_Release']=raw_extend['Age_at_Release'].apply(lambda x: int(x[:2]))\n",
    "raw_extend['Dependents']=raw_extend['Dependents'].apply(lambda x: int(x[:1]))\n",
    "raw_extend['Prior_Arrest_Episodes_Felony']=raw_extend['Prior_Arrest_Episodes_Felony'].apply(lambda x: int(x[:2]))\n",
    "raw_extend['Prior_Arrest_Episodes_Drug']=raw_extend['Prior_Arrest_Episodes_Drug'].apply(lambda x: int(x[:2]))\n",
    "raw_extend['Prior_Arrest_Episodes_Misd']=raw_extend['Prior_Arrest_Episodes_Misd'].apply(lambda x: int(x[:2]))\n",
    "raw_extend['Prior_Arrest_Episodes_Violent']=raw_extend['Prior_Arrest_Episodes_Violent'].apply(lambda x: int(x[:2]))\n",
    "raw_extend['Prior_Arrest_Episodes_Property']=raw_extend['Prior_Arrest_Episodes_Property'].apply(lambda x: int(x[:2]))\n",
    "raw_extend['Prior_Arrest_Episodes_PPViolationCharges']=raw_extend['Prior_Arrest_Episodes_PPViolationCharges'].apply(lambda x: int(x[:2]))\n",
    "raw_extend['Prior_Conviction_Episodes_Felony']=raw_extend['Prior_Conviction_Episodes_Felony'].apply(lambda x: int(x[:1]))\n",
    "raw_extend['Prior_Conviction_Episodes_Misd']=raw_extend['Prior_Conviction_Episodes_Misd'].apply(lambda x: int(x[:1]))\n",
    "raw_extend['Prior_Conviction_Episodes_Prop']=raw_extend['Prior_Conviction_Episodes_Prop'].apply(lambda x: int(x[:1]))\n",
    "raw_extend['Prior_Conviction_Episodes_Drug']=raw_extend['Prior_Conviction_Episodes_Drug'].apply(lambda x: int(x[:1]))\n",
    "#raw_extend['Delinquency_Reports']=raw_extend['Delinquency_Reports'].apply(lambda x: int(x[:1]))\n",
    "#raw_extend['Program_Attendances']=raw_extend['Program_Attendances'].apply(lambda x: int(x[:2]))\n",
    "#raw_extend['Program_UnexcusedAbsences']=raw_extend['Program_UnexcusedAbsences'].apply(lambda x: int(x[:1]))\n",
    "#raw_extend['Residence_Changes']=raw_extend['Residence_Changes'].apply(lambda x: int(x[:1]))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaling_set=[]\n",
    "for column in raw_extend.columns:\n",
    "    if raw_extend[column].dtype == object:\n",
    "        raw_extend[column]=raw_extend[column].astype('category')\n",
    "        raw_extend[column]=raw_extend[column].cat.codes\n",
    "    elif raw_extend[column].dtype in ['int64','float32','float64'] :\n",
    "        scaling_set+=[column]\n",
    "raw_extend[scaling_set]=scaler.fit_transform(raw_extend[scaling_set].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7605239",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression train score: 0.7465219224283305 \n",
      " test score: 0.7420170723996206 \n",
      " Logistic regression train Brier score: 0.17236565921948835 \n",
      " test Brier score: 0.1753431584611295 \n",
      " AUROC: 0.6857945758726021\n",
      "Random forest train score: 1.0 \n",
      " test score: 0.7473917167246286 \n",
      " Random forest  train Brier score: 0.024640289488476238 \n",
      " test Brier score: 0.17694515052517001 \n",
      " AUROC: 0.6623156384596705\n",
      "SGD best layer size: {'min_samples_split': 4, 'n_estimators': 100} \n",
      " best train score: -0.1709255324726474 \n",
      " test score: -0.1697356918297213 \n",
      " SGD  train Brier score: 0.15192509009585523 \n",
      " test Brier score: 0.16973569182972115 \n",
      " AUROC: 0.7105305479936438\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\myjr\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208/208 [==============================] - 1s 2ms/step - loss: 0.7403 - accuracy: 0.6058 - val_loss: 0.5624 - val_accuracy: 0.7446\n",
      "Epoch 2/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.6300 - accuracy: 0.7038 - val_loss: 0.5596 - val_accuracy: 0.7446\n",
      "Epoch 3/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5938 - accuracy: 0.7247 - val_loss: 0.5586 - val_accuracy: 0.7446\n",
      "Epoch 4/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5827 - accuracy: 0.7341 - val_loss: 0.5592 - val_accuracy: 0.7446\n",
      "Epoch 5/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5771 - accuracy: 0.7360 - val_loss: 0.5597 - val_accuracy: 0.7446\n",
      "Epoch 6/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5733 - accuracy: 0.7381 - val_loss: 0.5589 - val_accuracy: 0.7446\n",
      "Epoch 7/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5698 - accuracy: 0.7377 - val_loss: 0.5564 - val_accuracy: 0.7446\n",
      "Epoch 8/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5668 - accuracy: 0.7398 - val_loss: 0.5562 - val_accuracy: 0.7446\n",
      "Epoch 9/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5689 - accuracy: 0.7383 - val_loss: 0.5551 - val_accuracy: 0.7446\n",
      "Epoch 10/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5645 - accuracy: 0.7389 - val_loss: 0.5538 - val_accuracy: 0.7446\n",
      "Epoch 11/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5666 - accuracy: 0.7381 - val_loss: 0.5529 - val_accuracy: 0.7446\n",
      "Epoch 12/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5645 - accuracy: 0.7390 - val_loss: 0.5508 - val_accuracy: 0.7446\n",
      "Epoch 13/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5610 - accuracy: 0.7407 - val_loss: 0.5508 - val_accuracy: 0.7446\n",
      "Epoch 14/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5611 - accuracy: 0.7386 - val_loss: 0.5489 - val_accuracy: 0.7450\n",
      "Epoch 15/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5634 - accuracy: 0.7377 - val_loss: 0.5494 - val_accuracy: 0.7450\n",
      "Epoch 16/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5612 - accuracy: 0.7384 - val_loss: 0.5475 - val_accuracy: 0.7450\n",
      "Epoch 17/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5565 - accuracy: 0.7375 - val_loss: 0.5448 - val_accuracy: 0.7450\n",
      "Epoch 18/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5573 - accuracy: 0.7359 - val_loss: 0.5446 - val_accuracy: 0.7457\n",
      "Epoch 19/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5561 - accuracy: 0.7384 - val_loss: 0.5429 - val_accuracy: 0.7457\n",
      "Epoch 20/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5549 - accuracy: 0.7360 - val_loss: 0.5429 - val_accuracy: 0.7446\n",
      "Epoch 21/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5554 - accuracy: 0.7368 - val_loss: 0.5415 - val_accuracy: 0.7478\n",
      "Epoch 22/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5550 - accuracy: 0.7383 - val_loss: 0.5411 - val_accuracy: 0.7446\n",
      "Epoch 23/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5524 - accuracy: 0.7386 - val_loss: 0.5398 - val_accuracy: 0.7450\n",
      "Epoch 24/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5533 - accuracy: 0.7378 - val_loss: 0.5401 - val_accuracy: 0.7450\n",
      "Epoch 25/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5519 - accuracy: 0.7372 - val_loss: 0.5397 - val_accuracy: 0.7460\n",
      "Epoch 26/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5477 - accuracy: 0.7378 - val_loss: 0.5370 - val_accuracy: 0.7422\n",
      "Epoch 27/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5490 - accuracy: 0.7393 - val_loss: 0.5363 - val_accuracy: 0.7422\n",
      "Epoch 28/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5494 - accuracy: 0.7381 - val_loss: 0.5357 - val_accuracy: 0.7425\n",
      "Epoch 29/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5484 - accuracy: 0.7363 - val_loss: 0.5375 - val_accuracy: 0.7450\n",
      "Epoch 30/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5493 - accuracy: 0.7384 - val_loss: 0.5350 - val_accuracy: 0.7443\n",
      "Epoch 31/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5466 - accuracy: 0.7384 - val_loss: 0.5353 - val_accuracy: 0.7429\n",
      "Epoch 32/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5486 - accuracy: 0.7407 - val_loss: 0.5340 - val_accuracy: 0.7450\n",
      "Epoch 33/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5479 - accuracy: 0.7378 - val_loss: 0.5340 - val_accuracy: 0.7443\n",
      "Epoch 34/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5443 - accuracy: 0.7384 - val_loss: 0.5334 - val_accuracy: 0.7443\n",
      "Epoch 35/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5451 - accuracy: 0.7368 - val_loss: 0.5328 - val_accuracy: 0.7439\n",
      "Epoch 36/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5452 - accuracy: 0.7338 - val_loss: 0.5334 - val_accuracy: 0.7446\n",
      "Epoch 37/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5417 - accuracy: 0.7402 - val_loss: 0.5330 - val_accuracy: 0.7436\n",
      "Epoch 38/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5402 - accuracy: 0.7381 - val_loss: 0.5315 - val_accuracy: 0.7453\n",
      "Epoch 39/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5435 - accuracy: 0.7378 - val_loss: 0.5321 - val_accuracy: 0.7439\n",
      "Epoch 40/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5440 - accuracy: 0.7418 - val_loss: 0.5319 - val_accuracy: 0.7439\n",
      "Epoch 41/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5395 - accuracy: 0.7413 - val_loss: 0.5319 - val_accuracy: 0.7457\n",
      "Epoch 42/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5441 - accuracy: 0.7368 - val_loss: 0.5313 - val_accuracy: 0.7446\n",
      "Epoch 43/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5423 - accuracy: 0.7387 - val_loss: 0.5310 - val_accuracy: 0.7450\n",
      "Epoch 44/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5418 - accuracy: 0.7406 - val_loss: 0.5307 - val_accuracy: 0.7439\n",
      "Epoch 45/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5433 - accuracy: 0.7380 - val_loss: 0.5303 - val_accuracy: 0.7457\n",
      "Epoch 46/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5409 - accuracy: 0.7396 - val_loss: 0.5306 - val_accuracy: 0.7446\n",
      "Epoch 47/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5406 - accuracy: 0.7404 - val_loss: 0.5299 - val_accuracy: 0.7450\n",
      "Epoch 48/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5371 - accuracy: 0.7371 - val_loss: 0.5293 - val_accuracy: 0.7478\n",
      "Epoch 49/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5395 - accuracy: 0.7387 - val_loss: 0.5295 - val_accuracy: 0.7446\n",
      "Epoch 50/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5387 - accuracy: 0.7378 - val_loss: 0.5285 - val_accuracy: 0.7450\n",
      "Epoch 51/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5384 - accuracy: 0.7392 - val_loss: 0.5296 - val_accuracy: 0.7439\n",
      "Epoch 52/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5386 - accuracy: 0.7402 - val_loss: 0.5304 - val_accuracy: 0.7460\n",
      "Epoch 53/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5366 - accuracy: 0.7410 - val_loss: 0.5286 - val_accuracy: 0.7460\n",
      "Epoch 54/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5379 - accuracy: 0.7415 - val_loss: 0.5283 - val_accuracy: 0.7464\n",
      "Epoch 55/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5390 - accuracy: 0.7413 - val_loss: 0.5290 - val_accuracy: 0.7446\n",
      "Epoch 56/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5368 - accuracy: 0.7398 - val_loss: 0.5282 - val_accuracy: 0.7471\n",
      "Epoch 57/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5368 - accuracy: 0.7399 - val_loss: 0.5281 - val_accuracy: 0.7468\n",
      "Epoch 58/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5380 - accuracy: 0.7399 - val_loss: 0.5289 - val_accuracy: 0.7475\n",
      "Epoch 59/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5385 - accuracy: 0.7409 - val_loss: 0.5281 - val_accuracy: 0.7482\n",
      "Epoch 60/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5339 - accuracy: 0.7410 - val_loss: 0.5278 - val_accuracy: 0.7482\n",
      "Epoch 61/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5355 - accuracy: 0.7407 - val_loss: 0.5280 - val_accuracy: 0.7492\n",
      "Epoch 62/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5372 - accuracy: 0.7412 - val_loss: 0.5280 - val_accuracy: 0.7453\n",
      "Epoch 63/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5380 - accuracy: 0.7392 - val_loss: 0.5281 - val_accuracy: 0.7457\n",
      "Epoch 64/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5336 - accuracy: 0.7413 - val_loss: 0.5272 - val_accuracy: 0.7478\n",
      "Epoch 65/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5331 - accuracy: 0.7458 - val_loss: 0.5265 - val_accuracy: 0.7482\n",
      "Epoch 66/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5353 - accuracy: 0.7412 - val_loss: 0.5266 - val_accuracy: 0.7457\n",
      "Epoch 67/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5334 - accuracy: 0.7380 - val_loss: 0.5263 - val_accuracy: 0.7478\n",
      "Epoch 68/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5337 - accuracy: 0.7415 - val_loss: 0.5266 - val_accuracy: 0.7471\n",
      "Epoch 69/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5346 - accuracy: 0.7387 - val_loss: 0.5260 - val_accuracy: 0.7464\n",
      "Epoch 70/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5355 - accuracy: 0.7404 - val_loss: 0.5266 - val_accuracy: 0.7450\n",
      "Epoch 71/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5344 - accuracy: 0.7377 - val_loss: 0.5257 - val_accuracy: 0.7460\n",
      "Epoch 72/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5379 - accuracy: 0.7412 - val_loss: 0.5265 - val_accuracy: 0.7485\n",
      "Epoch 73/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5334 - accuracy: 0.7401 - val_loss: 0.5261 - val_accuracy: 0.7482\n",
      "Epoch 74/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5359 - accuracy: 0.7404 - val_loss: 0.5259 - val_accuracy: 0.7468\n",
      "Epoch 75/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5317 - accuracy: 0.7396 - val_loss: 0.5251 - val_accuracy: 0.7471\n",
      "Epoch 76/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5316 - accuracy: 0.7396 - val_loss: 0.5253 - val_accuracy: 0.7478\n",
      "Epoch 77/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5309 - accuracy: 0.7407 - val_loss: 0.5251 - val_accuracy: 0.7475\n",
      "Epoch 78/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5276 - accuracy: 0.7419 - val_loss: 0.5246 - val_accuracy: 0.7471\n",
      "Epoch 79/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5346 - accuracy: 0.7387 - val_loss: 0.5253 - val_accuracy: 0.7464\n",
      "Epoch 80/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5325 - accuracy: 0.7401 - val_loss: 0.5248 - val_accuracy: 0.7464\n",
      "Epoch 81/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5294 - accuracy: 0.7416 - val_loss: 0.5244 - val_accuracy: 0.7482\n",
      "Epoch 82/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5297 - accuracy: 0.7418 - val_loss: 0.5248 - val_accuracy: 0.7457\n",
      "Epoch 83/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5333 - accuracy: 0.7419 - val_loss: 0.5244 - val_accuracy: 0.7471\n",
      "Epoch 84/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5318 - accuracy: 0.7409 - val_loss: 0.5244 - val_accuracy: 0.7489\n",
      "Epoch 85/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5325 - accuracy: 0.7433 - val_loss: 0.5247 - val_accuracy: 0.7485\n",
      "Epoch 86/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5314 - accuracy: 0.7446 - val_loss: 0.5248 - val_accuracy: 0.7482\n",
      "Epoch 87/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5285 - accuracy: 0.7404 - val_loss: 0.5240 - val_accuracy: 0.7464\n",
      "Epoch 88/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5321 - accuracy: 0.7409 - val_loss: 0.5247 - val_accuracy: 0.7475\n",
      "Epoch 89/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5300 - accuracy: 0.7390 - val_loss: 0.5248 - val_accuracy: 0.7471\n",
      "Epoch 90/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5318 - accuracy: 0.7412 - val_loss: 0.5247 - val_accuracy: 0.7482\n",
      "Epoch 91/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5305 - accuracy: 0.7415 - val_loss: 0.5249 - val_accuracy: 0.7489\n",
      "Epoch 92/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5298 - accuracy: 0.7442 - val_loss: 0.5250 - val_accuracy: 0.7464\n",
      "Epoch 93/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5308 - accuracy: 0.7398 - val_loss: 0.5243 - val_accuracy: 0.7468\n",
      "Epoch 94/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5307 - accuracy: 0.7433 - val_loss: 0.5237 - val_accuracy: 0.7482\n",
      "Epoch 95/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5312 - accuracy: 0.7383 - val_loss: 0.5242 - val_accuracy: 0.7464\n",
      "Epoch 96/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5307 - accuracy: 0.7390 - val_loss: 0.5240 - val_accuracy: 0.7471\n",
      "Epoch 97/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5295 - accuracy: 0.7395 - val_loss: 0.5250 - val_accuracy: 0.7475\n",
      "Epoch 98/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5267 - accuracy: 0.7402 - val_loss: 0.5241 - val_accuracy: 0.7475\n",
      "Epoch 99/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5298 - accuracy: 0.7406 - val_loss: 0.5243 - val_accuracy: 0.7475\n",
      "Epoch 100/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5268 - accuracy: 0.7440 - val_loss: 0.5237 - val_accuracy: 0.7510\n",
      "Epoch 101/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5307 - accuracy: 0.7404 - val_loss: 0.5239 - val_accuracy: 0.7482\n",
      "Epoch 102/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5269 - accuracy: 0.7404 - val_loss: 0.5241 - val_accuracy: 0.7453\n",
      "Epoch 103/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5318 - accuracy: 0.7393 - val_loss: 0.5236 - val_accuracy: 0.7478\n",
      "Epoch 104/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5276 - accuracy: 0.7427 - val_loss: 0.5239 - val_accuracy: 0.7485\n",
      "Epoch 105/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5299 - accuracy: 0.7413 - val_loss: 0.5241 - val_accuracy: 0.7482\n",
      "Epoch 106/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5275 - accuracy: 0.7430 - val_loss: 0.5240 - val_accuracy: 0.7468\n",
      "Epoch 107/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5260 - accuracy: 0.7470 - val_loss: 0.5243 - val_accuracy: 0.7471\n",
      "Epoch 108/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5275 - accuracy: 0.7407 - val_loss: 0.5230 - val_accuracy: 0.7478\n",
      "Epoch 109/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5278 - accuracy: 0.7445 - val_loss: 0.5236 - val_accuracy: 0.7489\n",
      "Epoch 110/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5319 - accuracy: 0.7412 - val_loss: 0.5238 - val_accuracy: 0.7482\n",
      "Epoch 111/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5278 - accuracy: 0.7410 - val_loss: 0.5235 - val_accuracy: 0.7443\n",
      "Epoch 112/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5285 - accuracy: 0.7356 - val_loss: 0.5237 - val_accuracy: 0.7457\n",
      "Epoch 113/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5266 - accuracy: 0.7433 - val_loss: 0.5236 - val_accuracy: 0.7482\n",
      "Epoch 114/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5277 - accuracy: 0.7445 - val_loss: 0.5241 - val_accuracy: 0.7450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5281 - accuracy: 0.7396 - val_loss: 0.5239 - val_accuracy: 0.7453\n",
      "Epoch 116/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5285 - accuracy: 0.7387 - val_loss: 0.5244 - val_accuracy: 0.7450\n",
      "Epoch 117/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5293 - accuracy: 0.7413 - val_loss: 0.5239 - val_accuracy: 0.7478\n",
      "Epoch 118/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5294 - accuracy: 0.7421 - val_loss: 0.5237 - val_accuracy: 0.7475\n",
      "Epoch 119/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5270 - accuracy: 0.7406 - val_loss: 0.5237 - val_accuracy: 0.7464\n",
      "Epoch 120/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5254 - accuracy: 0.7404 - val_loss: 0.5232 - val_accuracy: 0.7468\n",
      "Epoch 121/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5250 - accuracy: 0.7402 - val_loss: 0.5234 - val_accuracy: 0.7489\n",
      "Epoch 122/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5274 - accuracy: 0.7434 - val_loss: 0.5233 - val_accuracy: 0.7464\n",
      "Epoch 123/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5298 - accuracy: 0.7386 - val_loss: 0.5237 - val_accuracy: 0.7464\n",
      "Epoch 124/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5265 - accuracy: 0.7433 - val_loss: 0.5231 - val_accuracy: 0.7457\n",
      "Epoch 125/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5272 - accuracy: 0.7406 - val_loss: 0.5232 - val_accuracy: 0.7468\n",
      "Epoch 126/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5280 - accuracy: 0.7413 - val_loss: 0.5229 - val_accuracy: 0.7485\n",
      "Epoch 127/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5261 - accuracy: 0.7458 - val_loss: 0.5234 - val_accuracy: 0.7468\n",
      "Epoch 128/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5272 - accuracy: 0.7404 - val_loss: 0.5235 - val_accuracy: 0.7464\n",
      "Epoch 129/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5270 - accuracy: 0.7395 - val_loss: 0.5232 - val_accuracy: 0.7475\n",
      "Epoch 130/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5229 - accuracy: 0.7390 - val_loss: 0.5241 - val_accuracy: 0.7450\n",
      "Epoch 131/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5277 - accuracy: 0.7389 - val_loss: 0.5228 - val_accuracy: 0.7460\n",
      "Epoch 132/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5260 - accuracy: 0.7428 - val_loss: 0.5225 - val_accuracy: 0.7482\n",
      "Epoch 133/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5248 - accuracy: 0.7442 - val_loss: 0.5231 - val_accuracy: 0.7471\n",
      "Epoch 134/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5279 - accuracy: 0.7407 - val_loss: 0.5226 - val_accuracy: 0.7485\n",
      "Epoch 135/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5240 - accuracy: 0.7427 - val_loss: 0.5230 - val_accuracy: 0.7478\n",
      "Epoch 136/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5272 - accuracy: 0.7387 - val_loss: 0.5226 - val_accuracy: 0.7471\n",
      "Epoch 137/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5243 - accuracy: 0.7416 - val_loss: 0.5230 - val_accuracy: 0.7471\n",
      "Epoch 138/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5258 - accuracy: 0.7415 - val_loss: 0.5225 - val_accuracy: 0.7475\n",
      "Epoch 139/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5252 - accuracy: 0.7457 - val_loss: 0.5224 - val_accuracy: 0.7482\n",
      "Epoch 140/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5260 - accuracy: 0.7415 - val_loss: 0.5228 - val_accuracy: 0.7478\n",
      "Epoch 141/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5255 - accuracy: 0.7463 - val_loss: 0.5226 - val_accuracy: 0.7485\n",
      "Epoch 142/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5251 - accuracy: 0.7428 - val_loss: 0.5229 - val_accuracy: 0.7468\n",
      "Epoch 143/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5221 - accuracy: 0.7479 - val_loss: 0.5240 - val_accuracy: 0.7457\n",
      "Epoch 144/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5250 - accuracy: 0.7406 - val_loss: 0.5236 - val_accuracy: 0.7464\n",
      "Epoch 145/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5273 - accuracy: 0.7421 - val_loss: 0.5235 - val_accuracy: 0.7460\n",
      "Epoch 146/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5260 - accuracy: 0.7436 - val_loss: 0.5227 - val_accuracy: 0.7475\n",
      "Epoch 147/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5261 - accuracy: 0.7410 - val_loss: 0.5233 - val_accuracy: 0.7478\n",
      "Epoch 148/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5243 - accuracy: 0.7445 - val_loss: 0.5230 - val_accuracy: 0.7485\n",
      "Epoch 149/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5266 - accuracy: 0.7378 - val_loss: 0.5228 - val_accuracy: 0.7464\n",
      "Epoch 150/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5273 - accuracy: 0.7427 - val_loss: 0.5226 - val_accuracy: 0.7464\n",
      "Epoch 151/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5267 - accuracy: 0.7452 - val_loss: 0.5230 - val_accuracy: 0.7471\n",
      "Epoch 152/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5236 - accuracy: 0.7407 - val_loss: 0.5221 - val_accuracy: 0.7485\n",
      "Epoch 153/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5280 - accuracy: 0.7374 - val_loss: 0.5227 - val_accuracy: 0.7482\n",
      "Epoch 154/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5242 - accuracy: 0.7386 - val_loss: 0.5220 - val_accuracy: 0.7468\n",
      "Epoch 155/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5239 - accuracy: 0.7430 - val_loss: 0.5223 - val_accuracy: 0.7460\n",
      "Epoch 156/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5237 - accuracy: 0.7412 - val_loss: 0.5218 - val_accuracy: 0.7482\n",
      "Epoch 157/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5262 - accuracy: 0.7425 - val_loss: 0.5213 - val_accuracy: 0.7517\n",
      "Epoch 158/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5225 - accuracy: 0.7422 - val_loss: 0.5213 - val_accuracy: 0.7492\n",
      "Epoch 159/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5252 - accuracy: 0.7431 - val_loss: 0.5221 - val_accuracy: 0.7460\n",
      "Epoch 160/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5220 - accuracy: 0.7398 - val_loss: 0.5217 - val_accuracy: 0.7492\n",
      "Epoch 161/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5259 - accuracy: 0.7407 - val_loss: 0.5226 - val_accuracy: 0.7471\n",
      "Epoch 162/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5248 - accuracy: 0.7407 - val_loss: 0.5218 - val_accuracy: 0.7471\n",
      "Epoch 163/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5241 - accuracy: 0.7384 - val_loss: 0.5226 - val_accuracy: 0.7471\n",
      "Epoch 164/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5256 - accuracy: 0.7439 - val_loss: 0.5222 - val_accuracy: 0.7468\n",
      "Epoch 165/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5260 - accuracy: 0.7406 - val_loss: 0.5217 - val_accuracy: 0.7485\n",
      "Epoch 166/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5253 - accuracy: 0.7470 - val_loss: 0.5218 - val_accuracy: 0.7482\n",
      "Epoch 167/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5239 - accuracy: 0.7410 - val_loss: 0.5220 - val_accuracy: 0.7471\n",
      "Epoch 168/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5241 - accuracy: 0.7461 - val_loss: 0.5212 - val_accuracy: 0.7464\n",
      "Epoch 169/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5238 - accuracy: 0.7419 - val_loss: 0.5208 - val_accuracy: 0.7478\n",
      "Epoch 170/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5210 - accuracy: 0.7448 - val_loss: 0.5212 - val_accuracy: 0.7485\n",
      "Epoch 171/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5226 - accuracy: 0.7422 - val_loss: 0.5221 - val_accuracy: 0.7460\n",
      "Epoch 172/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5238 - accuracy: 0.7433 - val_loss: 0.5211 - val_accuracy: 0.7464\n",
      "Epoch 173/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5219 - accuracy: 0.7401 - val_loss: 0.5212 - val_accuracy: 0.7485\n",
      "Epoch 174/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5216 - accuracy: 0.7436 - val_loss: 0.5213 - val_accuracy: 0.7496\n",
      "Epoch 175/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5195 - accuracy: 0.7416 - val_loss: 0.5220 - val_accuracy: 0.7485\n",
      "Epoch 176/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5233 - accuracy: 0.7442 - val_loss: 0.5217 - val_accuracy: 0.7503\n",
      "Epoch 177/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5236 - accuracy: 0.7430 - val_loss: 0.5218 - val_accuracy: 0.7471\n",
      "Epoch 178/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5229 - accuracy: 0.7434 - val_loss: 0.5211 - val_accuracy: 0.7482\n",
      "Epoch 179/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5233 - accuracy: 0.7421 - val_loss: 0.5208 - val_accuracy: 0.7482\n",
      "Epoch 180/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5234 - accuracy: 0.7418 - val_loss: 0.5217 - val_accuracy: 0.7489\n",
      "Epoch 181/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5224 - accuracy: 0.7412 - val_loss: 0.5218 - val_accuracy: 0.7478\n",
      "Epoch 182/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5209 - accuracy: 0.7416 - val_loss: 0.5213 - val_accuracy: 0.7457\n",
      "Epoch 183/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5225 - accuracy: 0.7424 - val_loss: 0.5215 - val_accuracy: 0.7482\n",
      "Epoch 184/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5218 - accuracy: 0.7455 - val_loss: 0.5214 - val_accuracy: 0.7471\n",
      "Epoch 185/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5262 - accuracy: 0.7381 - val_loss: 0.5214 - val_accuracy: 0.7471\n",
      "Epoch 186/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5214 - accuracy: 0.7451 - val_loss: 0.5213 - val_accuracy: 0.7478\n",
      "Epoch 187/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5219 - accuracy: 0.7436 - val_loss: 0.5214 - val_accuracy: 0.7485\n",
      "Epoch 188/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5223 - accuracy: 0.7443 - val_loss: 0.5219 - val_accuracy: 0.7475\n",
      "Epoch 189/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5224 - accuracy: 0.7386 - val_loss: 0.5229 - val_accuracy: 0.7468\n",
      "Epoch 190/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5220 - accuracy: 0.7410 - val_loss: 0.5220 - val_accuracy: 0.7475\n",
      "Epoch 191/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5251 - accuracy: 0.7402 - val_loss: 0.5226 - val_accuracy: 0.7464\n",
      "Epoch 192/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5191 - accuracy: 0.7448 - val_loss: 0.5234 - val_accuracy: 0.7471\n",
      "Epoch 193/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5202 - accuracy: 0.7425 - val_loss: 0.5220 - val_accuracy: 0.7468\n",
      "Epoch 194/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5219 - accuracy: 0.7436 - val_loss: 0.5216 - val_accuracy: 0.7489\n",
      "Epoch 195/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5211 - accuracy: 0.7478 - val_loss: 0.5218 - val_accuracy: 0.7468\n",
      "Epoch 196/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5246 - accuracy: 0.7428 - val_loss: 0.5220 - val_accuracy: 0.7460\n",
      "Epoch 197/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5187 - accuracy: 0.7469 - val_loss: 0.5220 - val_accuracy: 0.7489\n",
      "Epoch 198/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5211 - accuracy: 0.7439 - val_loss: 0.5222 - val_accuracy: 0.7453\n",
      "Epoch 199/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5247 - accuracy: 0.7419 - val_loss: 0.5215 - val_accuracy: 0.7460\n",
      "Epoch 200/200\n",
      "208/208 [==============================] - 0s 1ms/step - loss: 0.5226 - accuracy: 0.7402 - val_loss: 0.5217 - val_accuracy: 0.7450\n",
      "297/297 [==============================] - 0s 1ms/step - loss: 0.5258 - accuracy: 0.7395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\myjr\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:430: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
      "  warnings.warn('`model.predict_proba()` is deprecated and '\n",
      "C:\\Users\\myjr\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:430: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
      "  warnings.warn('`model.predict_proba()` is deprecated and '\n",
      "C:\\Users\\myjr\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:430: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
      "  warnings.warn('`model.predict_proba()` is deprecated and '\n",
      "C:\\Users\\myjr\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:430: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
      "  warnings.warn('`model.predict_proba()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP train Brier score: 0.1704436991851747 \n",
      " test Brier score: 0.17458438374543317 \n",
      " AUROC: 0.6817159362498882\n",
      "Xgboost best layer size: {'colsample_bytree': 0.8, 'max_depth': 3, 'min_child_weight': 4, 'n_estimators': 100, 'reg_lambda': 1000, 'subsample': 0.9} \n",
      " best train score: -0.17219113427738023 \n",
      " test score: -0.1695792168784868 \n",
      " Xgboost train Brier score: 0.16168550375104063 \n",
      " test Brier score: 0.16957921690116723 \n",
      " AUROC: 0.7101540499895624\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression,LogisticRegression\n",
    "\n",
    "\n",
    "X_train1,X_test1,y_train1,y_test1=train_test_split(raw_extend.drop(['ID','Recidivism_Within_3years','Recidivism_Arrest_Year1','Recidivism_Arrest_Year2','Recidivism_Arrest_Year3'],axis=1),raw_extend['Recidivism_Arrest_Year2'])\n",
    "fpr_list,tpr_list,auc_list=dict(),dict(),dict()\n",
    "\n",
    "\n",
    "logistic=LogisticRegression(max_iter=1000)\n",
    "logistic.fit(X_train1.fillna(0),y_train1)\n",
    "fpr_list[0], tpr_list[0], _ = roc_curve(y_test1, y_roc(logistic,X_test1.fillna(0)))\n",
    "print('Logistic regression train score:',\n",
    "      logistic.score(X_train1.fillna(0),y_train1),'\\n test score:',logistic.score(X_test1.fillna(0),y_test1.fillna(0)),'\\n Logistic regression train Brier score:',\n",
    "      brier_score(logistic.predict_proba(X_train1.fillna(0)),y_train1),'\\n test Brier score:',brier_score(logistic.predict_proba(X_test1.fillna(0)),y_test1),\n",
    "     '\\n AUROC:',roc_auc_score(y_test1, y_roc(logistic,X_test1.fillna(0))))\n",
    "\n",
    "RF=RandomForestClassifier(n_estimators=150,min_samples_split=2)\n",
    "RF.fit(X_train1.fillna(0),y_train1)\n",
    "fpr_list[1], tpr_list[1], _ = roc_curve(y_test1, y_roc(RF,X_test1.fillna(0)))\n",
    "print('Random forest train score:',\n",
    "      RF.score(X_train1.fillna(0),y_train1),'\\n test score:',RF.score(X_test1.fillna(0),y_test1.fillna(0)),'\\n Random forest  train Brier score:',\n",
    "      brier_score(RF.predict_proba(X_train1.fillna(0)),y_train1),'\\n test Brier score:',brier_score(RF.predict_proba(X_test1.fillna(0)),y_test1),\n",
    "     '\\n AUROC:',roc_auc_score(y_test1, y_roc(RF,X_test1.fillna(0))))\n",
    "\n",
    "GBDT=GradientBoostingClassifier()\n",
    "params_SGD={'n_estimators':[150,100],'min_samples_split':[2,4]}\n",
    "grid_SGD=GridSearchCV(GBDT,param_grid=params_SGD, scoring='neg_brier_score',cv=3)\n",
    "grid_SGD.fit(X_train1.fillna(0),y_train1.fillna(0))\n",
    "fpr_list[2], tpr_list[2], _ = roc_curve(y_test1, y_roc(grid_SGD.best_estimator_,X_test1.fillna(0)))\n",
    "print('SGD best layer size:',grid_SGD.best_params_,'\\n best train score:',\n",
    "      grid_SGD.best_score_,'\\n test score:',grid_SGD.score(X_test1.fillna(0),y_test1.fillna(0)),'\\n SGD  train Brier score:',\n",
    "      brier_score(grid_SGD.predict_proba(X_train1.fillna(0)),y_train1),'\\n test Brier score:',brier_score(grid_SGD.predict_proba(X_test1.fillna(0)),y_test1),\n",
    "     '\\n AUROC:',roc_auc_score(y_test1, y_roc(grid_SGD.best_estimator_,X_test1.fillna(0))))\n",
    "\n",
    "pipe = Sequential()\n",
    "n_cols = X_train1.shape[1]\n",
    "pipe.add(BatchNormalization(input_shape=(n_cols,)))\n",
    "pipe.add(Dense(70, activation= 'linear'))\n",
    "pipe.add(Dropout(0.3))\n",
    "pipe.add(Dense(50, activation= 'relu'))\n",
    "pipe.add(Dropout(0.3))\n",
    "pipe.add(Dense(50, activation= 'relu'))\n",
    "pipe.add(Dropout(0.3))\n",
    "#model.add(Dense(100, activation='relu'))\n",
    "#model.add(Dropout(0.3))\n",
    "#model.add(Dense(100, activation='linear'))\n",
    "pipe.add(BatchNormalization())\n",
    "pipe.add(Dense(2, activation='softmax'))\n",
    "    #model.compile(\n",
    "        #optimizer='Adam',\n",
    "        #loss='mean_squared_error',\n",
    "        #metrics=['accuracy'])\n",
    "early_stopping_monitor = EarlyStopping(patience=50)\n",
    "sgd = keras.optimizers.SGD(lr=.001, decay=2e-4, momentum=0.9, nesterov=True)\n",
    "pipe.compile(loss = 'sparse_categorical_crossentropy',optimizer = 'sgd', metrics=['accuracy'])\n",
    "history=pipe.fit(X_train1.fillna(0).astype('float32'), y_train1, validation_split=0.3, epochs=200, callbacks=[early_stopping_monitor])\n",
    "#history=model.fit(X_train, y_train, validation_split=0.2, epochs=25)\n",
    "score = pipe.evaluate(X_test1.fillna(0).astype('float32'), y_test1, verbose=0)\n",
    "pipe.fit(X_train1.fillna(0).astype('float32'),y_train1.fillna(0))\n",
    "fpr_list[3], tpr_list[3], _ = roc_curve(y_test1, y_roc(pipe,X_test1.fillna(0).astype('float32')))\n",
    "print('MLP train Brier score:',\n",
    "      brier_score(pipe.predict_proba(X_train1.fillna(0).astype('float32')),y_train1),'\\n test Brier score:',brier_score(pipe.predict_proba(X_test1.fillna(0).astype('float32')),y_test1),\n",
    "     '\\n AUROC:',roc_auc_score(y_test1, y_roc(pipe,X_test1.fillna(0).astype('float32'))))\n",
    "\n",
    "\n",
    "XGB=XGBClassifier()\n",
    "params_XGB={    'n_estimators': [50, 100],\n",
    "    'max_depth': [2, 3],\n",
    "    'min_child_weight': [2,4,6],\n",
    "    'subsample': [0.9],\n",
    "    'colsample_bytree': [0.8],'reg_lambda':[1000]}\n",
    "grid_XGB=GridSearchCV(XGB,param_grid=params_XGB, scoring='neg_brier_score',cv=3)\n",
    "grid_XGB.fit(X_train1.fillna(0),y_train1.fillna(0))\n",
    "fpr_list[4], tpr_list[4], _ = roc_curve(y_test1, y_roc(grid_XGB.best_estimator_,X_test1.fillna(0)))\n",
    "print('Xgboost best layer size:',grid_XGB.best_params_,'\\n best train score:',\n",
    "      grid_XGB.best_score_,'\\n test score:',grid_XGB.score(X_test1.fillna(0),y_test1.fillna(0)),'\\n Xgboost train Brier score:',\n",
    "      brier_score(grid_XGB.predict_proba(X_train1.fillna(0)),y_train1),'\\n test Brier score:',brier_score(grid_XGB.predict_proba(X_test1.fillna(0)),y_test1),\n",
    "     '\\n AUROC:',roc_auc_score(y_test1, y_roc(grid_XGB.best_estimator_,X_test1.fillna(0))))\n",
    "\n",
    "\n",
    "\n",
    "colors = cycle(['aqua', 'red', 'green','orange','blue'])\n",
    "labels=['Logistic Regression','Random Forest','Gradient Boosting','MLP','Xgboost']\n",
    "for i, label, color in zip(range(len(fpr_list)), labels, colors):\n",
    "    legend= label + ' (area = {1:0.2f})'''.format(i, auc(fpr_list[i], tpr_list[i]))\n",
    "    plt.plot(fpr_list[i], tpr_list[i], color=color, lw=2,label=legend)\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC curve for different methods on Recidivism_1Year')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('ROC_plot.png',dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d82d63b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Percent_Days_Employed</td>\n",
       "      <td>0.186335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Jobs_Per_Year</td>\n",
       "      <td>0.146400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Prior_Arrest_Episodes_PPViolationCharges</td>\n",
       "      <td>0.071115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Age_at_Release</td>\n",
       "      <td>0.069156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gang_Affiliated</td>\n",
       "      <td>0.062296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>YBL</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>WATP</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>VEH</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>VALP</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>PUMA</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>201 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Feature  importance\n",
       "45                      Percent_Days_Employed    0.186335\n",
       "46                              Jobs_Per_Year    0.146400\n",
       "16   Prior_Arrest_Episodes_PPViolationCharges    0.071115\n",
       "2                              Age_at_Release    0.069156\n",
       "4                             Gang_Affiliated    0.062296\n",
       "..                                        ...         ...\n",
       "96                                        YBL    0.000000\n",
       "95                                       WATP    0.000000\n",
       "94                                        VEH    0.000000\n",
       "93                                       VALP    0.000000\n",
       "200                                      PUMA    0.000000\n",
       "\n",
       "[201 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GBDT_importance=pd.DataFrame()\n",
    "GBDT_importance['Feature']=raw_extend.drop(['ID','Recidivism_Within_3years','Recidivism_Arrest_Year1','Recidivism_Arrest_Year2','Recidivism_Arrest_Year3'],axis=1).columns\n",
    "GBDT_importance['importance']=grid_SGD.best_estimator_.feature_importances_\n",
    "GBDT_importance=GBDT_importance.sort_values(by='importance',ascending=False)\n",
    "GBDT_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ac59003",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>FSMOCP</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>FPARC</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>FINCP</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>FGRNTP</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>FFINCP</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>FES</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>YBL</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>WATP</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>VEH</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>VALP</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>PUMA</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Feature  importance\n",
       "103  FSMOCP         0.0\n",
       "102   FPARC         0.0\n",
       "101   FINCP         0.0\n",
       "99   FGRNTP         0.0\n",
       "98   FFINCP         0.0\n",
       "97      FES         0.0\n",
       "96      YBL         0.0\n",
       "95     WATP         0.0\n",
       "94      VEH         0.0\n",
       "93     VALP         0.0\n",
       "200    PUMA         0.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GBDT_importance.tail(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f0a0bcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>FVALP</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>FBUSP</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>FBROADBNDP</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>FTOILP</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>WORKSTAT</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>YBL</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>WATP</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>VEH</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>VALP</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>PUMA</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>95 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Feature  importance\n",
       "187       FVALP         0.0\n",
       "146       FBUSP         0.0\n",
       "145  FBROADBNDP         0.0\n",
       "185      FTOILP         0.0\n",
       "138    WORKSTAT         0.0\n",
       "..          ...         ...\n",
       "96          YBL         0.0\n",
       "95         WATP         0.0\n",
       "94          VEH         0.0\n",
       "93         VALP         0.0\n",
       "200        PUMA         0.0\n",
       "\n",
       "[95 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GBDT_importance[GBDT_importance.importance==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6654b20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_extend_remove=raw_extend.drop(list(GBDT_importance[GBDT_importance.importance==0].Feature),axis=1)\n",
    "raw_extend_remove=raw_extend_remove.drop(columns=['PUMA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd274977",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic_t regression train score: 0.7474704890387859 \n",
      " test score: 0.7413847613025608 \n",
      " logistic_t regression train Brier score: 0.17316677698794947 \n",
      " test Brier score: 0.1733842314181451 \n",
      " AUROC: 0.7061798958415116\n",
      "Random forest train score: 1.0 \n",
      " test score: 0.7407524502055011 \n",
      " Random forest  train Brier score: 0.024287961401535964 \n",
      " test Brier score: 0.17402142832051218 \n",
      " AUROC: 0.702015126534244\n",
      "SGD best layer size: {'min_samples_split': 4, 'n_estimators': 100} \n",
      " best train score: -0.17000461902078176 \n",
      " test score: -0.16864156061796565 \n",
      " SGD  train Brier score: 0.1525993611187537 \n",
      " test Brier score: 0.16864156061796584 \n",
      " AUROC: 0.7300798199471356\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\myjr\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208/208 [==============================] - 1s 3ms/step - loss: 0.7816 - accuracy: 0.6138 - val_loss: 0.5570 - val_accuracy: 0.7482\n",
      "Epoch 2/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.6229 - accuracy: 0.6916 - val_loss: 0.5526 - val_accuracy: 0.7492\n",
      "Epoch 3/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5940 - accuracy: 0.7198 - val_loss: 0.5504 - val_accuracy: 0.7496\n",
      "Epoch 4/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5758 - accuracy: 0.7336 - val_loss: 0.5493 - val_accuracy: 0.7492\n",
      "Epoch 5/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5712 - accuracy: 0.7366 - val_loss: 0.5490 - val_accuracy: 0.7489\n",
      "Epoch 6/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5679 - accuracy: 0.7378 - val_loss: 0.5482 - val_accuracy: 0.7499\n",
      "Epoch 7/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5670 - accuracy: 0.7401 - val_loss: 0.5481 - val_accuracy: 0.7496\n",
      "Epoch 8/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5635 - accuracy: 0.7387 - val_loss: 0.5467 - val_accuracy: 0.7499\n",
      "Epoch 9/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5616 - accuracy: 0.7413 - val_loss: 0.5452 - val_accuracy: 0.7503\n",
      "Epoch 10/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5614 - accuracy: 0.7393 - val_loss: 0.5442 - val_accuracy: 0.7499\n",
      "Epoch 11/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5612 - accuracy: 0.7396 - val_loss: 0.5435 - val_accuracy: 0.7510\n",
      "Epoch 12/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5601 - accuracy: 0.7409 - val_loss: 0.5418 - val_accuracy: 0.7503\n",
      "Epoch 13/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5607 - accuracy: 0.7412 - val_loss: 0.5426 - val_accuracy: 0.7499\n",
      "Epoch 14/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5610 - accuracy: 0.7384 - val_loss: 0.5413 - val_accuracy: 0.7503\n",
      "Epoch 15/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5573 - accuracy: 0.7390 - val_loss: 0.5405 - val_accuracy: 0.7499\n",
      "Epoch 16/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5612 - accuracy: 0.7399 - val_loss: 0.5398 - val_accuracy: 0.7496\n",
      "Epoch 17/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5550 - accuracy: 0.7386 - val_loss: 0.5371 - val_accuracy: 0.7492\n",
      "Epoch 18/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5544 - accuracy: 0.7350 - val_loss: 0.5372 - val_accuracy: 0.7492\n",
      "Epoch 19/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5557 - accuracy: 0.7387 - val_loss: 0.5359 - val_accuracy: 0.7496\n",
      "Epoch 20/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5541 - accuracy: 0.7374 - val_loss: 0.5369 - val_accuracy: 0.7503\n",
      "Epoch 21/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5523 - accuracy: 0.7398 - val_loss: 0.5352 - val_accuracy: 0.7496\n",
      "Epoch 22/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5479 - accuracy: 0.7406 - val_loss: 0.5334 - val_accuracy: 0.7496\n",
      "Epoch 23/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5549 - accuracy: 0.7380 - val_loss: 0.5359 - val_accuracy: 0.7489\n",
      "Epoch 24/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5498 - accuracy: 0.7407 - val_loss: 0.5333 - val_accuracy: 0.7492\n",
      "Epoch 25/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5510 - accuracy: 0.7390 - val_loss: 0.5319 - val_accuracy: 0.7468\n",
      "Epoch 26/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5491 - accuracy: 0.7389 - val_loss: 0.5319 - val_accuracy: 0.7475\n",
      "Epoch 27/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5500 - accuracy: 0.7393 - val_loss: 0.5323 - val_accuracy: 0.7499\n",
      "Epoch 28/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5487 - accuracy: 0.7410 - val_loss: 0.5316 - val_accuracy: 0.7489\n",
      "Epoch 29/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5487 - accuracy: 0.7404 - val_loss: 0.5322 - val_accuracy: 0.7482\n",
      "Epoch 30/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5476 - accuracy: 0.7401 - val_loss: 0.5308 - val_accuracy: 0.7475\n",
      "Epoch 31/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5476 - accuracy: 0.7398 - val_loss: 0.5321 - val_accuracy: 0.7489\n",
      "Epoch 32/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5465 - accuracy: 0.7393 - val_loss: 0.5300 - val_accuracy: 0.7485\n",
      "Epoch 33/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5475 - accuracy: 0.7387 - val_loss: 0.5290 - val_accuracy: 0.7475\n",
      "Epoch 34/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5454 - accuracy: 0.7387 - val_loss: 0.5276 - val_accuracy: 0.7482\n",
      "Epoch 35/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5445 - accuracy: 0.7410 - val_loss: 0.5284 - val_accuracy: 0.7468\n",
      "Epoch 36/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5444 - accuracy: 0.7366 - val_loss: 0.5289 - val_accuracy: 0.7471\n",
      "Epoch 37/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5440 - accuracy: 0.7406 - val_loss: 0.5293 - val_accuracy: 0.7482\n",
      "Epoch 38/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5429 - accuracy: 0.7402 - val_loss: 0.5274 - val_accuracy: 0.7457\n",
      "Epoch 39/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5430 - accuracy: 0.7372 - val_loss: 0.5283 - val_accuracy: 0.7460\n",
      "Epoch 40/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5441 - accuracy: 0.7380 - val_loss: 0.5272 - val_accuracy: 0.7482\n",
      "Epoch 41/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5406 - accuracy: 0.7378 - val_loss: 0.5260 - val_accuracy: 0.7460\n",
      "Epoch 42/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5427 - accuracy: 0.7418 - val_loss: 0.5268 - val_accuracy: 0.7485\n",
      "Epoch 43/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5416 - accuracy: 0.7402 - val_loss: 0.5265 - val_accuracy: 0.7464\n",
      "Epoch 44/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5446 - accuracy: 0.7392 - val_loss: 0.5273 - val_accuracy: 0.7464\n",
      "Epoch 45/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5386 - accuracy: 0.7416 - val_loss: 0.5264 - val_accuracy: 0.7457\n",
      "Epoch 46/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5429 - accuracy: 0.7390 - val_loss: 0.5258 - val_accuracy: 0.7460\n",
      "Epoch 47/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5418 - accuracy: 0.7431 - val_loss: 0.5263 - val_accuracy: 0.7464\n",
      "Epoch 48/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5384 - accuracy: 0.7402 - val_loss: 0.5250 - val_accuracy: 0.7460\n",
      "Epoch 49/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5397 - accuracy: 0.7402 - val_loss: 0.5251 - val_accuracy: 0.7475\n",
      "Epoch 50/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5418 - accuracy: 0.7428 - val_loss: 0.5263 - val_accuracy: 0.7478\n",
      "Epoch 51/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5403 - accuracy: 0.7413 - val_loss: 0.5242 - val_accuracy: 0.7475\n",
      "Epoch 52/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5387 - accuracy: 0.7415 - val_loss: 0.5247 - val_accuracy: 0.7471\n",
      "Epoch 53/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5389 - accuracy: 0.7427 - val_loss: 0.5244 - val_accuracy: 0.7460\n",
      "Epoch 54/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5384 - accuracy: 0.7425 - val_loss: 0.5245 - val_accuracy: 0.7446\n",
      "Epoch 55/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5363 - accuracy: 0.7413 - val_loss: 0.5232 - val_accuracy: 0.7446\n",
      "Epoch 56/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5391 - accuracy: 0.7430 - val_loss: 0.5235 - val_accuracy: 0.7453\n",
      "Epoch 57/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5365 - accuracy: 0.7399 - val_loss: 0.5240 - val_accuracy: 0.7468\n",
      "Epoch 58/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5357 - accuracy: 0.7443 - val_loss: 0.5234 - val_accuracy: 0.7464\n",
      "Epoch 59/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5354 - accuracy: 0.7419 - val_loss: 0.5239 - val_accuracy: 0.7471\n",
      "Epoch 60/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5369 - accuracy: 0.7413 - val_loss: 0.5237 - val_accuracy: 0.7471\n",
      "Epoch 61/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5345 - accuracy: 0.7395 - val_loss: 0.5247 - val_accuracy: 0.7492\n",
      "Epoch 62/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5374 - accuracy: 0.7415 - val_loss: 0.5231 - val_accuracy: 0.7475\n",
      "Epoch 63/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5374 - accuracy: 0.7434 - val_loss: 0.5228 - val_accuracy: 0.7457\n",
      "Epoch 64/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5341 - accuracy: 0.7458 - val_loss: 0.5232 - val_accuracy: 0.7468\n",
      "Epoch 65/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5323 - accuracy: 0.7390 - val_loss: 0.5235 - val_accuracy: 0.7460\n",
      "Epoch 66/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5352 - accuracy: 0.7428 - val_loss: 0.5229 - val_accuracy: 0.7457\n",
      "Epoch 67/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5387 - accuracy: 0.7381 - val_loss: 0.5240 - val_accuracy: 0.7464\n",
      "Epoch 68/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5362 - accuracy: 0.7455 - val_loss: 0.5228 - val_accuracy: 0.7468\n",
      "Epoch 69/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5387 - accuracy: 0.7398 - val_loss: 0.5226 - val_accuracy: 0.7482\n",
      "Epoch 70/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5373 - accuracy: 0.7421 - val_loss: 0.5229 - val_accuracy: 0.7457\n",
      "Epoch 71/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5360 - accuracy: 0.7390 - val_loss: 0.5219 - val_accuracy: 0.7475\n",
      "Epoch 72/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5348 - accuracy: 0.7458 - val_loss: 0.5217 - val_accuracy: 0.7485\n",
      "Epoch 73/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5328 - accuracy: 0.7433 - val_loss: 0.5226 - val_accuracy: 0.7468\n",
      "Epoch 74/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5349 - accuracy: 0.7424 - val_loss: 0.5222 - val_accuracy: 0.7464\n",
      "Epoch 75/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5319 - accuracy: 0.7412 - val_loss: 0.5209 - val_accuracy: 0.7485\n",
      "Epoch 76/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5310 - accuracy: 0.7439 - val_loss: 0.5212 - val_accuracy: 0.7471\n",
      "Epoch 77/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5342 - accuracy: 0.7427 - val_loss: 0.5211 - val_accuracy: 0.7485\n",
      "Epoch 78/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5323 - accuracy: 0.7380 - val_loss: 0.5217 - val_accuracy: 0.7482\n",
      "Epoch 79/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5316 - accuracy: 0.7433 - val_loss: 0.5213 - val_accuracy: 0.7468\n",
      "Epoch 80/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5340 - accuracy: 0.7433 - val_loss: 0.5219 - val_accuracy: 0.7496\n",
      "Epoch 81/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5340 - accuracy: 0.7436 - val_loss: 0.5218 - val_accuracy: 0.7468\n",
      "Epoch 82/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5332 - accuracy: 0.7428 - val_loss: 0.5215 - val_accuracy: 0.7464\n",
      "Epoch 83/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5343 - accuracy: 0.7402 - val_loss: 0.5213 - val_accuracy: 0.7475\n",
      "Epoch 84/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5314 - accuracy: 0.7451 - val_loss: 0.5214 - val_accuracy: 0.7460\n",
      "Epoch 85/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5333 - accuracy: 0.7437 - val_loss: 0.5216 - val_accuracy: 0.7457\n",
      "Epoch 86/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5294 - accuracy: 0.7458 - val_loss: 0.5215 - val_accuracy: 0.7478\n",
      "Epoch 87/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5346 - accuracy: 0.7413 - val_loss: 0.5210 - val_accuracy: 0.7468\n",
      "Epoch 88/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5320 - accuracy: 0.7451 - val_loss: 0.5210 - val_accuracy: 0.7471\n",
      "Epoch 89/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5318 - accuracy: 0.7436 - val_loss: 0.5212 - val_accuracy: 0.7475\n",
      "Epoch 90/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5322 - accuracy: 0.7431 - val_loss: 0.5212 - val_accuracy: 0.7478\n",
      "Epoch 91/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5306 - accuracy: 0.7443 - val_loss: 0.5206 - val_accuracy: 0.7468\n",
      "Epoch 92/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5322 - accuracy: 0.7404 - val_loss: 0.5216 - val_accuracy: 0.7464\n",
      "Epoch 93/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5350 - accuracy: 0.7440 - val_loss: 0.5210 - val_accuracy: 0.7475\n",
      "Epoch 94/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5319 - accuracy: 0.7387 - val_loss: 0.5213 - val_accuracy: 0.7475\n",
      "Epoch 95/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5340 - accuracy: 0.7418 - val_loss: 0.5218 - val_accuracy: 0.7485\n",
      "Epoch 96/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5356 - accuracy: 0.7439 - val_loss: 0.5219 - val_accuracy: 0.7460\n",
      "Epoch 97/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5308 - accuracy: 0.7433 - val_loss: 0.5215 - val_accuracy: 0.7468\n",
      "Epoch 98/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5333 - accuracy: 0.7381 - val_loss: 0.5213 - val_accuracy: 0.7464\n",
      "Epoch 99/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5341 - accuracy: 0.7448 - val_loss: 0.5213 - val_accuracy: 0.7471\n",
      "Epoch 100/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5328 - accuracy: 0.7415 - val_loss: 0.5205 - val_accuracy: 0.7446\n",
      "Epoch 101/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5321 - accuracy: 0.7421 - val_loss: 0.5205 - val_accuracy: 0.7464\n",
      "Epoch 102/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5299 - accuracy: 0.7476 - val_loss: 0.5202 - val_accuracy: 0.7471\n",
      "Epoch 103/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5277 - accuracy: 0.7430 - val_loss: 0.5216 - val_accuracy: 0.7506\n",
      "Epoch 104/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5289 - accuracy: 0.7457 - val_loss: 0.5212 - val_accuracy: 0.7492\n",
      "Epoch 105/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5308 - accuracy: 0.7422 - val_loss: 0.5208 - val_accuracy: 0.7464\n",
      "Epoch 106/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5302 - accuracy: 0.7428 - val_loss: 0.5212 - val_accuracy: 0.7460\n",
      "Epoch 107/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5317 - accuracy: 0.7442 - val_loss: 0.5218 - val_accuracy: 0.7464\n",
      "Epoch 108/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5320 - accuracy: 0.7431 - val_loss: 0.5210 - val_accuracy: 0.7471\n",
      "Epoch 109/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5307 - accuracy: 0.7424 - val_loss: 0.5206 - val_accuracy: 0.7450\n",
      "Epoch 110/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5323 - accuracy: 0.7404 - val_loss: 0.5205 - val_accuracy: 0.7485\n",
      "Epoch 111/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5310 - accuracy: 0.7401 - val_loss: 0.5202 - val_accuracy: 0.7475\n",
      "Epoch 112/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5319 - accuracy: 0.7410 - val_loss: 0.5206 - val_accuracy: 0.7468\n",
      "Epoch 113/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5300 - accuracy: 0.7425 - val_loss: 0.5203 - val_accuracy: 0.7460\n",
      "Epoch 114/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5308 - accuracy: 0.7419 - val_loss: 0.5202 - val_accuracy: 0.7485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5288 - accuracy: 0.7389 - val_loss: 0.5202 - val_accuracy: 0.7460\n",
      "Epoch 116/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5314 - accuracy: 0.7425 - val_loss: 0.5204 - val_accuracy: 0.7475\n",
      "Epoch 117/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5305 - accuracy: 0.7431 - val_loss: 0.5211 - val_accuracy: 0.7443\n",
      "Epoch 118/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5287 - accuracy: 0.7436 - val_loss: 0.5209 - val_accuracy: 0.7489\n",
      "Epoch 119/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5311 - accuracy: 0.7446 - val_loss: 0.5208 - val_accuracy: 0.7464\n",
      "Epoch 120/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5289 - accuracy: 0.7439 - val_loss: 0.5209 - val_accuracy: 0.7464\n",
      "Epoch 121/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5294 - accuracy: 0.7430 - val_loss: 0.5208 - val_accuracy: 0.7475\n",
      "Epoch 122/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5278 - accuracy: 0.7410 - val_loss: 0.5209 - val_accuracy: 0.7499\n",
      "Epoch 123/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5271 - accuracy: 0.7460 - val_loss: 0.5211 - val_accuracy: 0.7496\n",
      "Epoch 124/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5290 - accuracy: 0.7418 - val_loss: 0.5201 - val_accuracy: 0.7492\n",
      "Epoch 125/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5314 - accuracy: 0.7398 - val_loss: 0.5201 - val_accuracy: 0.7446\n",
      "Epoch 126/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5287 - accuracy: 0.7434 - val_loss: 0.5208 - val_accuracy: 0.7468\n",
      "Epoch 127/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5305 - accuracy: 0.7419 - val_loss: 0.5210 - val_accuracy: 0.7460\n",
      "Epoch 128/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5261 - accuracy: 0.7467 - val_loss: 0.5208 - val_accuracy: 0.7471\n",
      "Epoch 129/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5287 - accuracy: 0.7415 - val_loss: 0.5207 - val_accuracy: 0.7460\n",
      "Epoch 130/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5304 - accuracy: 0.7410 - val_loss: 0.5204 - val_accuracy: 0.7453\n",
      "Epoch 131/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5273 - accuracy: 0.7455 - val_loss: 0.5201 - val_accuracy: 0.7471\n",
      "Epoch 132/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5322 - accuracy: 0.7390 - val_loss: 0.5204 - val_accuracy: 0.7460\n",
      "Epoch 133/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5266 - accuracy: 0.7460 - val_loss: 0.5204 - val_accuracy: 0.7464\n",
      "Epoch 134/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5285 - accuracy: 0.7406 - val_loss: 0.5204 - val_accuracy: 0.7453\n",
      "Epoch 135/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5304 - accuracy: 0.7404 - val_loss: 0.5204 - val_accuracy: 0.7468\n",
      "Epoch 136/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5305 - accuracy: 0.7419 - val_loss: 0.5201 - val_accuracy: 0.7468\n",
      "Epoch 137/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5303 - accuracy: 0.7401 - val_loss: 0.5201 - val_accuracy: 0.7471\n",
      "Epoch 138/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5292 - accuracy: 0.7395 - val_loss: 0.5206 - val_accuracy: 0.7460\n",
      "Epoch 139/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5276 - accuracy: 0.7406 - val_loss: 0.5203 - val_accuracy: 0.7464\n",
      "Epoch 140/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5273 - accuracy: 0.7472 - val_loss: 0.5206 - val_accuracy: 0.7478\n",
      "Epoch 141/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5287 - accuracy: 0.7422 - val_loss: 0.5207 - val_accuracy: 0.7492\n",
      "Epoch 142/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5311 - accuracy: 0.7419 - val_loss: 0.5207 - val_accuracy: 0.7471\n",
      "Epoch 143/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5300 - accuracy: 0.7410 - val_loss: 0.5203 - val_accuracy: 0.7496\n",
      "Epoch 144/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5309 - accuracy: 0.7412 - val_loss: 0.5212 - val_accuracy: 0.7468\n",
      "Epoch 145/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5253 - accuracy: 0.7452 - val_loss: 0.5209 - val_accuracy: 0.7460\n",
      "Epoch 146/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5266 - accuracy: 0.7424 - val_loss: 0.5203 - val_accuracy: 0.7468\n",
      "Epoch 147/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5291 - accuracy: 0.7445 - val_loss: 0.5205 - val_accuracy: 0.7478\n",
      "Epoch 148/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5283 - accuracy: 0.7409 - val_loss: 0.5202 - val_accuracy: 0.7450\n",
      "Epoch 149/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5276 - accuracy: 0.7419 - val_loss: 0.5200 - val_accuracy: 0.7453\n",
      "Epoch 150/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5294 - accuracy: 0.7418 - val_loss: 0.5201 - val_accuracy: 0.7485\n",
      "Epoch 151/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5261 - accuracy: 0.7455 - val_loss: 0.5200 - val_accuracy: 0.7489\n",
      "Epoch 152/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5296 - accuracy: 0.7418 - val_loss: 0.5207 - val_accuracy: 0.7457\n",
      "Epoch 153/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5282 - accuracy: 0.7437 - val_loss: 0.5199 - val_accuracy: 0.7471\n",
      "Epoch 154/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5304 - accuracy: 0.7398 - val_loss: 0.5206 - val_accuracy: 0.7460\n",
      "Epoch 155/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5240 - accuracy: 0.7437 - val_loss: 0.5203 - val_accuracy: 0.7468\n",
      "Epoch 156/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5266 - accuracy: 0.7455 - val_loss: 0.5203 - val_accuracy: 0.7457\n",
      "Epoch 157/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5258 - accuracy: 0.7464 - val_loss: 0.5213 - val_accuracy: 0.7471\n",
      "Epoch 158/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5316 - accuracy: 0.7424 - val_loss: 0.5203 - val_accuracy: 0.7457\n",
      "Epoch 159/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5292 - accuracy: 0.7433 - val_loss: 0.5206 - val_accuracy: 0.7482\n",
      "Epoch 160/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5279 - accuracy: 0.7443 - val_loss: 0.5204 - val_accuracy: 0.7471\n",
      "Epoch 161/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5317 - accuracy: 0.7416 - val_loss: 0.5205 - val_accuracy: 0.7478\n",
      "Epoch 162/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5240 - accuracy: 0.7445 - val_loss: 0.5207 - val_accuracy: 0.7471\n",
      "Epoch 163/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5303 - accuracy: 0.7449 - val_loss: 0.5203 - val_accuracy: 0.7492\n",
      "Epoch 164/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5238 - accuracy: 0.7440 - val_loss: 0.5196 - val_accuracy: 0.7492\n",
      "Epoch 165/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5261 - accuracy: 0.7449 - val_loss: 0.5204 - val_accuracy: 0.7464\n",
      "Epoch 166/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5301 - accuracy: 0.7415 - val_loss: 0.5195 - val_accuracy: 0.7471\n",
      "Epoch 167/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5256 - accuracy: 0.7404 - val_loss: 0.5201 - val_accuracy: 0.7464\n",
      "Epoch 168/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5265 - accuracy: 0.7458 - val_loss: 0.5209 - val_accuracy: 0.7457\n",
      "Epoch 169/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5273 - accuracy: 0.7440 - val_loss: 0.5201 - val_accuracy: 0.7478\n",
      "Epoch 170/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5278 - accuracy: 0.7419 - val_loss: 0.5209 - val_accuracy: 0.7450\n",
      "Epoch 171/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5263 - accuracy: 0.7446 - val_loss: 0.5196 - val_accuracy: 0.7482\n",
      "Epoch 172/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5273 - accuracy: 0.7424 - val_loss: 0.5198 - val_accuracy: 0.7457\n",
      "Epoch 173/200\n",
      "208/208 [==============================] - 1s 2ms/step - loss: 0.5245 - accuracy: 0.7437 - val_loss: 0.5208 - val_accuracy: 0.7464\n",
      "Epoch 174/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5274 - accuracy: 0.7442 - val_loss: 0.5206 - val_accuracy: 0.7468\n",
      "Epoch 175/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5269 - accuracy: 0.7428 - val_loss: 0.5201 - val_accuracy: 0.7471\n",
      "Epoch 176/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5242 - accuracy: 0.7431 - val_loss: 0.5201 - val_accuracy: 0.7517\n",
      "Epoch 177/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5284 - accuracy: 0.7424 - val_loss: 0.5202 - val_accuracy: 0.7468\n",
      "Epoch 178/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5264 - accuracy: 0.7424 - val_loss: 0.5204 - val_accuracy: 0.7468\n",
      "Epoch 179/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5247 - accuracy: 0.7452 - val_loss: 0.5202 - val_accuracy: 0.7489\n",
      "Epoch 180/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5261 - accuracy: 0.7402 - val_loss: 0.5194 - val_accuracy: 0.7478\n",
      "Epoch 181/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5254 - accuracy: 0.7451 - val_loss: 0.5210 - val_accuracy: 0.7460\n",
      "Epoch 182/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5252 - accuracy: 0.7455 - val_loss: 0.5203 - val_accuracy: 0.7485\n",
      "Epoch 183/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5254 - accuracy: 0.7443 - val_loss: 0.5205 - val_accuracy: 0.7460\n",
      "Epoch 184/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5261 - accuracy: 0.7439 - val_loss: 0.5201 - val_accuracy: 0.7468\n",
      "Epoch 185/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5281 - accuracy: 0.7413 - val_loss: 0.5196 - val_accuracy: 0.7468\n",
      "Epoch 186/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5266 - accuracy: 0.7409 - val_loss: 0.5200 - val_accuracy: 0.7460\n",
      "Epoch 187/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5265 - accuracy: 0.7418 - val_loss: 0.5197 - val_accuracy: 0.7471\n",
      "Epoch 188/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5257 - accuracy: 0.7398 - val_loss: 0.5202 - val_accuracy: 0.7485\n",
      "Epoch 189/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5249 - accuracy: 0.7451 - val_loss: 0.5202 - val_accuracy: 0.7489\n",
      "Epoch 190/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5265 - accuracy: 0.7428 - val_loss: 0.5202 - val_accuracy: 0.7499\n",
      "Epoch 191/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5256 - accuracy: 0.7476 - val_loss: 0.5205 - val_accuracy: 0.7464\n",
      "Epoch 192/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5268 - accuracy: 0.7422 - val_loss: 0.5206 - val_accuracy: 0.7492\n",
      "Epoch 193/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5239 - accuracy: 0.7431 - val_loss: 0.5197 - val_accuracy: 0.7468\n",
      "Epoch 194/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5262 - accuracy: 0.7409 - val_loss: 0.5199 - val_accuracy: 0.7468\n",
      "Epoch 195/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5239 - accuracy: 0.7404 - val_loss: 0.5192 - val_accuracy: 0.7468\n",
      "Epoch 196/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5270 - accuracy: 0.7410 - val_loss: 0.5192 - val_accuracy: 0.7485\n",
      "Epoch 197/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5248 - accuracy: 0.7407 - val_loss: 0.5192 - val_accuracy: 0.7485\n",
      "Epoch 198/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5259 - accuracy: 0.7412 - val_loss: 0.5202 - val_accuracy: 0.7464\n",
      "Epoch 199/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5258 - accuracy: 0.7404 - val_loss: 0.5198 - val_accuracy: 0.7460\n",
      "Epoch 200/200\n",
      "208/208 [==============================] - 0s 2ms/step - loss: 0.5294 - accuracy: 0.7443 - val_loss: 0.5206 - val_accuracy: 0.7468\n",
      "297/297 [==============================] - 0s 2ms/step - loss: 0.5238 - accuracy: 0.7506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\myjr\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:430: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
      "  warnings.warn('`model.predict_proba()` is deprecated and '\n",
      "C:\\Users\\myjr\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:430: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
      "  warnings.warn('`model.predict_proba()` is deprecated and '\n",
      "C:\\Users\\myjr\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:430: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
      "  warnings.warn('`model.predict_proba()` is deprecated and '\n",
      "C:\\Users\\myjr\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:430: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
      "  warnings.warn('`model.predict_proba()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP train Brier score: 0.17112599446814658 \n",
      " test Brier score: 0.17388917553753533 \n",
      " AUROC: 0.7044222867760592\n",
      "Xgboost best layer size: {'colsample_bytree': 0.8, 'max_depth': 3, 'min_child_weight': 4, 'n_estimators': 100, 'reg_lambda': 1000, 'subsample': 0.9} \n",
      " best train score: -0.17122557885284376 \n",
      " test score: -0.16920286951037405 \n",
      " Xgboost train Brier score: 0.16209568199803753 \n",
      " test Brier score: 0.16920286941518003 \n",
      " AUROC: 0.7280128758734395\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression,LogisticRegression\n",
    "\n",
    "\n",
    "X_train1,X_test1,y_train1,y_test1=train_test_split(raw_extend_remove.drop(['ID','Recidivism_Within_3years','Recidivism_Arrest_Year1','Recidivism_Arrest_Year2','Recidivism_Arrest_Year3'],axis=1),raw_extend_remove['Recidivism_Arrest_Year2'])\n",
    "fpr_list,tpr_list,auc_list=dict(),dict(),dict()\n",
    "\n",
    "\n",
    "logistic_t=LogisticRegression(max_iter=1000)\n",
    "logistic_t.fit(X_train1.fillna(0),y_train1)\n",
    "fpr_list[0], tpr_list[0], _ = roc_curve(y_test1, y_roc(logistic_t,X_test1.fillna(0)))\n",
    "print('logistic_t regression train score:',\n",
    "      logistic_t.score(X_train1.fillna(0),y_train1),'\\n test score:',logistic_t.score(X_test1.fillna(0),y_test1.fillna(0)),'\\n logistic_t regression train Brier score:',\n",
    "      brier_score(logistic_t.predict_proba(X_train1.fillna(0)),y_train1),'\\n test Brier score:',brier_score(logistic_t.predict_proba(X_test1.fillna(0)),y_test1),\n",
    "     '\\n AUROC:',roc_auc_score(y_test1, y_roc(logistic_t,X_test1.fillna(0))))\n",
    "\n",
    "RF=RandomForestClassifier(n_estimators=150,min_samples_split=2)\n",
    "RF.fit(X_train1.fillna(0),y_train1)\n",
    "fpr_list[1], tpr_list[1], _ = roc_curve(y_test1, y_roc(RF,X_test1.fillna(0)))\n",
    "print('Random forest train score:',\n",
    "      RF.score(X_train1.fillna(0),y_train1),'\\n test score:',RF.score(X_test1.fillna(0),y_test1.fillna(0)),'\\n Random forest  train Brier score:',\n",
    "      brier_score(RF.predict_proba(X_train1.fillna(0)),y_train1),'\\n test Brier score:',brier_score(RF.predict_proba(X_test1.fillna(0)),y_test1),\n",
    "     '\\n AUROC:',roc_auc_score(y_test1, y_roc(RF,X_test1.fillna(0))))\n",
    "\n",
    "GBDT=GradientBoostingClassifier()\n",
    "params_SGD={'n_estimators':[150,100],'min_samples_split':[2,4]}\n",
    "grid_SGD_t=GridSearchCV(GBDT,param_grid=params_SGD, scoring='neg_brier_score',cv=3)\n",
    "grid_SGD_t.fit(X_train1.fillna(0),y_train1.fillna(0))\n",
    "fpr_list[2], tpr_list[2], _ = roc_curve(y_test1, y_roc(grid_SGD_t.best_estimator_,X_test1.fillna(0)))\n",
    "print('SGD best layer size:',grid_SGD_t.best_params_,'\\n best train score:',\n",
    "      grid_SGD_t.best_score_,'\\n test score:',grid_SGD_t.score(X_test1.fillna(0),y_test1.fillna(0)),'\\n SGD  train Brier score:',\n",
    "      brier_score(grid_SGD_t.predict_proba(X_train1.fillna(0)),y_train1),'\\n test Brier score:',brier_score(grid_SGD_t.predict_proba(X_test1.fillna(0)),y_test1),\n",
    "     '\\n AUROC:',roc_auc_score(y_test1, y_roc(grid_SGD_t.best_estimator_,X_test1.fillna(0))))\n",
    "\n",
    "pipe_t = Sequential()\n",
    "n_cols = X_train1.shape[1]\n",
    "pipe_t.add(BatchNormalization(input_shape=(n_cols,)))\n",
    "pipe_t.add(Dense(70, activation= 'linear'))\n",
    "pipe_t.add(Dropout(0.3))\n",
    "pipe_t.add(Dense(50, activation= 'relu'))\n",
    "pipe_t.add(Dropout(0.3))\n",
    "pipe_t.add(Dense(50, activation= 'relu'))\n",
    "pipe_t.add(Dropout(0.3))\n",
    "#model.add(Dense(100, activation='relu'))\n",
    "#model.add(Dropout(0.3))\n",
    "#model.add(Dense(100, activation='linear'))\n",
    "pipe_t.add(BatchNormalization())\n",
    "pipe_t.add(Dense(2, activation='softmax'))\n",
    "    #model.compile(\n",
    "        #optimizer='Adam',\n",
    "        #loss='mean_squared_error',\n",
    "        #metrics=['accuracy'])\n",
    "early_stopping_monitor = EarlyStopping(patience=50)\n",
    "sgd = keras.optimizers.SGD(lr=.001, decay=2e-4, momentum=0.9, nesterov=True)\n",
    "pipe_t.compile(loss = 'sparse_categorical_crossentropy',optimizer = 'sgd', metrics=['accuracy'])\n",
    "history=pipe_t.fit(X_train1.fillna(0).astype('float32'), y_train1, validation_split=0.3, epochs=200, callbacks=[early_stopping_monitor])\n",
    "#history=model.fit(X_train, y_train, validation_split=0.2, epochs=25)\n",
    "score = pipe_t.evaluate(X_test1.fillna(0).astype('float32'), y_test1, verbose=0)\n",
    "pipe_t.fit(X_train1.fillna(0).astype('float32'),y_train1.fillna(0))\n",
    "fpr_list[3], tpr_list[3], _ = roc_curve(y_test1, y_roc(pipe_t,X_test1.fillna(0).astype('float32')))\n",
    "print('MLP train Brier score:',\n",
    "      brier_score(pipe_t.predict_proba(X_train1.fillna(0).astype('float32')),y_train1),'\\n test Brier score:',brier_score(pipe_t.predict_proba(X_test1.fillna(0).astype('float32')),y_test1),\n",
    "     '\\n AUROC:',roc_auc_score(y_test1, y_roc(pipe_t,X_test1.fillna(0).astype('float32'))))\n",
    "\n",
    "\n",
    "XGB=XGBClassifier()\n",
    "params_XGB={    'n_estimators': [50, 100],\n",
    "    'max_depth': [2, 3],\n",
    "    'min_child_weight': [2,4,6],\n",
    "    'subsample': [0.9],\n",
    "    'colsample_bytree': [0.8],'reg_lambda':[1000]}\n",
    "grid_XGB_t=GridSearchCV(XGB,param_grid=params_XGB, scoring='neg_brier_score',cv=3)\n",
    "grid_XGB_t.fit(X_train1.fillna(0),y_train1.fillna(0))\n",
    "fpr_list[4], tpr_list[4], _ = roc_curve(y_test1, y_roc(grid_XGB_t.best_estimator_,X_test1.fillna(0)))\n",
    "print('Xgboost best layer size:',grid_XGB_t.best_params_,'\\n best train score:',\n",
    "      grid_XGB_t.best_score_,'\\n test score:',grid_XGB_t.score(X_test1.fillna(0),y_test1.fillna(0)),'\\n Xgboost train Brier score:',\n",
    "      brier_score(grid_XGB_t.predict_proba(X_train1.fillna(0)),y_train1),'\\n test Brier score:',brier_score(grid_XGB_t.predict_proba(X_test1.fillna(0)),y_test1),\n",
    "     '\\n AUROC:',roc_auc_score(y_test1, y_roc(grid_XGB_t.best_estimator_,X_test1.fillna(0))))\n",
    "\n",
    "\n",
    "\n",
    "colors = cycle(['aqua', 'red', 'green','orange','blue'])\n",
    "labels=['logistic_t Regression','Random Forest','Gradient Boosting','MLP','Xgboost']\n",
    "for i, label, color in zip(range(len(fpr_list)), labels, colors):\n",
    "    legend= label + ' (area = {1:0.2f})'''.format(i, auc(fpr_list[i], tpr_list[i]))\n",
    "    plt.plot(fpr_list[i], tpr_list[i], color=color, lw=2,label=legend)\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC curve for different methods on Recidivism_1Year')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('ROC_plot.png',dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7834d144",
   "metadata": {},
   "source": [
    "### Year2 Recidivism\n",
    "When training, we need to discard those who recommit crime in 1st year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80c105e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'raw_extend_remove' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-3192035a00fb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mLogisticRegression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mraw_extend_year2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mraw_extend_remove\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_extend_remove\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mraw_extend_remove\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRecidivism_Arrest_Year1\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mX_train1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_test1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_extend_year2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ID'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Recidivism_Within_3years'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Recidivism_Arrest_Year1'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Recidivism_Arrest_Year2'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Recidivism_Arrest_Year3'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mraw_extend_year2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Recidivism_Arrest_Year2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mfpr_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtpr_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mauc_list\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'raw_extend_remove' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression,LogisticRegression\n",
    "raw_extend_year2=raw_extend_remove.drop(raw_extend_remove[raw_extend_remove.Recidivism_Arrest_Year1==True].index)\n",
    "\n",
    "X_train1,X_test1,y_train1,y_test1=train_test_split(raw_extend_year2.drop(['ID','Recidivism_Within_3years','Recidivism_Arrest_Year1','Recidivism_Arrest_Year2','Recidivism_Arrest_Year3'],axis=1),raw_extend_year2['Recidivism_Arrest_Year2'])\n",
    "fpr_list,tpr_list,auc_list=dict(),dict(),dict()\n",
    "\n",
    "\n",
    "logistic=LogisticRegression(max_iter=1000)\n",
    "logistic.fit(X_train1.fillna(0),y_train1)\n",
    "fpr_list[0], tpr_list[0], _ = roc_curve(y_test1, y_roc(logistic,X_test1.fillna(0)))\n",
    "print('Logistic regression train score:',\n",
    "      logistic.score(X_train1.fillna(0),y_train1),'\\n test score:',logistic.score(X_test1.fillna(0),y_test1.fillna(0)),'\\n Logistic regression train Brier score:',\n",
    "      brier_score(logistic.predict_proba(X_train1.fillna(0)),y_train1),'\\n test Brier score:',brier_score(logistic.predict_proba(X_test1.fillna(0)),y_test1),\n",
    "     '\\n AUROC:',roc_auc_score(y_test1, y_roc(logistic,X_test1.fillna(0))))\n",
    "\n",
    "RF=RandomForestClassifier(n_estimators=150,min_samples_split=2)\n",
    "RF.fit(X_train1.fillna(0),y_train1)\n",
    "fpr_list[1], tpr_list[1], _ = roc_curve(y_test1, y_roc(RF,X_test1.fillna(0)))\n",
    "print('Random forest train score:',\n",
    "      RF.score(X_train1.fillna(0),y_train1),'\\n test score:',RF.score(X_test1.fillna(0),y_test1.fillna(0)),'\\n Random forest  train Brier score:',\n",
    "      brier_score(RF.predict_proba(X_train1.fillna(0)),y_train1),'\\n test Brier score:',brier_score(RF.predict_proba(X_test1.fillna(0)),y_test1),\n",
    "     '\\n AUROC:',roc_auc_score(y_test1, y_roc(RF,X_test1.fillna(0))))\n",
    "\n",
    "GBDT=GradientBoostingClassifier()\n",
    "params_SGD={'n_estimators':[150,100],'min_samples_split':[2,4]}\n",
    "grid_SGD=GridSearchCV(GBDT,param_grid=params_SGD, scoring='neg_brier_score',cv=3)\n",
    "grid_SGD.fit(X_train1.fillna(0),y_train1.fillna(0))\n",
    "fpr_list[2], tpr_list[2], _ = roc_curve(y_test1, y_roc(grid_SGD.best_estimator_,X_test1.fillna(0)))\n",
    "print('SGD best layer size:',grid_SGD.best_params_,'\\n best train score:',\n",
    "      grid_SGD.best_score_,'\\n test score:',grid_SGD.score(X_test1.fillna(0),y_test1.fillna(0)),'\\n SGD  train Brier score:',\n",
    "      brier_score(grid_SGD.predict_proba(X_train1.fillna(0)),y_train1),'\\n test Brier score:',brier_score(grid_SGD.predict_proba(X_test1.fillna(0)),y_test1),\n",
    "     '\\n AUROC:',roc_auc_score(y_test1, y_roc(grid_SGD.best_estimator_,X_test1.fillna(0))))\n",
    "\n",
    "pipe = Sequential()\n",
    "n_cols = X_train1.shape[1]\n",
    "pipe.add(BatchNormalization(input_shape=(n_cols,)))\n",
    "pipe.add(Dense(70, activation= 'linear'))\n",
    "pipe.add(Dropout(0.3))\n",
    "pipe.add(Dense(50, activation= 'relu'))\n",
    "pipe.add(Dropout(0.3))\n",
    "pipe.add(Dense(50, activation= 'relu'))\n",
    "pipe.add(Dropout(0.3))\n",
    "#model.add(Dense(100, activation='relu'))\n",
    "#model.add(Dropout(0.3))\n",
    "#model.add(Dense(100, activation='linear'))\n",
    "pipe.add(BatchNormalization())\n",
    "pipe.add(Dense(2, activation='softmax'))\n",
    "    #model.compile(\n",
    "        #optimizer='Adam',\n",
    "        #loss='mean_squared_error',\n",
    "        #metrics=['accuracy'])\n",
    "early_stopping_monitor = EarlyStopping(patience=50)\n",
    "sgd = keras.optimizers.SGD(lr=.001, decay=2e-4, momentum=0.9, nesterov=True)\n",
    "pipe.compile(loss = 'sparse_categorical_crossentropy',optimizer = 'sgd', metrics=['accuracy'])\n",
    "history=pipe.fit(X_train1.fillna(0).astype('float32'), y_train1, validation_split=0.3, epochs=200, callbacks=[early_stopping_monitor])\n",
    "#history=model.fit(X_train, y_train, validation_split=0.2, epochs=25)\n",
    "score = pipe.evaluate(X_test1.fillna(0).astype('float32'), y_test1, verbose=0)\n",
    "pipe.fit(X_train1.fillna(0).astype('float32'),y_train1.fillna(0))\n",
    "fpr_list[3], tpr_list[3], _ = roc_curve(y_test1, y_roc(pipe,X_test1.fillna(0).astype('float32')))\n",
    "print('MLP train Brier score:',\n",
    "      brier_score(pipe.predict_proba(X_train1.fillna(0).astype('float32')),y_train1),'\\n test Brier score:',brier_score(pipe.predict_proba(X_test1.fillna(0).astype('float32')),y_test1),\n",
    "     '\\n AUROC:',roc_auc_score(y_test1, y_roc(pipe,X_test1.fillna(0).astype('float32'))))\n",
    "\n",
    "\n",
    "XGB=XGBClassifier()\n",
    "params_XGB={    'n_estimators': [50, 100],\n",
    "    'max_depth': [2, 3],\n",
    "    'min_child_weight': [2,4,6],\n",
    "    'subsample': [0.9],\n",
    "    'colsample_bytree': [0.8],'reg_lambda':[1000]}\n",
    "grid_XGB=GridSearchCV(XGB,param_grid=params_XGB, scoring='neg_brier_score',cv=3)\n",
    "grid_XGB.fit(X_train1.fillna(0),y_train1.fillna(0))\n",
    "fpr_list[4], tpr_list[4], _ = roc_curve(y_test1, y_roc(grid_XGB.best_estimator_,X_test1.fillna(0)))\n",
    "print('Xgboost best layer size:',grid_XGB.best_params_,'\\n best train score:',\n",
    "      grid_XGB.best_score_,'\\n test score:',grid_XGB.score(X_test1.fillna(0),y_test1.fillna(0)),'\\n Xgboost train Brier score:',\n",
    "      brier_score(grid_XGB.predict_proba(X_train1.fillna(0)),y_train1),'\\n test Brier score:',brier_score(grid_XGB.predict_proba(X_test1.fillna(0)),y_test1),\n",
    "     '\\n AUROC:',roc_auc_score(y_test1, y_roc(grid_XGB.best_estimator_,X_test1.fillna(0))))\n",
    "\n",
    "\n",
    "\n",
    "colors = cycle(['aqua', 'red', 'green','orange','blue'])\n",
    "labels=['Logistic Regression','Random Forest','Gradient Boosting','MLP','Xgboost']\n",
    "for i, label, color in zip(range(len(fpr_list)), labels, colors):\n",
    "    legend= label + ' (area = {1:0.2f})'''.format(i, auc(fpr_list[i], tpr_list[i]))\n",
    "    plt.plot(fpr_list[i], tpr_list[i], color=color, lw=2,label=legend)\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC curve for different methods on Recidivism_1Year')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('ROC_plot.png',dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a7de4c75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17605078595905532"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def aver_prob(prob_lists):\n",
    "    n=len(prob_lists)\n",
    "    return np.sum(np.array(prob_lists),0)/n\n",
    "X_train1,X_test1,y_train1,y_test1=train_test_split(raw_extend.drop(['ID','Recidivism_Within_3years','Recidivism_Arrest_Year1','Recidivism_Arrest_Year2','Recidivism_Arrest_Year3'],axis=1),raw_extend_remove['Recidivism_Arrest_Year2'])\n",
    "\n",
    "X_train1_t,X_test1_t,y_train1_t,y_test1_t=train_test_split(raw_extend_remove.drop(['ID','Recidivism_Within_3years','Recidivism_Arrest_Year1','Recidivism_Arrest_Year2','Recidivism_Arrest_Year3'],axis=1),raw_extend_remove['Recidivism_Arrest_Year2'])\n",
    "\n",
    "brier_score(aver_prob([grid_SGD_t.best_estimator_.predict_proba(X_test1_t.fillna(0)),\n",
    "                       grid_SGD.best_estimator_.predict_proba(X_test1.fillna(0)),\n",
    "                       #grid_XGB_t.best_estimator_.predict_proba(X_test1_t.fillna(0)),\n",
    "                       grid_XGB.best_estimator_.predict_proba(X_test1.fillna(0)),                       \n",
    "                       ]),y_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d760a98e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17137843522146545"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def aver_prob(prob_lists):\n",
    "    n=len(prob_lists)\n",
    "    return np.sum(np.array(prob_lists),0)/n\n",
    "X_train1,X_test1,y_train1,y_test1=train_test_split(raw_extend.drop(['ID','Recidivism_Within_3years','Recidivism_Arrest_Year1','Recidivism_Arrest_Year2','Recidivism_Arrest_Year3'],axis=1),raw_extend_remove['Recidivism_Arrest_Year2'])\n",
    "\n",
    "X_train1_t,X_test1_t,y_train1_t,y_test1_t=train_test_split(raw_extend_remove.drop(['ID','Recidivism_Within_3years','Recidivism_Arrest_Year1','Recidivism_Arrest_Year2','Recidivism_Arrest_Year3'],axis=1),raw_extend_remove['Recidivism_Arrest_Year2'])\n",
    "\n",
    "brier_score(aver_prob([grid_SGD_t.best_estimator_.predict_proba(X_test1_t.fillna(0)),\n",
    "                       grid_SGD.best_estimator_.predict_proba(X_test1.fillna(0)),\n",
    "                       grid_XGB_t.best_estimator_.predict_proba(X_test1_t.fillna(0)),\n",
    "                       grid_XGB.best_estimator_.predict_proba(X_test1.fillna(0)),                       \n",
    "                       ]),y_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e78cd05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15887170588258817"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def aver_prob(prob_lists):\n",
    "    n=len(prob_lists)\n",
    "    return np.sum(np.array(prob_lists),0)/n\n",
    "X_train1,X_test1,y_train1,y_test1=train_test_split(raw_extend.drop(['ID','Recidivism_Within_3years','Recidivism_Arrest_Year1','Recidivism_Arrest_Year2','Recidivism_Arrest_Year3'],axis=1),raw_extend_remove['Recidivism_Arrest_Year2'])\n",
    "\n",
    "X_train1_t,X_test1_t,y_train1_t,y_test1_t=train_test_split(raw_extend_remove.drop(['ID','Recidivism_Within_3years','Recidivism_Arrest_Year1','Recidivism_Arrest_Year2','Recidivism_Arrest_Year3'],axis=1),raw_extend_remove['Recidivism_Arrest_Year2'])\n",
    "\n",
    "brier_score(aver_prob([#grid_SGD_t.best_estimator_.predict_proba(X_test1_t.fillna(0)),\n",
    "                       grid_SGD.best_estimator_.predict_proba(X_test1.fillna(0)),\n",
    "                       #grid_XGB_t.best_estimator_.predict_proba(X_test1_t.fillna(0)),\n",
    "                       grid_XGB.best_estimator_.predict_proba(X_test1.fillna(0)),                       \n",
    "                       ]),y_test1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3f35ad",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "56de3f6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Race</th>\n",
       "      <th>Age_at_Release</th>\n",
       "      <th>Residence_PUMA</th>\n",
       "      <th>Gang_Affiliated</th>\n",
       "      <th>Supervision_Risk_Score_First</th>\n",
       "      <th>Supervision_Level_First</th>\n",
       "      <th>Education_Level</th>\n",
       "      <th>Dependents</th>\n",
       "      <th>...</th>\n",
       "      <th>Total\\nofficers</th>\n",
       "      <th>Total\\ncivilians</th>\n",
       "      <th>Murder</th>\n",
       "      <th>Rape</th>\n",
       "      <th>Robbery</th>\n",
       "      <th>Assault</th>\n",
       "      <th>Burglary</th>\n",
       "      <th>Larceny</th>\n",
       "      <th>Vehicle Theft</th>\n",
       "      <th>PUMA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>M</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>38</td>\n",
       "      <td>16</td>\n",
       "      <td>False</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Standard</td>\n",
       "      <td>High School Diploma</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>60.230769</td>\n",
       "      <td>18.384615</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>4.615385</td>\n",
       "      <td>5.769231</td>\n",
       "      <td>68.769231</td>\n",
       "      <td>197.769231</td>\n",
       "      <td>587.615385</td>\n",
       "      <td>43.538462</td>\n",
       "      <td>3146.153846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>M</td>\n",
       "      <td>BLACK</td>\n",
       "      <td>38</td>\n",
       "      <td>15</td>\n",
       "      <td>False</td>\n",
       "      <td>5.0</td>\n",
       "      <td>High</td>\n",
       "      <td>High School Diploma</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>111.750000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>36.750000</td>\n",
       "      <td>158.250000</td>\n",
       "      <td>541.250000</td>\n",
       "      <td>1707.750000</td>\n",
       "      <td>150.750000</td>\n",
       "      <td>3000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>M</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>33</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Standard</td>\n",
       "      <td>Less than HS diploma</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>55.700000</td>\n",
       "      <td>15.700000</td>\n",
       "      <td>2.100000</td>\n",
       "      <td>9.100000</td>\n",
       "      <td>37.900000</td>\n",
       "      <td>72.900000</td>\n",
       "      <td>376.300000</td>\n",
       "      <td>867.800000</td>\n",
       "      <td>82.400000</td>\n",
       "      <td>1780.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>M</td>\n",
       "      <td>BLACK</td>\n",
       "      <td>33</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Standard</td>\n",
       "      <td>Less than HS diploma</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>15.750000</td>\n",
       "      <td>16.375000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.125000</td>\n",
       "      <td>7.250000</td>\n",
       "      <td>40.625000</td>\n",
       "      <td>135.875000</td>\n",
       "      <td>357.125000</td>\n",
       "      <td>13.625000</td>\n",
       "      <td>1275.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23</td>\n",
       "      <td>F</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>48</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>High School Diploma</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>55.700000</td>\n",
       "      <td>15.700000</td>\n",
       "      <td>2.100000</td>\n",
       "      <td>9.100000</td>\n",
       "      <td>37.900000</td>\n",
       "      <td>72.900000</td>\n",
       "      <td>376.300000</td>\n",
       "      <td>867.800000</td>\n",
       "      <td>82.400000</td>\n",
       "      <td>1780.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5455</th>\n",
       "      <td>26740</td>\n",
       "      <td>M</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>48</td>\n",
       "      <td>11</td>\n",
       "      <td>False</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Standard</td>\n",
       "      <td>At least some college</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>241.666667</td>\n",
       "      <td>45.666667</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>19.333333</td>\n",
       "      <td>26.666667</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>463.000000</td>\n",
       "      <td>1567.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>3333.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5456</th>\n",
       "      <td>26744</td>\n",
       "      <td>M</td>\n",
       "      <td>BLACK</td>\n",
       "      <td>48</td>\n",
       "      <td>20</td>\n",
       "      <td>False</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Standard</td>\n",
       "      <td>Less than HS diploma</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>40.500000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>354.500000</td>\n",
       "      <td>984.000000</td>\n",
       "      <td>64.500000</td>\n",
       "      <td>1552.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5457</th>\n",
       "      <td>26746</td>\n",
       "      <td>M</td>\n",
       "      <td>BLACK</td>\n",
       "      <td>43</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Standard</td>\n",
       "      <td>At least some college</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>233.000000</td>\n",
       "      <td>592.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>2002.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5458</th>\n",
       "      <td>26752</td>\n",
       "      <td>M</td>\n",
       "      <td>BLACK</td>\n",
       "      <td>28</td>\n",
       "      <td>6</td>\n",
       "      <td>False</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Standard</td>\n",
       "      <td>At least some college</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>30.153846</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>1.153846</td>\n",
       "      <td>4.923077</td>\n",
       "      <td>16.692308</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>194.461538</td>\n",
       "      <td>579.000000</td>\n",
       "      <td>29.384615</td>\n",
       "      <td>338.461538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5459</th>\n",
       "      <td>26755</td>\n",
       "      <td>M</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>33</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Standard</td>\n",
       "      <td>At least some college</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>55.700000</td>\n",
       "      <td>15.700000</td>\n",
       "      <td>2.100000</td>\n",
       "      <td>9.100000</td>\n",
       "      <td>37.900000</td>\n",
       "      <td>72.900000</td>\n",
       "      <td>376.300000</td>\n",
       "      <td>867.800000</td>\n",
       "      <td>82.400000</td>\n",
       "      <td>1780.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5460 rows × 202 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID Gender   Race  Age_at_Release  Residence_PUMA Gang_Affiliated  \\\n",
       "0         6      M  WHITE              38              16           False   \n",
       "1         8      M  BLACK              38              15           False   \n",
       "2        15      M  WHITE              33               4           False   \n",
       "3        16      M  BLACK              33               2           False   \n",
       "4        23      F  WHITE              48               4             NaN   \n",
       "...     ...    ...    ...             ...             ...             ...   \n",
       "5455  26740      M  WHITE              48              11           False   \n",
       "5456  26744      M  BLACK              48              20           False   \n",
       "5457  26746      M  BLACK              43               5           False   \n",
       "5458  26752      M  BLACK              28               6           False   \n",
       "5459  26755      M  WHITE              33               4           False   \n",
       "\n",
       "      Supervision_Risk_Score_First Supervision_Level_First  \\\n",
       "0                              5.0                Standard   \n",
       "1                              5.0                    High   \n",
       "2                              7.0                Standard   \n",
       "3                              4.0                Standard   \n",
       "4                              4.0                     NaN   \n",
       "...                            ...                     ...   \n",
       "5455                           5.0                Standard   \n",
       "5456                           7.0                Standard   \n",
       "5457                           5.0                Standard   \n",
       "5458                           5.0                Standard   \n",
       "5459                           5.0                Standard   \n",
       "\n",
       "            Education_Level  Dependents  ... Total\\nofficers Total\\ncivilians  \\\n",
       "0       High School Diploma           0  ...       60.230769        18.384615   \n",
       "1       High School Diploma           3  ...      111.750000        24.000000   \n",
       "2      Less than HS diploma           1  ...       55.700000        15.700000   \n",
       "3      Less than HS diploma           3  ...       15.750000        16.375000   \n",
       "4       High School Diploma           0  ...       55.700000        15.700000   \n",
       "...                     ...         ...  ...             ...              ...   \n",
       "5455  At least some college           3  ...      241.666667        45.666667   \n",
       "5456   Less than HS diploma           0  ...       94.000000        40.500000   \n",
       "5457  At least some college           3  ...       31.000000         5.000000   \n",
       "5458  At least some college           3  ...       30.153846        24.000000   \n",
       "5459  At least some college           3  ...       55.700000        15.700000   \n",
       "\n",
       "        Murder       Rape    Robbery     Assault    Burglary      Larceny  \\\n",
       "0     0.384615   4.615385   5.769231   68.769231  197.769231   587.615385   \n",
       "1     1.500000  15.000000  36.750000  158.250000  541.250000  1707.750000   \n",
       "2     2.100000   9.100000  37.900000   72.900000  376.300000   867.800000   \n",
       "3     1.000000   2.125000   7.250000   40.625000  135.875000   357.125000   \n",
       "4     2.100000   9.100000  37.900000   72.900000  376.300000   867.800000   \n",
       "...        ...        ...        ...         ...         ...          ...   \n",
       "5455  2.333333  19.333333  26.666667  101.000000  463.000000  1567.000000   \n",
       "5456  2.500000  11.000000  25.000000   83.000000  354.500000   984.000000   \n",
       "5457  1.000000   3.000000   9.000000  118.000000  233.000000   592.000000   \n",
       "5458  1.153846   4.923077  16.692308   69.000000  194.461538   579.000000   \n",
       "5459  2.100000   9.100000  37.900000   72.900000  376.300000   867.800000   \n",
       "\n",
       "      Vehicle Theft         PUMA  \n",
       "0         43.538462  3146.153846  \n",
       "1        150.750000  3000.000000  \n",
       "2         82.400000  1780.000000  \n",
       "3         13.625000  1275.000000  \n",
       "4         82.400000  1780.000000  \n",
       "...             ...          ...  \n",
       "5455     117.000000  3333.333333  \n",
       "5456      64.500000  1552.000000  \n",
       "5457      13.000000  2002.000000  \n",
       "5458      29.384615   338.461538  \n",
       "5459      82.400000  1780.000000  \n",
       "\n",
       "[5460 rows x 202 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_test=pd.read_csv(r'.\\data\\NIJ_s_Recidivism_Challenge_Test_Dataset2.csv')\n",
    "raw_test=pd.merge(raw_test,PUMA_new,left_on='Residence_PUMA',right_on='Code',how='left')\n",
    "# Change the dtype of the feature from object to intagar\n",
    "raw_test['TEN']=raw_test['TEN'].astype('category')\n",
    "raw_test['TEN']=raw_test['TEN'].cat.codes\n",
    "raw_test['Residence_PUMA']=raw_test['Residence_PUMA'].astype('category')\n",
    "raw_test['Residence_PUMA']=raw_test['Residence_PUMA'].cat.codes\n",
    "raw_test['Age_at_Release']=raw_test['Age_at_Release'].apply(lambda x: int(x[:2]))\n",
    "raw_test['Dependents']=raw_test['Dependents'].apply(lambda x: int(x[:1]))\n",
    "raw_test['Prior_Arrest_Episodes_Felony']=raw_test['Prior_Arrest_Episodes_Felony'].apply(lambda x: int(x[:2]))\n",
    "raw_test['Prior_Arrest_Episodes_Drug']=raw_test['Prior_Arrest_Episodes_Drug'].apply(lambda x: int(x[:2]))\n",
    "raw_test['Prior_Arrest_Episodes_Misd']=raw_test['Prior_Arrest_Episodes_Misd'].apply(lambda x: int(x[:2]))\n",
    "raw_test['Prior_Arrest_Episodes_Violent']=raw_test['Prior_Arrest_Episodes_Violent'].apply(lambda x: int(x[:2]))\n",
    "raw_test['Prior_Arrest_Episodes_Property']=raw_test['Prior_Arrest_Episodes_Property'].apply(lambda x: int(x[:2]))\n",
    "raw_test['Prior_Arrest_Episodes_PPViolationCharges']=raw_test['Prior_Arrest_Episodes_PPViolationCharges'].apply(lambda x: int(x[:2]))\n",
    "raw_test['Prior_Conviction_Episodes_Felony']=raw_test['Prior_Conviction_Episodes_Felony'].apply(lambda x: int(x[:1]))\n",
    "raw_test['Prior_Conviction_Episodes_Misd']=raw_test['Prior_Conviction_Episodes_Misd'].apply(lambda x: int(x[:1]))\n",
    "raw_test['Prior_Conviction_Episodes_Prop']=raw_test['Prior_Conviction_Episodes_Prop'].apply(lambda x: int(x[:1]))\n",
    "raw_test['Prior_Conviction_Episodes_Drug']=raw_test['Prior_Conviction_Episodes_Drug'].apply(lambda x: int(x[:1]))\n",
    "#raw_test['Delinquency_Reports']=raw_test['Delinquency_Reports'].apply(lambda x: int(x[:1]))\n",
    "#raw_test['Program_Attendances']=raw_test['Program_Attendances'].apply(lambda x: int(x[:2]))\n",
    "#raw_test['Program_UnexcusedAbsences']=raw_test['Program_UnexcusedAbsences'].apply(lambda x: int(x[:1]))\n",
    "#raw_test['Residence_Changes']=raw_test['Residence_Changes'].apply(lambda x: int(x[:1]))\n",
    "\n",
    "\n",
    "raw_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "36939990",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\myjr\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\myjr\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "#raw_extend_test=pd.merge(raw_test,PUMA_new,left_on='Residence_PUMA',right_on='Code',how='outer')\n",
    "raw_extend_test=raw_test.copy()\n",
    "# Change the dtype of the feature from object to intagar\n",
    "\n",
    "# scale the columns which are not bool or category\n",
    "scaler = StandardScaler()\n",
    "scaling_set=[]\n",
    "for column in raw_extend_test.columns:\n",
    "    if raw_extend_test[column].dtype == object:\n",
    "        raw_extend_test[column]=raw_extend_test[column].astype('category')\n",
    "        raw_extend_test[column]=raw_extend_test[column].cat.codes\n",
    "    elif raw_extend_test[column].dtype in ['int64','float32','float64'] :\n",
    "        scaling_set+=[column]\n",
    "raw_extend_test[scaling_set]=scaler.fit_transform(raw_extend_test[scaling_set].values)\n",
    "raw_extend_test\n",
    "#raw_extend_test=raw_extend_test.drop(index=raw_extend_test[raw_extend_test.Supervision_Risk_Score_First.isnull()].index)\n",
    "#raw_extend_test=raw_extend_test.drop(index=set(raw_extend_test[raw_extend_test.Supervision_Level_First.isnull()].index) & set(raw_extend_test[raw_extend_test.Prison_Offense .isnull()].index))\n",
    "#raw_extend_test=raw_extend_test.reset_index(drop=True)\n",
    "# impute missing value 'Supervision_Level_First' and 'Prison_Offense' with relative feature\n",
    "for missing_column in ['Supervision_Level_First','Prison_Offense']:\n",
    "    test_index=raw_extend_test[raw_extend_test[missing_column]==-1].index\n",
    "    train_index=raw_extend_test[raw_extend_test[missing_column]!=-1].index\n",
    "    X=raw_extend_test.drop(columns=[missing_column])\n",
    "    y=raw_extend_test[missing_column]\n",
    "    logreg = LogisticRegression()\n",
    "    logreg.fit(X.iloc[train_index,:].fillna(0),y[train_index])\n",
    "    raw_extend_test.loc[test_index,missing_column]=logreg.predict(X.iloc[test_index,:].fillna(0))\n",
    "    raw_extend_test[missing_column]=raw_extend_test[missing_column].astype('category')\n",
    "    raw_extend_test[missing_column]=raw_extend_test[missing_column].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fa1be636",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Race</th>\n",
       "      <th>Age_at_Release</th>\n",
       "      <th>Gang_Affiliated</th>\n",
       "      <th>Supervision_Risk_Score_First</th>\n",
       "      <th>Supervision_Level_First</th>\n",
       "      <th>Education_Level</th>\n",
       "      <th>Dependents</th>\n",
       "      <th>Prison_Offense</th>\n",
       "      <th>...</th>\n",
       "      <th>FTELP</th>\n",
       "      <th>FVACSP</th>\n",
       "      <th>FVEHP</th>\n",
       "      <th>Total law\\nenforcement\\nemployees</th>\n",
       "      <th>Total\\nofficers</th>\n",
       "      <th>Total\\ncivilians</th>\n",
       "      <th>Burglary</th>\n",
       "      <th>Larceny</th>\n",
       "      <th>Vehicle Theft</th>\n",
       "      <th>PUMA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.682777</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.507782</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.385252</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.219910</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.170101</td>\n",
       "      <td>-0.440234</td>\n",
       "      <td>-0.754932</td>\n",
       "      <td>-0.442168</td>\n",
       "      <td>-0.389742</td>\n",
       "      <td>-0.512988</td>\n",
       "      <td>-0.371568</td>\n",
       "      <td>-0.350737</td>\n",
       "      <td>-0.293308</td>\n",
       "      <td>0.556955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.682520</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.507782</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.385252</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.234595</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.582757</td>\n",
       "      <td>-1.400531</td>\n",
       "      <td>-0.651192</td>\n",
       "      <td>-0.216168</td>\n",
       "      <td>-0.123506</td>\n",
       "      <td>-0.433774</td>\n",
       "      <td>-0.210058</td>\n",
       "      <td>-0.173294</td>\n",
       "      <td>-0.213081</td>\n",
       "      <td>0.443348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.681620</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.033508</td>\n",
       "      <td>0</td>\n",
       "      <td>0.459367</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.401742</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000267</td>\n",
       "      <td>-0.262700</td>\n",
       "      <td>0.129668</td>\n",
       "      <td>-0.470708</td>\n",
       "      <td>-0.413155</td>\n",
       "      <td>-0.550859</td>\n",
       "      <td>-0.287621</td>\n",
       "      <td>-0.306352</td>\n",
       "      <td>-0.264228</td>\n",
       "      <td>-0.504963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.681492</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.033508</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.807562</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.234595</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.090767</td>\n",
       "      <td>1.752553</td>\n",
       "      <td>0.835333</td>\n",
       "      <td>-0.626063</td>\n",
       "      <td>-0.619605</td>\n",
       "      <td>-0.541337</td>\n",
       "      <td>-0.400672</td>\n",
       "      <td>-0.387249</td>\n",
       "      <td>-0.315693</td>\n",
       "      <td>-0.897502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.680592</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.590364</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.807562</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.219910</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000267</td>\n",
       "      <td>-0.262700</td>\n",
       "      <td>0.129668</td>\n",
       "      <td>-0.470708</td>\n",
       "      <td>-0.413155</td>\n",
       "      <td>-0.550859</td>\n",
       "      <td>-0.287621</td>\n",
       "      <td>-0.306352</td>\n",
       "      <td>-0.264228</td>\n",
       "      <td>-0.504963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5455</th>\n",
       "      <td>1.752851</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.590364</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.385252</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.234595</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.650412</td>\n",
       "      <td>-1.053449</td>\n",
       "      <td>-0.396083</td>\n",
       "      <td>0.383428</td>\n",
       "      <td>0.547864</td>\n",
       "      <td>-0.128131</td>\n",
       "      <td>-0.246853</td>\n",
       "      <td>-0.195590</td>\n",
       "      <td>-0.238336</td>\n",
       "      <td>0.702450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5456</th>\n",
       "      <td>1.753365</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.590364</td>\n",
       "      <td>0</td>\n",
       "      <td>0.459367</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.219910</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.862282</td>\n",
       "      <td>0.375927</td>\n",
       "      <td>0.316156</td>\n",
       "      <td>-0.221113</td>\n",
       "      <td>-0.215232</td>\n",
       "      <td>-0.201015</td>\n",
       "      <td>-0.297871</td>\n",
       "      <td>-0.287945</td>\n",
       "      <td>-0.277623</td>\n",
       "      <td>-0.682188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5457</th>\n",
       "      <td>1.753622</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.049073</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.385252</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.234595</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014744</td>\n",
       "      <td>-0.895153</td>\n",
       "      <td>0.365913</td>\n",
       "      <td>-0.610735</td>\n",
       "      <td>-0.540798</td>\n",
       "      <td>-0.701799</td>\n",
       "      <td>-0.355002</td>\n",
       "      <td>-0.350042</td>\n",
       "      <td>-0.316161</td>\n",
       "      <td>-0.332401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5458</th>\n",
       "      <td>1.754393</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.574799</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.385252</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.234595</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.186956</td>\n",
       "      <td>2.589765</td>\n",
       "      <td>-1.099334</td>\n",
       "      <td>-0.538927</td>\n",
       "      <td>-0.545170</td>\n",
       "      <td>-0.433774</td>\n",
       "      <td>-0.373124</td>\n",
       "      <td>-0.352102</td>\n",
       "      <td>-0.303900</td>\n",
       "      <td>-1.625477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5459</th>\n",
       "      <td>1.754779</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.033508</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.385252</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.234595</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000267</td>\n",
       "      <td>-0.262700</td>\n",
       "      <td>0.129668</td>\n",
       "      <td>-0.470708</td>\n",
       "      <td>-0.413155</td>\n",
       "      <td>-0.550859</td>\n",
       "      <td>-0.287621</td>\n",
       "      <td>-0.306352</td>\n",
       "      <td>-0.264228</td>\n",
       "      <td>-0.504963</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5460 rows × 112 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID  Gender  Race  Age_at_Release  Gang_Affiliated  \\\n",
       "0    -1.682777       1     1        0.507782                0   \n",
       "1    -1.682520       1     0        0.507782                0   \n",
       "2    -1.681620       1     1       -0.033508                0   \n",
       "3    -1.681492       1     0       -0.033508                0   \n",
       "4    -1.680592       0     1        1.590364               -1   \n",
       "...        ...     ...   ...             ...              ...   \n",
       "5455  1.752851       1     1        1.590364                0   \n",
       "5456  1.753365       1     0        1.590364                0   \n",
       "5457  1.753622       1     0        1.049073                0   \n",
       "5458  1.754393       1     0       -0.574799                0   \n",
       "5459  1.754779       1     1       -0.033508                0   \n",
       "\n",
       "      Supervision_Risk_Score_First  Supervision_Level_First  Education_Level  \\\n",
       "0                        -0.385252                        2                1   \n",
       "1                        -0.385252                        0                1   \n",
       "2                         0.459367                        2                2   \n",
       "3                        -0.807562                        2                2   \n",
       "4                        -0.807562                        2                1   \n",
       "...                            ...                      ...              ...   \n",
       "5455                     -0.385252                        2                0   \n",
       "5456                      0.459367                        2                2   \n",
       "5457                     -0.385252                        2                0   \n",
       "5458                     -0.385252                        2                0   \n",
       "5459                     -0.385252                        2                0   \n",
       "\n",
       "      Dependents  Prison_Offense  ...     FTELP    FVACSP     FVEHP  \\\n",
       "0      -1.219910               2  ...  0.170101 -0.440234 -0.754932   \n",
       "1       1.234595               0  ...  3.582757 -1.400531 -0.651192   \n",
       "2      -0.401742               3  ...  0.000267 -0.262700  0.129668   \n",
       "3       1.234595               0  ...  0.090767  1.752553  0.835333   \n",
       "4      -1.219910               3  ...  0.000267 -0.262700  0.129668   \n",
       "...          ...             ...  ...       ...       ...       ...   \n",
       "5455    1.234595               0  ... -0.650412 -1.053449 -0.396083   \n",
       "5456   -1.219910               2  ... -0.862282  0.375927  0.316156   \n",
       "5457    1.234595               3  ... -0.014744 -0.895153  0.365913   \n",
       "5458    1.234595               2  ... -0.186956  2.589765 -1.099334   \n",
       "5459    1.234595               2  ...  0.000267 -0.262700  0.129668   \n",
       "\n",
       "      Total law\\nenforcement\\nemployees  Total\\nofficers  Total\\ncivilians  \\\n",
       "0                             -0.442168        -0.389742         -0.512988   \n",
       "1                             -0.216168        -0.123506         -0.433774   \n",
       "2                             -0.470708        -0.413155         -0.550859   \n",
       "3                             -0.626063        -0.619605         -0.541337   \n",
       "4                             -0.470708        -0.413155         -0.550859   \n",
       "...                                 ...              ...               ...   \n",
       "5455                           0.383428         0.547864         -0.128131   \n",
       "5456                          -0.221113        -0.215232         -0.201015   \n",
       "5457                          -0.610735        -0.540798         -0.701799   \n",
       "5458                          -0.538927        -0.545170         -0.433774   \n",
       "5459                          -0.470708        -0.413155         -0.550859   \n",
       "\n",
       "      Burglary   Larceny  Vehicle Theft      PUMA  \n",
       "0    -0.371568 -0.350737      -0.293308  0.556955  \n",
       "1    -0.210058 -0.173294      -0.213081  0.443348  \n",
       "2    -0.287621 -0.306352      -0.264228 -0.504963  \n",
       "3    -0.400672 -0.387249      -0.315693 -0.897502  \n",
       "4    -0.287621 -0.306352      -0.264228 -0.504963  \n",
       "...        ...       ...            ...       ...  \n",
       "5455 -0.246853 -0.195590      -0.238336  0.702450  \n",
       "5456 -0.297871 -0.287945      -0.277623 -0.682188  \n",
       "5457 -0.355002 -0.350042      -0.316161 -0.332401  \n",
       "5458 -0.373124 -0.352102      -0.303900 -1.625477  \n",
       "5459 -0.287621 -0.306352      -0.264228 -0.504963  \n",
       "\n",
       "[5460 rows x 112 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_extend_test_t=raw_extend_test.drop(list(GBDT_importance[GBDT_importance.importance==0].Feature),axis=1)\n",
    "raw_extend_test_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f4ba4f8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def aver_prob(prob_lists):\n",
    "    n=len(prob_lists)\n",
    "    return np.sum(np.array(prob_lists),0)/n\n",
    "prob_list=aver_prob([[x[1] for x in grid_SGD.best_estimator_.predict_proba(raw_extend_test.drop(['ID'],axis=1).fillna(0))],\n",
    "          [x[1] for x in grid_XGB.best_estimator_.predict_proba(raw_extend_test.drop(['ID'],axis=1))],\n",
    "          ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5d289d2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Race</th>\n",
       "      <th>Age_at_Release</th>\n",
       "      <th>Residence_PUMA</th>\n",
       "      <th>Gang_Affiliated</th>\n",
       "      <th>Supervision_Risk_Score_First</th>\n",
       "      <th>Supervision_Level_First</th>\n",
       "      <th>Education_Level</th>\n",
       "      <th>Dependents</th>\n",
       "      <th>...</th>\n",
       "      <th>Total\\ncivilians</th>\n",
       "      <th>Murder</th>\n",
       "      <th>Rape</th>\n",
       "      <th>Robbery</th>\n",
       "      <th>Assault</th>\n",
       "      <th>Burglary</th>\n",
       "      <th>Larceny</th>\n",
       "      <th>Vehicle Theft</th>\n",
       "      <th>PUMA</th>\n",
       "      <th>Probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.507782</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.385252</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.219910</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.512988</td>\n",
       "      <td>-0.334484</td>\n",
       "      <td>-0.417857</td>\n",
       "      <td>-0.296440</td>\n",
       "      <td>-0.310084</td>\n",
       "      <td>-0.371568</td>\n",
       "      <td>-0.350737</td>\n",
       "      <td>-0.293308</td>\n",
       "      <td>0.556955</td>\n",
       "      <td>0.055036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.507782</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.385252</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.234595</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.433774</td>\n",
       "      <td>-0.291915</td>\n",
       "      <td>-0.194198</td>\n",
       "      <td>-0.252712</td>\n",
       "      <td>-0.193511</td>\n",
       "      <td>-0.210058</td>\n",
       "      <td>-0.173294</td>\n",
       "      <td>-0.213081</td>\n",
       "      <td>0.443348</td>\n",
       "      <td>0.360398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.033508</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.459367</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.401742</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.550859</td>\n",
       "      <td>-0.269017</td>\n",
       "      <td>-0.321270</td>\n",
       "      <td>-0.251089</td>\n",
       "      <td>-0.304702</td>\n",
       "      <td>-0.287621</td>\n",
       "      <td>-0.306352</td>\n",
       "      <td>-0.264228</td>\n",
       "      <td>-0.504963</td>\n",
       "      <td>0.308977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.033508</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.807562</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.234595</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.541337</td>\n",
       "      <td>-0.310998</td>\n",
       "      <td>-0.471494</td>\n",
       "      <td>-0.294350</td>\n",
       "      <td>-0.346749</td>\n",
       "      <td>-0.400672</td>\n",
       "      <td>-0.387249</td>\n",
       "      <td>-0.315693</td>\n",
       "      <td>-0.897502</td>\n",
       "      <td>0.278147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.590364</td>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.807562</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.219910</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.550859</td>\n",
       "      <td>-0.269017</td>\n",
       "      <td>-0.321270</td>\n",
       "      <td>-0.251089</td>\n",
       "      <td>-0.304702</td>\n",
       "      <td>-0.287621</td>\n",
       "      <td>-0.306352</td>\n",
       "      <td>-0.264228</td>\n",
       "      <td>-0.504963</td>\n",
       "      <td>0.224093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5455</th>\n",
       "      <td>26740</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.590364</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.385252</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.234595</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.128131</td>\n",
       "      <td>-0.260112</td>\n",
       "      <td>-0.100869</td>\n",
       "      <td>-0.266944</td>\n",
       "      <td>-0.268094</td>\n",
       "      <td>-0.246853</td>\n",
       "      <td>-0.195590</td>\n",
       "      <td>-0.238336</td>\n",
       "      <td>0.702450</td>\n",
       "      <td>0.100192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5456</th>\n",
       "      <td>26744</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.590364</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0.459367</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.219910</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.201015</td>\n",
       "      <td>-0.253751</td>\n",
       "      <td>-0.280348</td>\n",
       "      <td>-0.269296</td>\n",
       "      <td>-0.291544</td>\n",
       "      <td>-0.297871</td>\n",
       "      <td>-0.287945</td>\n",
       "      <td>-0.277623</td>\n",
       "      <td>-0.682188</td>\n",
       "      <td>0.041388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5457</th>\n",
       "      <td>26746</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.049073</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.385252</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.234595</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.701799</td>\n",
       "      <td>-0.310998</td>\n",
       "      <td>-0.452649</td>\n",
       "      <td>-0.291880</td>\n",
       "      <td>-0.245947</td>\n",
       "      <td>-0.355002</td>\n",
       "      <td>-0.350042</td>\n",
       "      <td>-0.316161</td>\n",
       "      <td>-0.332401</td>\n",
       "      <td>0.122643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5458</th>\n",
       "      <td>26752</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.574799</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.385252</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.234595</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.433774</td>\n",
       "      <td>-0.305126</td>\n",
       "      <td>-0.411230</td>\n",
       "      <td>-0.281022</td>\n",
       "      <td>-0.309783</td>\n",
       "      <td>-0.373124</td>\n",
       "      <td>-0.352102</td>\n",
       "      <td>-0.303900</td>\n",
       "      <td>-1.625477</td>\n",
       "      <td>0.146249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5459</th>\n",
       "      <td>26755</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.033508</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.385252</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.234595</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.550859</td>\n",
       "      <td>-0.269017</td>\n",
       "      <td>-0.321270</td>\n",
       "      <td>-0.251089</td>\n",
       "      <td>-0.304702</td>\n",
       "      <td>-0.287621</td>\n",
       "      <td>-0.306352</td>\n",
       "      <td>-0.264228</td>\n",
       "      <td>-0.504963</td>\n",
       "      <td>0.108520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5460 rows × 203 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID  Gender  Race  Age_at_Release  Residence_PUMA  Gang_Affiliated  \\\n",
       "0         6       1     1        0.507782              16                0   \n",
       "1         8       1     0        0.507782              15                0   \n",
       "2        15       1     1       -0.033508               4                0   \n",
       "3        16       1     0       -0.033508               2                0   \n",
       "4        23       0     1        1.590364               4               -1   \n",
       "...     ...     ...   ...             ...             ...              ...   \n",
       "5455  26740       1     1        1.590364              11                0   \n",
       "5456  26744       1     0        1.590364              20                0   \n",
       "5457  26746       1     0        1.049073               5                0   \n",
       "5458  26752       1     0       -0.574799               6                0   \n",
       "5459  26755       1     1       -0.033508               4                0   \n",
       "\n",
       "      Supervision_Risk_Score_First  Supervision_Level_First  Education_Level  \\\n",
       "0                        -0.385252                        2                1   \n",
       "1                        -0.385252                        0                1   \n",
       "2                         0.459367                        2                2   \n",
       "3                        -0.807562                        2                2   \n",
       "4                        -0.807562                        2                1   \n",
       "...                            ...                      ...              ...   \n",
       "5455                     -0.385252                        2                0   \n",
       "5456                      0.459367                        2                2   \n",
       "5457                     -0.385252                        2                0   \n",
       "5458                     -0.385252                        2                0   \n",
       "5459                     -0.385252                        2                0   \n",
       "\n",
       "      Dependents  ...  Total\\ncivilians    Murder      Rape   Robbery  \\\n",
       "0      -1.219910  ...         -0.512988 -0.334484 -0.417857 -0.296440   \n",
       "1       1.234595  ...         -0.433774 -0.291915 -0.194198 -0.252712   \n",
       "2      -0.401742  ...         -0.550859 -0.269017 -0.321270 -0.251089   \n",
       "3       1.234595  ...         -0.541337 -0.310998 -0.471494 -0.294350   \n",
       "4      -1.219910  ...         -0.550859 -0.269017 -0.321270 -0.251089   \n",
       "...          ...  ...               ...       ...       ...       ...   \n",
       "5455    1.234595  ...         -0.128131 -0.260112 -0.100869 -0.266944   \n",
       "5456   -1.219910  ...         -0.201015 -0.253751 -0.280348 -0.269296   \n",
       "5457    1.234595  ...         -0.701799 -0.310998 -0.452649 -0.291880   \n",
       "5458    1.234595  ...         -0.433774 -0.305126 -0.411230 -0.281022   \n",
       "5459    1.234595  ...         -0.550859 -0.269017 -0.321270 -0.251089   \n",
       "\n",
       "       Assault  Burglary   Larceny  Vehicle Theft      PUMA  Probability  \n",
       "0    -0.310084 -0.371568 -0.350737      -0.293308  0.556955     0.055036  \n",
       "1    -0.193511 -0.210058 -0.173294      -0.213081  0.443348     0.360398  \n",
       "2    -0.304702 -0.287621 -0.306352      -0.264228 -0.504963     0.308977  \n",
       "3    -0.346749 -0.400672 -0.387249      -0.315693 -0.897502     0.278147  \n",
       "4    -0.304702 -0.287621 -0.306352      -0.264228 -0.504963     0.224093  \n",
       "...        ...       ...       ...            ...       ...          ...  \n",
       "5455 -0.268094 -0.246853 -0.195590      -0.238336  0.702450     0.100192  \n",
       "5456 -0.291544 -0.297871 -0.287945      -0.277623 -0.682188     0.041388  \n",
       "5457 -0.245947 -0.355002 -0.350042      -0.316161 -0.332401     0.122643  \n",
       "5458 -0.309783 -0.373124 -0.352102      -0.303900 -1.625477     0.146249  \n",
       "5459 -0.304702 -0.287621 -0.306352      -0.264228 -0.504963     0.108520  \n",
       "\n",
       "[5460 rows x 203 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_extend_test['Probability']=prob_list\n",
    "raw_extend_test['ID']=raw_test.ID\n",
    "raw_extend_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "192677cc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>M</td>\n",
       "      <td>0.055036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>M</td>\n",
       "      <td>0.360398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>M</td>\n",
       "      <td>0.308977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>M</td>\n",
       "      <td>0.278147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27</td>\n",
       "      <td>M</td>\n",
       "      <td>0.266731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5455</th>\n",
       "      <td>26543</td>\n",
       "      <td>F</td>\n",
       "      <td>0.060433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5456</th>\n",
       "      <td>26604</td>\n",
       "      <td>F</td>\n",
       "      <td>0.222661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5457</th>\n",
       "      <td>26617</td>\n",
       "      <td>F</td>\n",
       "      <td>0.091506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5458</th>\n",
       "      <td>26622</td>\n",
       "      <td>F</td>\n",
       "      <td>0.116771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5459</th>\n",
       "      <td>26701</td>\n",
       "      <td>F</td>\n",
       "      <td>0.257236</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5460 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID Gender  Probability\n",
       "0         6      M     0.055036\n",
       "1         8      M     0.360398\n",
       "2        15      M     0.308977\n",
       "3        16      M     0.278147\n",
       "4        27      M     0.266731\n",
       "...     ...    ...          ...\n",
       "5455  26543      F     0.060433\n",
       "5456  26604      F     0.222661\n",
       "5457  26617      F     0.091506\n",
       "5458  26622      F     0.116771\n",
       "5459  26701      F     0.257236\n",
       "\n",
       "[5460 rows x 3 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Male\n",
    "Output_male=raw_extend_test[raw_extend_test['Gender']==1][['ID','Gender','Probability']]\n",
    "Output_male.ID=Output_male.ID.astype(int)\n",
    "Output_male.Gender='M'\n",
    "# Female\n",
    "Output_female=raw_extend_test[raw_extend_test['Gender']==0][['ID','Gender','Probability']]\n",
    "Output_female.ID=Output_female.ID.astype(int)\n",
    "Output_female.Gender='F'\n",
    "Final_out=Output_male.append(Output_female).reset_index(drop=True)\n",
    "Final_out.to_csv(r'Recidivism_year2.csv',index=False)\n",
    "Final_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "2507db93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Final_out.Probability.isnull().sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
